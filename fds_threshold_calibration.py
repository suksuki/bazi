#!/usr/bin/env python3
"""
FDS é˜ˆå€¼æ ¡å‡†è„šæœ¬ (Threshold Calibration)
========================================
[ç¬¬019å·å·¥ç¨‹æŒ‡ä»¤] é€†å‘é˜ˆå€¼é”šå®š

**ç›®æ ‡**ï¼š
- è®¡ç®—æ‰€æœ‰é€»è¾‘åŒ¹é…æ ·æœ¬çš„é©¬æ°è·ç¦»åˆ†å¸ƒ
- ä½¿ç”¨äºŒåˆ†æ³•æœç´¢æœ€ä¼˜é˜ˆå€¼ï¼Œä½¿ç‰©ç†è¯†åˆ«ç‡æ¥è¿‘åŸºå‡†ä¸°åº¦
- å°†æœ€ä¼˜é˜ˆå€¼å†™å…¥registry

**æµç¨‹**ï¼š
1. è®¡ç®—æ‰€æœ‰åŒ¹é…æ ·æœ¬çš„é©¬æ°è·ç¦»åˆ†å¸ƒ
2. äºŒåˆ†æ³•æœç´¢æœ€ä¼˜é˜ˆå€¼ï¼ˆç›®æ ‡ï¼šç‰©ç†è¯†åˆ«ç‡ â‰ˆ åŸºå‡†ä¸°åº¦21.79%ï¼‰
3. æ›´æ–°registryæ–‡ä»¶
"""

import argparse
import json
import os
import sys
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
from scipy import stats

# å¼ºåˆ¶ä¾èµ–
try:
    from json_logic import jsonLogic
except ImportError:
    print("âŒ Critical: json-logic-quibble missing. Run: pip install json-logic-quibble")
    sys.exit(1)

REGISTRY_DIR = Path("./registry/holographic_pattern")
MANIFEST_DIR = Path("./config/patterns")
DEFAULT_DATA = "./data/holographic_universe_518k.jsonl"


def load_registry(pattern_id: str) -> Dict[str, Any]:
    """ä»registryç›®å½•åŠ è½½æ ¼å±€æ•°æ®"""
    registry_path = REGISTRY_DIR / f"{pattern_id}.json"
    if not registry_path.exists():
        raise FileNotFoundError(f"Registryæ–‡ä»¶ä¸å­˜åœ¨: {registry_path}")
    
    with open(registry_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_manifest(pattern_id: str) -> Dict[str, Any]:
    """ä»manifestç›®å½•åŠ è½½æ ¼å±€é…ç½®"""
    possible_names = [
        f"manifest_{pattern_id}.json",
        f"manifest_{pattern_id.replace('-', '')}.json",
        f"{pattern_id}.json"
    ]
    
    for name in possible_names:
        path = MANIFEST_DIR / name
        if path.exists():
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
    
    raise FileNotFoundError(f"Manifestæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•äº†: {possible_names}")


def get_weights_matrix(manifest: Dict[str, Any]) -> Tuple[np.ndarray, list]:
    """ä»manifestæ„å»ºæƒé‡çŸ©é˜µï¼ˆ10x5ï¼‰"""
    tmm = manifest['tensor_mapping_matrix']
    gods = tmm['ten_gods']
    matrix = []
    for god in gods:
        matrix.append(tmm['weights'][god])
    return np.array(matrix), gods


def calculate_5d_tensor(case_ten_gods: Dict[str, int], weights_matrix: np.ndarray, god_index_map: Dict[str, int]) -> np.ndarray:
    """è®¡ç®—æ ·æœ¬çš„5Då¼ é‡"""
    vec = np.zeros(10)
    for god, val in case_ten_gods.items():
        if god in god_index_map:
            vec[god_index_map[god]] = float(val)
    tensor = np.dot(weights_matrix.T, vec)
    return tensor


def compute_mahalanobis_distance(tensor: np.ndarray, mean: np.ndarray, cov_matrix: np.ndarray) -> float:
    """è®¡ç®—é©¬æ°è·ç¦»"""
    diff = tensor - mean
    try:
        inv_cov = np.linalg.pinv(cov_matrix)
        mahal_dist = np.sqrt(np.dot(np.dot(diff, inv_cov), diff))
        return mahal_dist
    except np.linalg.LinAlgError:
        # é™çº§ä¸ºæ¬§æ°è·ç¦»
        return np.sqrt(np.dot(diff, diff))


def compute_mahalanobis_distances_for_matched_samples(
    pattern_id: str,
    data_path: str
) -> Tuple[List[float], int]:
    """
    è®¡ç®—æ‰€æœ‰é€»è¾‘åŒ¹é…æ ·æœ¬çš„é©¬æ°è·ç¦»
    
    è¿”å›: (è·ç¦»åˆ—è¡¨, åŒ¹é…æ ·æœ¬æ•°)
    """
    print(f"ğŸš€ å¼€å§‹è®¡ç®— {pattern_id} åŒ¹é…æ ·æœ¬çš„é©¬æ°è·ç¦»åˆ†å¸ƒ...")
    
    # åŠ è½½æ•°æ®
    registry_data = load_registry(pattern_id)
    manifest = load_manifest(pattern_id)
    
    # æå–æµå½¢ç‰¹å¾
    fa = registry_data['data']['feature_anchors']['standard_manifold']
    mean_vector = np.array(fa['mean_vector'])
    cov_matrix = np.array(fa['covariance_matrix'])
    
    # æ„å»ºæƒé‡çŸ©é˜µ
    weights_matrix, gods_list = get_weights_matrix(manifest)
    god_index_map = {g: i for i, g in enumerate(gods_list)}
    
    # æå–é€»è¾‘è§„åˆ™
    logic_expression = manifest['classical_logic_rules']['expression']
    
    # è®¡ç®—æ‰€æœ‰åŒ¹é…æ ·æœ¬çš„é©¬æ°è·ç¦»
    distances = []
    total_samples = 0
    matched_samples = 0
    
    print(f"ğŸ“Š æ‰«ææ ·æœ¬æ•°æ®: {data_path}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if not line.strip():
                continue
            
            try:
                case = json.loads(line)
                total_samples += 1
                
                # é€»è¾‘è¿‡æ»¤
                if jsonLogic(logic_expression, case):
                    matched_samples += 1
                    
                    # è®¡ç®—5Då¼ é‡
                    tensor = calculate_5d_tensor(case['ten_gods'], weights_matrix, god_index_map)
                    
                    # è®¡ç®—é©¬æ°è·ç¦»
                    dist = compute_mahalanobis_distance(tensor, mean_vector, cov_matrix)
                    distances.append(dist)
                
                # è¿›åº¦æç¤º
                if line_num % 50000 == 0:
                    print(f"   è¿›åº¦: {line_num:,} è¡Œï¼ŒåŒ¹é…: {matched_samples:,}", end='\r')
                    
            except (json.JSONDecodeError, KeyError, Exception):
                continue
    
    print()  # æ¢è¡Œ
    
    print(f"âœ… æ”¶é›†åˆ° {matched_samples:,} ä¸ªåŒ¹é…æ ·æœ¬çš„é©¬æ°è·ç¦»")
    
    return distances, matched_samples


def calculate_physics_recognition_rate_with_threshold(
    pattern_id: str,
    data_path: str,
    threshold: float
) -> Tuple[float, int, int]:
    """
    ä½¿ç”¨æŒ‡å®šé˜ˆå€¼è®¡ç®—ç‰©ç†è¯†åˆ«ç‡
    
    è¿”å›: (è¯†åˆ«ç‡ç™¾åˆ†æ¯”, å‘½ä¸­æ•°, æ€»æ ·æœ¬æ•°)
    """
    # åŠ è½½æ•°æ®
    registry_data = load_registry(pattern_id)
    manifest = load_manifest(pattern_id)
    
    # æå–æµå½¢ç‰¹å¾
    fa = registry_data['data']['feature_anchors']['standard_manifold']
    mean_vector = np.array(fa['mean_vector'])
    cov_matrix = np.array(fa['covariance_matrix'])
    
    # æ„å»ºæƒé‡çŸ©é˜µ
    weights_matrix, gods_list = get_weights_matrix(manifest)
    god_index_map = {g: i for i, g in enumerate(gods_list)}
    
    # æ‰«æå…¨é‡æ ·æœ¬
    total_samples = 0
    hits = 0
    
    with open(data_path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            
            try:
                case = json.loads(line)
                if 'ten_gods' not in case:
                    continue
                
                total_samples += 1
                
                # è®¡ç®—5Då¼ é‡
                tensor = calculate_5d_tensor(case['ten_gods'], weights_matrix, god_index_map)
                
                # è®¡ç®—é©¬æ°è·ç¦»
                dist = compute_mahalanobis_distance(tensor, mean_vector, cov_matrix)
                
                # åˆ¤å®š
                if dist < threshold:
                    hits += 1
                    
            except (json.JSONDecodeError, KeyError, Exception):
                continue
    
    recognition_rate = (hits / total_samples * 100.0) if total_samples > 0 else 0.0
    return recognition_rate, hits, total_samples


def binary_search_optimal_threshold(
    pattern_id: str,
    data_path: str,
    target_abundance: float,
    search_range: Tuple[float, float] = (1.0, 3.5),
    tolerance: float = 0.01,  # ç›®æ ‡åå·®å®¹å¿åº¦ï¼ˆç™¾åˆ†æ¯”ï¼‰
    max_iterations: int = 20
) -> Tuple[float, float]:
    """
    äºŒåˆ†æ³•æœç´¢æœ€ä¼˜é˜ˆå€¼
    
    è¿”å›: (æœ€ä¼˜é˜ˆå€¼, å¯¹åº”çš„è¯†åˆ«ç‡)
    """
    print(f"\nğŸ” äºŒåˆ†æ³•æœç´¢æœ€ä¼˜é˜ˆå€¼ï¼ˆç›®æ ‡ä¸°åº¦: {target_abundance:.4f}%ï¼‰")
    print(f"   æœç´¢èŒƒå›´: [{search_range[0]:.2f}, {search_range[1]:.2f}]")
    
    low, high = search_range
    
    for iteration in range(max_iterations):
        mid = (low + high) / 2.0
        
        # ä½¿ç”¨å½“å‰é˜ˆå€¼è®¡ç®—è¯†åˆ«ç‡
        recognition_rate, _, _ = calculate_physics_recognition_rate_with_threshold(
            pattern_id, data_path, mid
        )
        
        error = abs(recognition_rate - target_abundance)
        
        print(f"   è¿­ä»£ {iteration + 1}: é˜ˆå€¼={mid:.4f}, è¯†åˆ«ç‡={recognition_rate:.4f}%, åå·®={error:.4f}%")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if error <= tolerance:
            print(f"   âœ… æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼: {mid:.4f}ï¼ˆè¯†åˆ«ç‡={recognition_rate:.4f}%ï¼Œåå·®={error:.4f}%ï¼‰")
            return mid, recognition_rate
        
        # äºŒåˆ†æ³•è°ƒæ•´
        if recognition_rate < target_abundance:
            # è¯†åˆ«ç‡è¿‡ä½ï¼Œéœ€è¦æé«˜é˜ˆå€¼ï¼ˆæ‰©å¤§èŒƒå›´ï¼ŒåŒ…å«æ›´å¤šæ ·æœ¬ï¼‰
            low = mid
        else:
            # è¯†åˆ«ç‡è¿‡é«˜ï¼Œéœ€è¦é™ä½é˜ˆå€¼ï¼ˆç¼©å°èŒƒå›´ï¼Œæ’é™¤æ›´å¤šæ ·æœ¬ï¼‰
            high = mid
        
        # æ£€æŸ¥æœç´¢èŒƒå›´æ˜¯å¦è¶³å¤Ÿå°
        if (high - low) < 0.001:
            print(f"   âš ï¸  æœç´¢èŒƒå›´å·²è¶³å¤Ÿå°ï¼Œåœæ­¢æœç´¢")
            break
    
    # ä½¿ç”¨æœ€åçš„ä¸­å€¼ä½œä¸ºæœ€ä¼˜é˜ˆå€¼
    optimal_threshold = (low + high) / 2.0
    final_rate, _, _ = calculate_physics_recognition_rate_with_threshold(
        pattern_id, data_path, optimal_threshold
    )
    
    print(f"   âœ… æœ€ç»ˆé˜ˆå€¼: {optimal_threshold:.4f}ï¼ˆè¯†åˆ«ç‡={final_rate:.4f}%ï¼Œåå·®={abs(final_rate - target_abundance):.4f}%ï¼‰")
    
    return optimal_threshold, final_rate


def update_registry_threshold(pattern_id: str, threshold: float):
    """æ›´æ–°registryæ–‡ä»¶ï¼Œæ·»åŠ æœ€ä¼˜é˜ˆå€¼"""
    registry_path = REGISTRY_DIR / f"{pattern_id}.json"
    
    with open(registry_path, 'r', encoding='utf-8') as f:
        registry_data = json.load(f)
    
    # ç¡®ä¿feature_anchorså­˜åœ¨
    if 'feature_anchors' not in registry_data['data']:
        registry_data['data']['feature_anchors'] = {}
    
    # æ›´æ–°standard_manifoldï¼Œæ·»åŠ é˜ˆå€¼
    if 'standard_manifold' not in registry_data['data']['feature_anchors']:
        registry_data['data']['feature_anchors']['standard_manifold'] = {}
    
    registry_data['data']['feature_anchors']['standard_manifold']['optimal_threshold'] = threshold
    registry_data['data']['feature_anchors']['standard_manifold']['calibration_method'] = 'binary_search'
    
    # å†™å›æ–‡ä»¶
    with open(registry_path, 'w', encoding='utf-8') as f:
        json.dump(registry_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… å·²æ›´æ–°registryæ–‡ä»¶: {registry_path}")
    print(f"   optimal_threshold = {threshold:.4f}")


def main():
    parser = argparse.ArgumentParser(
        description='FDS é˜ˆå€¼æ ¡å‡†ï¼ˆé€†å‘é˜ˆå€¼é”šå®šï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python fds_threshold_calibration.py --target A-01
  python fds_threshold_calibration.py --target A-01 --data ./data/holographic_universe_518k.jsonl
        """
    )
    
    parser.add_argument(
        '--target',
        required=True,
        help='æ ¼å±€IDï¼ˆå¦‚ A-01ï¼‰'
    )
    
    parser.add_argument(
        '--data',
        default=DEFAULT_DATA,
        help=f'æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: {DEFAULT_DATA}ï¼‰'
    )
    
    parser.add_argument(
        '--skip-distribution',
        action='store_true',
        help='è·³è¿‡è·ç¦»åˆ†å¸ƒè®¡ç®—ï¼Œç›´æ¥è¿›è¡Œé˜ˆå€¼æœç´¢'
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½registryè·å–åŸºå‡†ä¸°åº¦
        registry_data = load_registry(args.target)
        base_abundance = registry_data['data']['population_stats']['base_abundance']
        print(f"ğŸ¯ åŸºå‡†ä¸°åº¦: {base_abundance:.4f}%")
        
        # ä»»åŠ¡1ï¼šè®¡ç®—é©¬æ°è·ç¦»åˆ†å¸ƒ
        if not args.skip_distribution:
            distances, matched_count = compute_mahalanobis_distances_for_matched_samples(
                args.target, args.data
            )
            
            distances_array = np.array(distances)
            
            print(f"\nğŸ“Š é©¬æ°è·ç¦»åˆ†å¸ƒç»Ÿè®¡:")
            print(f"   æ ·æœ¬æ•°: {len(distances):,}")
            print(f"   æœ€å°å€¼: {distances_array.min():.4f}")
            print(f"   æœ€å¤§å€¼: {distances_array.max():.4f}")
            print(f"   å¹³å‡å€¼: {distances_array.mean():.4f}")
            print(f"   ä¸­ä½æ•°: {np.median(distances_array):.4f}")
            print(f"   æ ‡å‡†å·®: {distances_array.std():.4f}")
            print(f"\n   åˆ†ä½ç‚¹:")
            print(f"     25%: {np.percentile(distances_array, 25):.4f}")
            print(f"     50%: {np.percentile(distances_array, 50):.4f}")
            print(f"     75%: {np.percentile(distances_array, 75):.4f}")
            print(f"     85%: {np.percentile(distances_array, 85):.4f}")
            print(f"     90%: {np.percentile(distances_array, 90):.4f}")
            print(f"     95%: {np.percentile(distances_array, 95):.4f}")
            print(f"     99%: {np.percentile(distances_array, 99):.4f}")
        
        # ä»»åŠ¡2ï¼šäºŒåˆ†æ³•æœç´¢æœ€ä¼˜é˜ˆå€¼
        optimal_threshold, optimal_rate = binary_search_optimal_threshold(
            args.target,
            args.data,
            base_abundance,
            search_range=(1.0, 3.5),
            tolerance=0.01,  # 1%å®¹å¿åº¦
            max_iterations=20
        )
        
        # ä»»åŠ¡3ï¼šæ›´æ–°registry
        update_registry_threshold(args.target, optimal_threshold)
        
        # ä»»åŠ¡4ï¼šæœ€ç»ˆæŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“‹ é˜ˆå€¼æ ¡å‡†æŠ¥å‘Š")
        print("=" * 60)
        print(f"æ ¼å±€ID:        {args.target}")
        print(f"åŸºå‡†ä¸°åº¦:      {base_abundance:.4f}%")
        print(f"æœ€ä¼˜é˜ˆå€¼:      {optimal_threshold:.4f}")
        print(f"è¯†åˆ«ç‡:        {optimal_rate:.4f}%")
        print(f"ç»å¯¹åå·®:      {abs(optimal_rate - base_abundance):.4f}%")
        print(f"ç›¸å¯¹åå·®:      {abs(optimal_rate - base_abundance) / base_abundance * 100:.2f}%")
        print("=" * 60)
        
        print("\nâœ… é˜ˆå€¼æ ¡å‡†å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

