
import sys
import os
import json
import logging
import time
import numpy as np
from datetime import datetime

sys.path.insert(0, os.getcwd())

from core.logic_compiler import get_knowledge_census
from core.protocol_checker import LOGIC_PROTOCOLS

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Constants
BASE_PATTERNS = ["A-01", "A-02", "A-03", "B-01", "B-02", "C-01", "C-02", "D-01", "D-02"]
BRANCHES = ['å­', 'ä¸‘', 'å¯…', 'å¯', 'è¾°', 'å·³', 'åˆ', 'æœª', 'ç”³', 'é…‰', 'æˆŒ', 'äº¥']
BRANCH_EN_MAP = {
    'å­': 'Rat', 'ä¸‘': 'Ox', 'å¯…': 'Tiger', 'å¯': 'Rabbit', 'è¾°': 'Dragon', 'å·³': 'Snake',
    'åˆ': 'Horse', 'æœª': 'Goat', 'ç”³': 'Monkey', 'é…‰': 'Rooster', 'æˆŒ': 'Dog', 'äº¥': 'Pig'
}

BATCH_LIMIT = 50000  # Scan limit per task
REPORT_PATH = "census_preview_report.md"

def run_full_census():
    """
    Execute Full Universe Census for 108 Patterns.
    """
    start_time = time.time()
    census = get_knowledge_census()
    from core.census_cache import get_census_cache
    cache = get_census_cache()

    results = []
    
    print("ğŸš€ Starting FULL UNIVERSE CENSUS [108 Patterns]...")
    total_tasks = len(BASE_PATTERNS) * len(BRANCHES)
    completed = 0
    
    for base in BASE_PATTERNS:
        base_proto = LOGIC_PROTOCOLS.get(base, {})
        base_name = base_proto.get('name', base)
        
        for branch in BRANCHES:
            target_id = f"{base}@{branch}"
            branch_en = BRANCH_EN_MAP.get(branch, branch)
            
            logger.info(f"[{completed+1}/{total_tasks}] Scanning {target_id} ({base_name} in {branch_en})...")
            
            try:
                # 1. Execute Census
                res = census.request_census(target_id, limit=BATCH_LIMIT, include_tensor=True)
                
                # 2. Cache Result (calculates physics)
                cache_res = cache.cache_census_result(
                    target_id, 
                    res['samples'], 
                    {'name': f"{base_name} @ {branch}"}
                )
                
                # 3. Analyze Physics Confidence
                # We use Abundance and Trace of Covariance (Stability) as proxy
                samples_count = res['matched_count']
                abundance = res['abundance']
                
                stability_score = 0.0
                confidence_score = 0.0
                
                # Get the cached object to see covariance
                cached_obj = cache.get_cached_manifold(target_id)
                if cached_obj:
                    cov = np.array(cached_obj.get('covariance', []))
                    if cov.shape == (5,5):
                        trace_cov = np.trace(cov)
                        # Lower trace = more compact = higher stability
                        stability_score = 1.0 / (trace_cov + 1e-5) 
                        # Mock confidence score
                        confidence_score = min(0.99, stability_score * 0.1 * abundance * 1000)

                results.append({
                    "id": target_id,
                    "name": f"{base_name} @ {branch}",
                    "samples": samples_count,
                    "abundance": abundance,
                    "stability": stability_score,
                    "confidence": confidence_score,
                    "mean_E": cached_obj.get("mean_vector", [0]*5)[0] if cached_obj else 0
                })
                
            except Exception as e:
                logger.error(f"Failed {target_id}: {e}")
            
            completed += 1
            
    elapsed = time.time() - start_time
    generate_report(results, elapsed)
    print(f"\nâœ… Census Completed in {elapsed:.2f}s. Report generated at {REPORT_PATH}")

def generate_report(results, elapsed):
    """Generate Markdown report."""
    
    # Sort by Confidence (Stability)
    sorted_by_conf = sorted(results, key=lambda x: x['confidence'], reverse=True)
    # Sort by Abundance
    sorted_by_abun = sorted(results, key=lambda x: x['abundance'], reverse=True)
    
    top_3_conf = sorted_by_conf[:3]
    bottom_3_conf = sorted_by_conf[-3:]
    
    top_3_abun = sorted_by_abun[:3]
    
    md = f"""# ğŸŒŒ å…¨é‡å®‡å®™æ™®æŸ¥æŠ¥å‘Š (Census Preview)

**Run ID**: FULL_UNIVERSE_BATCH_001
**Time**: {datetime.now().isoformat()}
**Total Patterns**: 108
**Execution Time**: {elapsed:.2f}s

## ğŸ† é¢†å¥–å° (The Podium)

### ğŸ¥‡ ç‰©ç†ç½®ä¿¡åº¦æœ€é«˜ (Most Stable Manifolds)
è¿™äº›æ ¼å±€åœ¨ 5D ç©ºé—´ä¸­èšç±»æœ€ç´§å¯†ï¼Œç‰©ç†å®šä¹‰æœ€ç¨³å›ºã€‚

| æ ¼å±€ ID | åç§° | æ ·æœ¬æ•° | ç¨³å®šæ€§ (1/Tr) | ä¸°åº¦ |
| :--- | :--- | :--- | :--- | :--- |
"""
    for r in top_3_conf:
        md += f"| **{r['id']}** | {r['name']} | {r['samples']} | {r['stability']:.2f} | {r['abundance']:.4f} |\n"
        
    md += """
### ğŸ‘» é‡å­å¹½çµ (Quantum Ghosts)
è¿™äº›æ ¼å±€æå…¶ç¨€ç¼ºæˆ–ç‰©ç†æå…¶å‘æ•£ï¼Œéš¾ä»¥è¢«ç°æœ‰ç‰©ç†å¼•æ“æ•æ‰ã€‚

| æ ¼å±€ ID | åç§° | æ ·æœ¬æ•° | ç¨³å®šæ€§ | ä¸°åº¦ |
| :--- | :--- | :--- | :--- | :--- |
"""
    for r in bottom_3_conf:
        md += f"| `{r['id']}` | {r['name']} | {r['samples']} | {r['stability']:.2f} | {r['abundance']:.6f} |\n"

    md += """
## ğŸš¨ ä¸°åº¦å¼‚å¸¸æŠ¥è­¦ (Abundance Anomalies)
ä»¥ä¸‹æ ¼å±€è¦†ç›–ç‡è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨â€œå¤å…¸å®šä¹‰è¿‡å®½â€çš„é—®é¢˜ã€‚

| æ ¼å±€ ID | åç§° | ä¸°åº¦ | æ ·æœ¬æ•° |
| :--- | :--- | :--- | :--- |
"""
    for r in top_3_abun:
        md += f"| âš ï¸ {r['id']} | {r['name']} | **{r['abundance']:.4f}** | {r['samples']} |\n"

    md += """
## ğŸ“Š ç»Ÿè®¡æ‘˜è¦
- **æ€»æ‰«ææ ·æœ¬**: ~51.8k * 108 (Mock Limit per batch {BATCH_LIMIT})
- **å¹³å‡ä¸°åº¦**: {sum(r['abundance'] for r in results)/len(results):.6f}
- **å¹³å‡ç¨³å®šæ€§**: {sum(r['stability'] for r in results)/len(results):.2f}

"""
    with open(REPORT_PATH, 'w', encoding='utf-8') as f:
        f.write(md)

if __name__ == "__main__":
    run_full_census()
