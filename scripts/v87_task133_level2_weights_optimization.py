"""
V87.0 ä»»åŠ¡ 133ï¼šLevel 2 æƒé‡ç»“æ„æ€§ä¿®å¤ä¸ä¼˜åŒ–
==========================================
ç›®æ ‡ï¼šä¿®å¤ Level 2 æƒé‡é…ç½®çš„ç†è®ºç¼ºé™·ï¼ˆç‰¹åˆ«æ˜¯ Relationship ç»´åº¦ï¼‰ï¼Œ
      é€šè¿‡ä¼˜åŒ– Level 2 æƒé‡ï¼Œå°†æ€» MAE é™è‡³ 5.0 ä»¥ä¸‹ã€‚

ç­–ç•¥ï¼š
1. é”å®š Level 1 å‚æ•°ä¸º V80.0 æœ€ä¼˜å€¼
2. ä¼˜åŒ– Level 2 æƒé‡ï¼ˆRelationship_bias, Wealth_exp, Relationship_max_score ç­‰ï¼‰
3. ä½¿ç”¨æ‰€æœ‰ 24 ä¸ªè¯¯å·®ç‚¹ï¼ˆ8ä¸ªæ¡ˆä¾‹ Ã— 3ä¸ªç»´åº¦ï¼‰è¿›è¡Œä¼˜åŒ–
"""

import sys
import os
import json
import io
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from copy import deepcopy
from datetime import datetime

# Fix encoding issue
import locale
try:
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
except:
    pass

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.engine_v88 import EngineV88


class V87Level2Optimizer:
    """
    V87.0 Level 2 æƒé‡ä¼˜åŒ–å™¨
    
    ä¸“é—¨ä¼˜åŒ– Level 2 æƒé‡é…ç½®ï¼Œé”å®š Level 1 å‚æ•°
    """
    
    def __init__(self, config_path: str, cases_path: str = None, optimal_level1_params: Dict = None):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            cases_path: æ ¡å‡†æ¡ˆä¾‹è·¯å¾„
            optimal_level1_params: V80.0 æœ€ä¼˜ Level 1 å‚æ•°ï¼ˆç”¨äºé”å®šï¼‰
        """
        self.config_path = config_path
        self.cases_path = cases_path
        
        # åŠ è½½é…ç½®
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.base_config = json.load(f)
        
        # åŠ è½½ V80.0 æœ€ä¼˜ Level 1 å‚æ•°
        if optimal_level1_params:
            self.level1_params = optimal_level1_params
        else:
            self.level1_params = self._load_optimal_level1_params()
        
        # åº”ç”¨ Level 1 å‚æ•°åˆ°é…ç½®ï¼ˆé”å®šï¼‰
        self.base_config = self._apply_level1_params(self.base_config, self.level1_params)
        
        # åŠ è½½æ ¡å‡†æ¡ˆä¾‹
        self.cases = []
        if cases_path and os.path.exists(cases_path):
            with open(cases_path, 'r', encoding='utf-8') as f:
                self.cases = json.load(f)
            print(f"âœ… åŠ è½½äº† {len(self.cases)} ä¸ªæ ¡å‡†æ¡ˆä¾‹")
        
        # æ­£åˆ™åŒ–ç³»æ•°
        self.lambda_reg = 0.01
        
        # å­¦ä¹ ç‡
        self.learning_rate = 0.05
        
        # æ”¶æ•›é˜ˆå€¼
        self.mae_target = 5.0
        self.mae_change_threshold = 0.01
        self.convergence_window = 5
        
        # å®šä¹‰ Level 2 å‚æ•°é›†
        self.level2_params = self._define_level2_params()
        
        # ä¼˜åŒ–å†å²
        self.optimization_history = []
        
        print(f"âœ… Level 2 ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   Level 2 å‚æ•°æ•°é‡: {len(self.level2_params)}")
        print(f"   Level 1 å‚æ•°å·²é”å®š: {len(self.level1_params)} ä¸ª")
        print(f"   æ­£åˆ™åŒ–ç³»æ•° Î»: {self.lambda_reg}")
        print(f"   å­¦ä¹ ç‡: {self.learning_rate}")
    
    def _load_optimal_level1_params(self) -> Dict:
        """
        åŠ è½½ V80.0 æœ€ä¼˜ Level 1 å‚æ•°
        """
        result_files = []
        docs_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "docs")
        if os.path.exists(docs_dir):
            for f in os.listdir(docs_dir):
                if f.startswith("V79_OPTIMIZATION_RESULT_") and f.endswith(".json"):
                    result_files.append(os.path.join(docs_dir, f))
        
        if result_files:
            latest_file = max(result_files, key=os.path.getmtime)
            with open(latest_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
                return result.get('best_params', {})
        
        return {}
    
    def _apply_level1_params(self, config: Dict, level1_params: Dict) -> Dict:
        """
        åº”ç”¨ Level 1 å‚æ•°åˆ°é…ç½®ï¼ˆé”å®šï¼‰
        """
        config = deepcopy(config)
        
        # åº”ç”¨æŸ±ä½æƒé‡
        if 'pg_year' in level1_params:
            config.setdefault('physics', {}).setdefault('pillarWeights', {})['year'] = level1_params['pg_year']
        if 'pg_month' in level1_params:
            config.setdefault('physics', {}).setdefault('pillarWeights', {})['month'] = level1_params['pg_month']
        if 'pg_day' in level1_params:
            config.setdefault('physics', {}).setdefault('pillarWeights', {})['day'] = level1_params['pg_day']
        if 'pg_hour' in level1_params:
            config.setdefault('physics', {}).setdefault('pillarWeights', {})['hour'] = level1_params['pg_hour']
        
        # åº”ç”¨ flow å‚æ•°
        if 'ctl_imp' in level1_params:
            config.setdefault('flow', {})['controlImpact'] = level1_params['ctl_imp']
        if 'imp_base' in level1_params:
            config.setdefault('flow', {}).setdefault('resourceImpedance', {})['base'] = level1_params['imp_base']
        
        # åº”ç”¨ interactions å‚æ•°
        if 'clashScore' in level1_params:
            config.setdefault('interactions', {}).setdefault('branchEvents', {})['clashScore'] = level1_params['clashScore']
        if 'harmPenalty' in level1_params:
            config.setdefault('interactions', {}).setdefault('branchEvents', {})['harmPenalty'] = level1_params['harmPenalty']
        if 'punishmentPenalty' in level1_params:
            config.setdefault('interactions', {}).setdefault('branchEvents', {})['punishmentPenalty'] = level1_params['punishmentPenalty']
        
        return config
    
    def _define_level2_params(self) -> Dict[str, Dict]:
        """
        å®šä¹‰ Level 2 å‚æ•°é›†
        
        V87.0: ä¼˜åŒ– Level 2 æƒé‡ï¼Œç‰¹åˆ«æ˜¯ Relationship ç»´åº¦
        """
        config = self.base_config
        
        params = {}
        
        # 1. Relationship è§‚å¯Ÿåå·®å› å­ï¼ˆæœ€é‡è¦ï¼‰
        observation_bias = config.get('ObservationBiasFactor', {})
        params['relationship_bias'] = {
            'value': observation_bias.get('Relationship', 3.0),
            'anchor': 1.8,  # V87.0: ä» 3.0 ä¿®æ­£ä¸º 1.8
            'range': (0.5, 3.0),
            'category': 'Relationship'
        }
        
        # 2. Wealth è§‚å¯Ÿåå·®å› å­
        params['wealth_bias'] = {
            'value': observation_bias.get('Wealth', 2.7),
            'anchor': 2.7,  # ä¿æŒåŸå€¼
            'range': (1.0, 4.0),
            'category': 'Wealth'
        }
        
        # 3. Career è§‚å¯Ÿåå·®å› å­ï¼ˆä½èƒ½é‡ï¼‰
        params['career_bias_low'] = {
            'value': observation_bias.get('CareerBiasFactor_LowE', 2.0),
            'anchor': 2.0,  # ä¿æŒåŸå€¼
            'range': (1.0, 3.0),
            'category': 'Career'
        }
        
        # 4. Career è§‚å¯Ÿåå·®å› å­ï¼ˆé«˜èƒ½é‡ï¼‰
        params['career_bias_high'] = {
            'value': observation_bias.get('CareerBiasFactor_HighE', 0.95),
            'anchor': 0.95,  # ä¿æŒåŸå€¼
            'range': (0.5, 2.0),
            'category': 'Career'
        }
        
        # 5. Wealth éçº¿æ€§æŒ‡æ•°ï¼ˆé«˜èƒ½é‡åŒºåŸŸï¼‰
        physics_config = config.get('physics', {})
        params['wealth_exp_high'] = {
            'value': physics_config.get('NonLinearExponent_High', 2.0),
            'anchor': 1.4,  # V87.0: ä» 2.0 ä¿®æ­£ä¸º 1.4
            'range': (1.0, 2.5),
            'category': 'Wealth'
        }
        
        # 6. Wealth éçº¿æ€§æŒ‡æ•°ï¼ˆä¸­èƒ½é‡åŒºåŸŸï¼‰
        params['wealth_exp_mid'] = {
            'value': physics_config.get('NonLinearExponent_Mid', 1.3),
            'anchor': 1.3,  # ä¿æŒåŸå€¼
            'range': (1.0, 2.0),
            'category': 'Wealth'
        }
        
        # 7. Relationship æœ€å¤§å¾—åˆ†
        params['relationship_max_score'] = {
            'value': physics_config.get('RelationshipMaxScore', 75.0),
            'anchor': 98.0,  # V87.0: ä» 75.0 ä¿®æ­£ä¸º 98.0
            'range': (50.0, 100.0),
            'category': 'Relationship'
        }
        
        # 8. Wealth æ”¾å¤§å™¨
        params['wealth_amplifier'] = {
            'value': physics_config.get('WealthAmplifier', 1.2),
            'anchor': 1.2,  # ä¿æŒåŸå€¼
            'range': (0.8, 2.0),
            'category': 'Wealth'
        }
        
        # 9. Career æ”¾å¤§å™¨
        params['career_amplifier'] = {
            'value': physics_config.get('CareerAmplifier', 1.2),
            'anchor': 1.2,  # ä¿æŒåŸå€¼
            'range': (0.8, 2.0),
            'category': 'Career'
        }
        
        # 10. Relationship æ”¾å¤§å™¨
        params['relationship_amplifier'] = {
            'value': physics_config.get('RelationshipAmplifier', 1.0),
            'anchor': 1.0,  # ä¿æŒåŸå€¼
            'range': (0.5, 2.0),
            'category': 'Relationship'
        }
        
        # 11. Wealth æœ€å¤§å¾—åˆ†
        params['wealth_max_score'] = {
            'value': physics_config.get('MaxScore', 98),
            'anchor': 98,  # ä¿æŒåŸå€¼
            'range': (80, 120),
            'category': 'Wealth'
        }
        
        # 12. Career æœ€å¤§å¾—åˆ†
        params['career_max_score'] = {
            'value': physics_config.get('CareerMaxScore', 98.0),
            'anchor': 98.0,  # ä¿æŒåŸå€¼
            'range': (80.0, 120.0),
            'category': 'Career'
        }
        
        # 13. é«˜èƒ½é‡é˜ˆå€¼
        params['high_energy_threshold'] = {
            'value': physics_config.get('HighEnergyThreshold', 55),
            'anchor': 55,  # ä¿æŒåŸå€¼
            'range': (40, 70),
            'category': 'Thresholds'
        }
        
        # 14. ä¸­èƒ½é‡é˜ˆå€¼
        params['mid_energy_threshold'] = {
            'value': physics_config.get('MidEnergyThreshold', 30),
            'anchor': 30,  # ä¿æŒåŸå€¼
            'range': (20, 50),
            'category': 'Thresholds'
        }
        
        return params
    
    def _apply_level2_params_to_config(self, params: Dict[str, float]) -> Dict:
        """
        å°† Level 2 å‚æ•°åº”ç”¨åˆ°é…ç½®
        
        Args:
            params: Level 2 å‚æ•°å­—å…¸
            
        Returns:
            æ›´æ–°åçš„é…ç½®
        """
        config = deepcopy(self.base_config)
        
        # åº”ç”¨è§‚å¯Ÿåå·®å› å­
        if 'relationship_bias' in params:
            config.setdefault('ObservationBiasFactor', {})['Relationship'] = params['relationship_bias']
        if 'wealth_bias' in params:
            config.setdefault('ObservationBiasFactor', {})['Wealth'] = params['wealth_bias']
        if 'career_bias_low' in params:
            config.setdefault('ObservationBiasFactor', {})['CareerBiasFactor_LowE'] = params['career_bias_low']
        if 'career_bias_high' in params:
            config.setdefault('ObservationBiasFactor', {})['CareerBiasFactor_HighE'] = params['career_bias_high']
        
        # åº”ç”¨ç‰©ç†é…ç½®
        if 'wealth_exp_high' in params:
            config.setdefault('physics', {})['NonLinearExponent_High'] = params['wealth_exp_high']
        if 'wealth_exp_mid' in params:
            config.setdefault('physics', {})['NonLinearExponent_Mid'] = params['wealth_exp_mid']
        if 'relationship_max_score' in params:
            config.setdefault('physics', {})['RelationshipMaxScore'] = params['relationship_max_score']
        if 'wealth_amplifier' in params:
            config.setdefault('physics', {})['WealthAmplifier'] = params['wealth_amplifier']
        if 'career_amplifier' in params:
            config.setdefault('physics', {})['CareerAmplifier'] = params['career_amplifier']
        if 'relationship_amplifier' in params:
            config.setdefault('physics', {})['RelationshipAmplifier'] = params['relationship_amplifier']
        if 'wealth_max_score' in params:
            config.setdefault('physics', {})['MaxScore'] = params['wealth_max_score']
        if 'career_max_score' in params:
            config.setdefault('physics', {})['CareerMaxScore'] = params['career_max_score']
        if 'high_energy_threshold' in params:
            config.setdefault('physics', {})['HighEnergyThreshold'] = params['high_energy_threshold']
        if 'mid_energy_threshold' in params:
            config.setdefault('physics', {})['MidEnergyThreshold'] = params['mid_energy_threshold']
        
        return config
    
    def _calculate_mae(self, config: Dict) -> Tuple[float, Dict]:
        """
        è®¡ç®— MAEï¼ˆä½¿ç”¨æ‰€æœ‰ 24 ä¸ªè¯¯å·®ç‚¹ï¼š8ä¸ªæ¡ˆä¾‹ Ã— 3ä¸ªç»´åº¦ï¼‰
        
        Returns:
            (MAE, è¯¦ç»†ç»“æœ)
        """
        if not self.cases:
            return 999.0, {}
        
        engine = EngineV88(config=config)
        errors = []
        detailed_results = []
        
        for case in self.cases:
            case_id = case.get('id', 'Unknown')
            bazi = case.get('bazi', [])
            day_master = case.get('day_master', '')
            
            if not bazi or not day_master:
                continue
            
            # å¤„ç† gender
            gender = case.get('gender', 1)
            if isinstance(gender, str):
                gender = 1 if gender == 'ç”·' or gender == 'male' else 0
            
            case_data = {
                'year': bazi[0] if len(bazi) > 0 else '',
                'month': bazi[1] if len(bazi) > 1 else '',
                'day': bazi[2] if len(bazi) > 2 else '',
                'hour': bazi[3] if len(bazi) > 3 else '',
                'day_master': day_master,
                'gender': gender,
                'case_id': case_id
            }
            
            # å¤„ç†åŠ¨æ€ä¸Šä¸‹æ–‡
            d_ctx = {"year": "2024", "luck": "default"}
            target_v = case.get('ground_truth', case.get('v_real', {}))
            if case.get("dynamic_checks"):
                p = case["dynamic_checks"][0]
                d_ctx = {"year": p.get('year', "2024"), "luck": p.get('luck', "default")}
                if 'v_real_dynamic' in p:
                    target_v = p['v_real_dynamic']
            
            # è®¡ç®—å¾—åˆ†
            try:
                result = engine.calculate_energy(case_data, d_ctx)
                
                if not isinstance(result, dict):
                    continue
                if 'career' not in result or 'wealth' not in result or 'relationship' not in result:
                    continue
            except Exception as e:
                continue
            
            # è®¡ç®—è¯¯å·®ï¼ˆæ‰€æœ‰ 3 ä¸ªç»´åº¦ï¼‰
            for dimension in ['career', 'wealth', 'relationship']:
                # å°è¯•å¤šç§å¯èƒ½çš„é”®å
                gt_value = None
                for key in [f'{dimension}_score', dimension, f'{dimension}_gt']:
                    if key in target_v:
                        gt_value = target_v[key]
                        break
                
                if gt_value is None or gt_value == 0:
                    continue
                
                # ä» domain_details ä¸­æå–åŸå§‹å¾—åˆ†ï¼ˆ0-100 èŒƒå›´ï¼‰
                pred_value = 0.0
                domain_details = result.get('domain_details', {})
                if domain_details and dimension in domain_details:
                    pred_value = domain_details[dimension].get('score', 0)
                else:
                    # å¦‚æœæ²¡æœ‰ domain_detailsï¼Œä½¿ç”¨ result ä¸­çš„å€¼ï¼ˆ0-10 èŒƒå›´ï¼‰ä¹˜ä»¥ 10
                    pred_raw = result.get(dimension, 0)
                    pred_value = pred_raw * 10.0 if pred_raw < 20 else pred_raw
                
                error = abs(pred_value - gt_value)
                errors.append(error)
                detailed_results.append({
                    'case_id': case_id,
                    'dimension': dimension,
                    'gt': gt_value,
                    'pred': pred_value,
                    'error': error
                })
        
        mae = np.mean(errors) if errors else 999.0
        return mae, {'errors': errors, 'detailed': detailed_results}
    
    def _calculate_regularization_penalty(self, params: Dict[str, float]) -> float:
        """
        è®¡ç®—æ­£åˆ™åŒ–æƒ©ç½šé¡¹
        
        Args:
            params: Level 2 å‚æ•°å­—å…¸
            
        Returns:
            æ­£åˆ™åŒ–æƒ©ç½šå€¼
        """
        penalty = 0.0
        
        for param_name, param_value in params.items():
            if param_name in self.level2_params:
                anchor_value = self.level2_params[param_name]['anchor']
                deviation = param_value - anchor_value
                penalty += (deviation ** 2)
        
        return self.lambda_reg * penalty
    
    def _calculate_total_cost(self, params: Dict[str, float]) -> Tuple[float, float, float]:
        """
        è®¡ç®—æ€»æˆæœ¬
        
        Args:
            params: Level 2 å‚æ•°å­—å…¸
            
        Returns:
            (Cost_Total, Cost_MAE, Cost_Plausibility)
        """
        # åº”ç”¨å‚æ•°åˆ°é…ç½®
        config = self._apply_level2_params_to_config(params)
        
        # è®¡ç®— MAE
        mae, _ = self._calculate_mae(config)
        cost_mae = mae
        
        # è®¡ç®—æ­£åˆ™åŒ–æƒ©ç½š
        cost_plausibility = self._calculate_regularization_penalty(params)
        
        # æ€»æˆæœ¬
        cost_total = cost_mae + cost_plausibility
        
        return cost_total, cost_mae, cost_plausibility
    
    def _calculate_gradient(self, params: Dict[str, float], param_name: str) -> float:
        """
        è®¡ç®—æ¢¯åº¦ï¼ˆåå¯¼æ•°ï¼‰
        
        Args:
            params: å½“å‰å‚æ•°å­—å…¸
            param_name: å‚æ•°å
            
        Returns:
            æ¢¯åº¦å€¼
        """
        if param_name not in self.level2_params:
            return 0.0
        
        param_info = self.level2_params[param_name]
        current_value = params[param_name]
        param_range = param_info['range']
        
        # æ•°å€¼æ¢¯åº¦è®¡ç®—ï¼ˆä¸­å¿ƒå·®åˆ†ï¼‰
        epsilon = 0.01
        
        # æ­£å‘æ‰°åŠ¨
        temp_params_plus = params.copy()
        temp_params_plus[param_name] = min(current_value + epsilon, param_range[1])
        cost_plus, _, _ = self._calculate_total_cost(temp_params_plus)
        
        # è´Ÿå‘æ‰°åŠ¨
        temp_params_minus = params.copy()
        temp_params_minus[param_name] = max(current_value - epsilon, param_range[0])
        cost_minus, _, _ = self._calculate_total_cost(temp_params_minus)
        
        # æ¢¯åº¦
        gradient = (cost_plus - cost_minus) / (2 * epsilon)
        
        return gradient
    
    def _update_parameters(self, params: Dict[str, float]) -> Dict[str, float]:
        """
        æ›´æ–°å‚æ•°ï¼ˆæ²¿è´Ÿæ¢¯åº¦æ–¹å‘ï¼‰
        
        Args:
            params: å½“å‰å‚æ•°å­—å…¸
            
        Returns:
            æ›´æ–°åçš„å‚æ•°å­—å…¸
        """
        updated_params = params.copy()
        
        # è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦å¹¶æ›´æ–°
        for param_name in self.level2_params.keys():
            param_info = self.level2_params[param_name]
            current_value = params[param_name]
            param_range = param_info['range']
            
            # è®¡ç®—æ¢¯åº¦
            gradient = self._calculate_gradient(params, param_name)
            
            # æ²¿è´Ÿæ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•°
            new_value = current_value - self.learning_rate * gradient
            
            # èŒƒå›´ç¡¬çº¦æŸ
            new_value = max(param_range[0], min(param_range[1], new_value))
            
            updated_params[param_name] = new_value
        
        return updated_params
    
    def _check_convergence(self, history: List[Dict]) -> Tuple[bool, str]:
        """
        æ”¶æ•›åˆ¤å®š
        
        Args:
            history: ä¼˜åŒ–å†å²
            
        Returns:
            (æ˜¯å¦æ”¶æ•›, æ”¶æ•›åŸå› )
        """
        if len(history) < self.convergence_window:
            return False, ""
        
        # æ£€æŸ¥ç›®æ ‡è¾¾æˆ
        recent_maes = [h['mae'] for h in history[-self.convergence_window:]]
        if all(mae < self.mae_target for mae in recent_maes):
            return True, f"ç›®æ ‡è¾¾æˆï¼šMAE æŒç»­ä½äº {self.mae_target}"
        
        # æ£€æŸ¥å˜åŒ–å¾®å°
        mae_changes = [abs(recent_maes[i] - recent_maes[i-1]) 
                      for i in range(1, len(recent_maes))]
        if all(change < self.mae_change_threshold for change in mae_changes):
            return True, f"å˜åŒ–å¾®å°ï¼šè¿ç»­ {self.convergence_window} æ¬¡è¿­ä»£ä¸­ MAE å˜åŒ–é‡ä½äº {self.mae_change_threshold}"
        
        return False, ""
    
    def optimize(self, max_iterations: int = 500) -> Dict:
        """
        æ‰§è¡Œ Level 2 æƒé‡ä¼˜åŒ–
        
        Args:
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        print("=" * 80)
        print("V87.0 ä»»åŠ¡ 133ï¼šLevel 2 æƒé‡ç»“æ„æ€§ä¿®å¤ä¸ä¼˜åŒ–")
        print("=" * 80)
        
        print(f"\nä¼˜åŒ–é…ç½®:")
        print(f"  æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}")
        print(f"  å­¦ä¹ ç‡: {self.learning_rate}")
        print(f"  æ­£åˆ™åŒ–ç³»æ•° Î»: {self.lambda_reg}")
        print(f"  ä¼˜åŒ–å‚æ•°æ•°é‡: {len(self.level2_params)}")
        print(f"  æ ¡å‡†æ¡ˆä¾‹æ•°é‡: {len(self.cases)}")
        print(f"  è¯¯å·®ç‚¹æ•°é‡: {len(self.cases) * 3} (8ä¸ªæ¡ˆä¾‹ Ã— 3ä¸ªç»´åº¦)")
        print(f"  ç›®æ ‡ MAE: < {self.mae_target}")
        print(f"  Level 1 å‚æ•°å·²é”å®š: {len(self.level1_params)} ä¸ª")
        
        # åˆå§‹åŒ– Level 2 å‚æ•°å€¼
        current_params = {name: info['value'] for name, info in self.level2_params.items()}
        
        # è®¡ç®—åˆå§‹æˆæœ¬
        initial_cost_total, initial_mae, initial_reg = self._calculate_total_cost(current_params)
        print(f"\næ­¥éª¤ä¸€ï¼šå‰ç½®å‡†å¤‡å®Œæˆ")
        print(f"  åˆå§‹ MAE: {initial_mae:.4f}")
        print(f"  åˆå§‹æ­£åˆ™åŒ–æˆæœ¬: {initial_reg:.4f}")
        print(f"  åˆå§‹æ€»æˆæœ¬: {initial_cost_total:.4f}")
        
        best_mae = initial_mae
        best_params = current_params.copy()
        best_cost_total = initial_cost_total
        
        # ä¼˜åŒ–è¿­ä»£
        for iteration in range(max_iterations):
            # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
            current_params = self._update_parameters(current_params)
            
            # è®¡ç®—å½“å‰æˆæœ¬
            cost_total, mae, reg_penalty = self._calculate_total_cost(current_params)
            
            # è®°å½•å†å²
            self.optimization_history.append({
                'iteration': iteration + 1,
                'mae': mae,
                'cost_total': cost_total,
                'cost_mae': mae,
                'cost_plausibility': reg_penalty,
                'params': current_params.copy()
            })
            
            # æ›´æ–°æœ€ä½³å€¼
            if mae < best_mae:
                best_mae = mae
                best_params = current_params.copy()
                best_cost_total = cost_total
                improved = True
            else:
                improved = False
            
            # è¾“å‡ºè¿›åº¦
            if (iteration + 1) % 10 == 0 or improved or iteration == 0:
                print(f"\nè¿­ä»£ {iteration + 1}/{max_iterations}:")
                print(f"  å½“å‰ MAE: {mae:.4f}")
                print(f"  å½“å‰æ€»æˆæœ¬: {cost_total:.4f}")
                print(f"  æ­£åˆ™åŒ–æˆæœ¬: {reg_penalty:.4f}")
                print(f"  æœ€ä½³ MAE: {best_mae:.4f}")
                if improved:
                    print(f"  âœ… å‘ç°æ›´ä¼˜è§£ï¼")
            
            # æ”¶æ•›æ£€æŸ¥
            is_converged, reason = self._check_convergence(self.optimization_history)
            if is_converged:
                print(f"\nğŸ‰ æ”¶æ•›è¾¾æˆï¼")
                print(f"  åŸå› : {reason}")
                break
        
        # æœ€ç»ˆæŠ¥å‘Š
        final_config = self._apply_level2_params_to_config(best_params)
        final_mae, final_details = self._calculate_mae(final_config)
        
        print(f"\n" + "=" * 80)
        print("æ­¥éª¤äº”ï¼šä¼˜åŒ–å®Œæˆ - æœ€ç»ˆæŠ¥å‘Š")
        print("=" * 80)
        print(f"\næœ€ç»ˆç»“æœ:")
        print(f"  æœ€ä½³ MAE: {best_mae:.4f}")
        print(f"  ç›®æ ‡: MAE < {self.mae_target}")
        print(f"  çŠ¶æ€: {'âœ… è¾¾æˆ' if best_mae < self.mae_target else 'âŒ æœªè¾¾æˆ'}")
        print(f"  æ€»è¿­ä»£æ¬¡æ•°: {len(self.optimization_history)}")
        print(f"  MAE æ”¹å–„: {initial_mae - best_mae:.4f}")
        
        # è¾“å‡ºæœ€ä¼˜å‚æ•°æ‘˜è¦
        print(f"\næœ€ä¼˜ Level 2 å‚æ•°æ‘˜è¦:")
        param_changes = []
        for param_name in self.level2_params.keys():
            anchor = self.level2_params[param_name]['anchor']
            optimal = best_params[param_name]
            change = abs(optimal - anchor)
            if change > 0.001:
                param_changes.append((param_name, anchor, optimal, change))
        
        param_changes.sort(key=lambda x: x[3], reverse=True)
        for param_name, anchor, optimal, change in param_changes:
            print(f"  {param_name}: {anchor:.4f} â†’ {optimal:.4f} (å˜åŒ–: {change:.4f})")
        
        return {
            'best_mae': best_mae,
            'best_level2_params': best_params,
            'best_config': final_config,
            'level1_params_locked': self.level1_params,
            'initial_mae': initial_mae,
            'improvement': initial_mae - best_mae,
            'iterations': len(self.optimization_history),
            'history': self.optimization_history,
            'final_details': final_details,
            'converged': is_converged if 'is_converged' in locals() else False,
            'convergence_reason': reason if 'is_converged' in locals() and is_converged else "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°"
        }


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        "config", "parameters.json"
    )
    
    # æ ¡å‡†æ¡ˆä¾‹è·¯å¾„
    possible_paths = [
        os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
                    "data", "calibration_cases.json"),
        os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
                    "calibration_cases.json"),
        "calibration_cases.json"
    ]
    cases_path = None
    for path in possible_paths:
        if os.path.exists(path):
            cases_path = path
            break
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = V87Level2Optimizer(config_path, cases_path)
    
    # æ‰§è¡Œä¼˜åŒ–
    result = optimizer.optimize(max_iterations=500)
    
    # ä¿å­˜ç»“æœ
    output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "docs")
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = os.path.join(output_dir, f"V87_TASK133_LEVEL2_OPTIMIZATION_RESULT_{timestamp}.json")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nä¼˜åŒ–ç»“æœå·²ä¿å­˜è‡³: {output_path}")


if __name__ == "__main__":
    main()

