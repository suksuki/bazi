"""
V11.0 RANSAC ç¦»ç¾¤ç‚¹æ£€æµ‹å’Œæ•°æ®æ¸…æ´—

ä½¿ç”¨RANSACç®—æ³•è¯†åˆ«æ ¡å‡†æ•°æ®é›†ä¸­çš„ç¦»ç¾¤ç‚¹ï¼ˆè„æ•°æ®ï¼‰
"""

import sys
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Set
from collections import defaultdict
import logging
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.engine_graph import GraphNetworkEngine
from core.models.config_model import ConfigModel

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class CaseResult:
    """æ¡ˆä¾‹è®¡ç®—ç»“æœ"""
    case_id: str
    case_name: str
    ground_truth: str
    predicted_label: str
    strength_score: float
    self_team_ratio: float
    is_correct: bool
    error_magnitude: float  # é¢„æµ‹é”™è¯¯çš„ä¸¥é‡ç¨‹åº¦


class RANSACOutlierDetector:
    """RANSACç¦»ç¾¤ç‚¹æ£€æµ‹å™¨"""
    
    def __init__(self, config_model: ConfigModel = None):
        self.config_model = config_model or ConfigModel()
        self.config = self.config_model.load_config()
        self.engine = GraphNetworkEngine(config=self.config)
        
    def load_calibration_cases(self) -> Tuple[List[Dict], Dict[str, float]]:
        """åŠ è½½æ ¡å‡†æ¡ˆä¾‹"""
        data_dir = project_root / "data"
        classic_file = data_dir / "classic_cases.json"
        calibration_file = data_dir / "calibration_cases.json"
        
        cases = []
        case_weights = {}
        
        # åŠ è½½ç»å…¸æ¡ˆä¾‹ï¼ˆæƒé‡3.0xï¼‰
        if classic_file.exists():
            with open(classic_file, 'r', encoding='utf-8') as f:
                classic_cases = json.load(f)
                for case in classic_cases:
                    case_id = case.get('id', f"CLASSIC_{len(cases)}")
                    case['weight'] = 3.0
                    cases.append(case)
                    case_weights[case_id] = 3.0
        
        # åŠ è½½æ ¡å‡†æ¡ˆä¾‹
        if calibration_file.exists():
            with open(calibration_file, 'r', encoding='utf-8') as f:
                cal_cases = json.load(f)
                loaded_ids = {c.get('id') for c in cases if 'id' in c}
                
                for case in cal_cases:
                    case_id = case.get('id', f"CAL_{len(cases)}")
                    
                    # é¿å…é‡å¤åŠ è½½
                    if case_id in loaded_ids:
                        continue
                    
                    # æ ¹æ®caseç±»å‹è®¾ç½®æƒé‡
                    if case.get('id', '').startswith('STRENGTH_CN_'):
                        case['weight'] = 1.5  # ç°ä»£ä¸­å›½æ¡ˆä¾‹
                        case_weights[case_id] = 1.5
                    else:
                        case['weight'] = 0.8  # å¤–å›½æ¡ˆä¾‹
                        case_weights[case_id] = 0.8
                    
                    cases.append(case)
        
        logger.info(f"âœ… åŠ è½½äº† {len(cases)} ä¸ªæ¡ˆä¾‹")
        return cases, case_weights
    
    def evaluate_case(self, case: Dict) -> CaseResult:
        """è¯„ä¼°å•ä¸ªæ¡ˆä¾‹"""
        bazi_list = case.get('bazi', [])
        if isinstance(bazi_list, str):
            bazi_list = bazi_list.split()
        
        day_master = case.get('day_master', '')
        
        # åˆå§‹åŒ–å¼•æ“
        self.engine.initialize_nodes(
            bazi=bazi_list,
            day_master=day_master,
            luck_pillar=None,
            year_pillar=None
        )
        
        # è®¡ç®—æ—ºè¡°
        result = self.engine.calculate_strength_score(day_master)
        
        ground_truth = case.get('ground_truth', {}).get('strength', 'Unknown')
        predicted_label = result.get('strength_label', 'Unknown')
        
        self_team_energy = result.get('self_team_energy', 0)
        total_energy = result.get('total_energy', 1)
        self_team_ratio = self_team_energy / total_energy if total_energy > 0 else 0
        
        is_correct = predicted_label == ground_truth
        
        # è®¡ç®—é”™è¯¯ä¸¥é‡ç¨‹åº¦
        # å¦‚æœé¢„æµ‹é”™è¯¯ï¼Œé”™è¯¯ä¸¥é‡ç¨‹åº¦ = |predicted_score - expected_score_range_center|
        error_magnitude = 0.0
        if not is_correct:
            # æ ¹æ®ground_truthä¼°ç®—æœŸæœ›åˆ†æ•°èŒƒå›´
            expected_score_ranges = {
                'Follower': (0, 20),
                'Weak': (20, 45),
                'Balanced': (45, 55),
                'Strong': (55, 80),
                'Special_Strong': (75, 100)
            }
            expected_range = expected_score_ranges.get(ground_truth, (0, 100))
            expected_center = (expected_range[0] + expected_range[1]) / 2
            error_magnitude = abs(result.get('strength_score', 0) - expected_center)
        
        return CaseResult(
            case_id=case.get('id', 'Unknown'),
            case_name=case.get('name', 'Unknown'),
            ground_truth=ground_truth,
            predicted_label=predicted_label,
            strength_score=result.get('strength_score', 0),
            self_team_ratio=self_team_ratio,
            is_correct=is_correct,
            error_magnitude=error_magnitude
        )
    
    def evaluate_parameter_set(self, cases: List[Dict], sample_indices: List[int]) -> Tuple[float, List[CaseResult]]:
        """ä½¿ç”¨ç»™å®šçš„å‚æ•°ï¼ˆå®é™…ä¸Šæ˜¯æ¡ˆä¾‹å­é›†ï¼‰è¯„ä¼°åŒ¹é…ç‡"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šä½¿ç”¨å½“å‰é…ç½®è¯„ä¼°æ¡ˆä¾‹å­é›†
        # åœ¨å®é™…RANSACä¸­ï¼Œæˆ‘ä»¬ä¼šç”¨è¿™ä¸ªå­é›†æ¥ä¼˜åŒ–å‚æ•°
        
        results = []
        correct_count = 0
        total_weight = 0.0
        
        for idx in sample_indices:
            if idx >= len(cases):
                continue
            
            case = cases[idx]
            result = self.evaluate_case(case)
            results.append(result)
            
            case_weight = case.get('weight', 1.0)
            total_weight += case_weight
            if result.is_correct:
                correct_count += case_weight
        
        match_rate = correct_count / total_weight if total_weight > 0 else 0.0
        
        return match_rate, results
    
    def run_ransac(self, cases: List[Dict], n_iterations: int = 1000, sample_size: int = 20) -> Dict:
        """
        è¿è¡ŒRANSACç®—æ³•
        
        Args:
            cases: æ¡ˆä¾‹åˆ—è¡¨
            n_iterations: è¿­ä»£æ¬¡æ•°
            sample_size: æ¯æ¬¡éšæœºæŠ½æ ·çš„æ ·æœ¬æ•°
        
        Returns:
            RANSACç»“æœå­—å…¸
        """
        logger.info(f"ğŸš€ å¼€å§‹RANSACåˆ†æ: {n_iterations}æ¬¡è¿­ä»£, æ¯æ¬¡æŠ½æ ·{sample_size}ä¸ªæ¡ˆä¾‹")
        
        best_match_rate = 0.0
        best_sample_indices = []
        best_results = []
        all_inlier_counts = []
        
        n_cases = len(cases)
        
        for iteration in range(n_iterations):
            if (iteration + 1) % 100 == 0:
                logger.info(f"  è¿›åº¦: {iteration + 1}/{n_iterations}, å½“å‰æœ€ä½³åŒ¹é…ç‡: {best_match_rate:.2%}")
            
            # éšæœºæŠ½æ ·
            sample_indices = np.random.choice(n_cases, size=min(sample_size, n_cases), replace=False).tolist()
            
            # è¯„ä¼°è¿™ä¸ªæ ·æœ¬
            match_rate, results = self.evaluate_parameter_set(cases, sample_indices)
            
            # è®¡ç®—inlieræ•°é‡ï¼ˆåœ¨è¿™ä¸ªæ ·æœ¬ä¸­é¢„æµ‹æ­£ç¡®çš„æ¡ˆä¾‹æ•°ï¼‰
            inlier_count = sum(1 for r in results if r.is_correct)
            all_inlier_counts.append(inlier_count)
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if match_rate > best_match_rate:
                best_match_rate = match_rate
                best_sample_indices = sample_indices
                best_results = results
        
        # ä½¿ç”¨æœ€ä½³æ ·æœ¬è®¡ç®—æ‰€æœ‰æ¡ˆä¾‹çš„ç»“æœ
        logger.info("ğŸ“Š ä½¿ç”¨æœ€ä½³æ ·æœ¬è¯„ä¼°æ‰€æœ‰æ¡ˆä¾‹...")
        all_results = []
        for idx, case in enumerate(cases):
            result = self.evaluate_case(case)
            all_results.append(result)
        
        # è®¡ç®—æ¯ä¸ªæ¡ˆä¾‹çš„outlieråˆ†æ•°
        # æ–¹æ³•ï¼šå¦‚æœæ¡ˆä¾‹åœ¨æœ€ä½³æ ·æœ¬ä¸­é¢„æµ‹é”™è¯¯ï¼Œæˆ–è€…é”™è¯¯ä¸¥é‡ç¨‹åº¦å¾ˆé«˜ï¼Œåˆ™æ ‡è®°ä¸ºoutlier
        outlier_scores = {}
        for idx, result in enumerate(all_results):
            case_id = result.case_id
            is_in_best_sample = idx in best_sample_indices
            
            # Outlieråˆ†æ•°åŸºäºï¼š
            # 1. é¢„æµ‹é”™è¯¯ (is_correct = False)
            # 2. é”™è¯¯ä¸¥é‡ç¨‹åº¦ (error_magnitude)
            # 3. ä¸åœ¨æœ€ä½³æ ·æœ¬ä¸­ä¸”é¢„æµ‹é”™è¯¯
            outlier_score = 0.0
            if not result.is_correct:
                outlier_score += 50.0  # åŸºç¡€åˆ†æ•°
                outlier_score += result.error_magnitude * 0.5  # é”™è¯¯ä¸¥é‡ç¨‹åº¦
                if not is_in_best_sample:
                    outlier_score += 20.0  # ä¸åœ¨æœ€ä½³æ ·æœ¬ä¸­
            
            outlier_scores[case_id] = outlier_score
        
        # æŒ‰outlieråˆ†æ•°æ’åº
        sorted_cases = sorted(outlier_scores.items(), key=lambda x: x[1], reverse=True)
        
        # è¯†åˆ«outliersï¼ˆåˆ†æ•°è¶…è¿‡é˜ˆå€¼çš„æ¡ˆä¾‹ï¼‰
        # ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•ï¼šå–top 20%ä½œä¸ºæ½œåœ¨outliers
        threshold_percentile = 80
        threshold_score = np.percentile([s[1] for s in sorted_cases], threshold_percentile)
        
        outliers = [case_id for case_id, score in sorted_cases if score >= threshold_score]
        
        logger.info(f"âœ… RANSACåˆ†æå®Œæˆ")
        logger.info(f"   æœ€ä½³åŒ¹é…ç‡: {best_match_rate:.2%}")
        logger.info(f"   è¯†åˆ«å‡º {len(outliers)} ä¸ªæ½œåœ¨outliers (é˜ˆå€¼: {threshold_score:.2f})")
        
        return {
            'best_match_rate': best_match_rate,
            'best_sample_size': len(best_sample_indices),
            'total_cases': n_cases,
            'outlier_count': len(outliers),
            'outliers': outliers,
            'outlier_scores': dict(sorted_cases),
            'threshold_score': threshold_score,
            'all_results': [
                {
                    'case_id': r.case_id,
                    'case_name': r.case_name,
                    'ground_truth': r.ground_truth,
                    'predicted_label': r.predicted_label,
                    'strength_score': r.strength_score,
                    'self_team_ratio': r.self_team_ratio,
                    'is_correct': r.is_correct,
                    'error_magnitude': r.error_magnitude,
                    'outlier_score': outlier_scores.get(r.case_id, 0.0)
                }
                for r in all_results
            ]
        }
    
    def generate_removal_recommendations(self, ransac_result: Dict, cases: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆå‰”é™¤å»ºè®®åå•"""
        outliers = ransac_result['outliers']
        outlier_scores = ransac_result['outlier_scores']
        
        recommendations = []
        for case_id in outliers:
            case = next((c for c in cases if c.get('id') == case_id), None)
            if not case:
                continue
            
            result = next((r for r in ransac_result['all_results'] if r['case_id'] == case_id), None)
            if not result:
                continue
            
            recommendations.append({
                'case_id': case_id,
                'case_name': case.get('name', 'Unknown'),
                'ground_truth': result['ground_truth'],
                'predicted_label': result['predicted_label'],
                'strength_score': result['strength_score'],
                'outlier_score': outlier_scores.get(case_id, 0.0),
                'reason': f"é¢„æµ‹é”™è¯¯: {result['ground_truth']} â†’ {result['predicted_label']}, é”™è¯¯ä¸¥é‡ç¨‹åº¦: {result['error_magnitude']:.2f}",
                'suggested_action': 'æ ‡è®°ä¸ºDirty_Dataï¼Œæƒé‡å½’é›¶æˆ–ç§»å…¥ignored_cases.json'
            })
        
        # æŒ‰outlier_scoreæ’åº
        recommendations.sort(key=lambda x: x['outlier_score'], reverse=True)
        
        return recommendations
    
    def save_recommendations(self, recommendations: List[Dict], output_file: Path):
        """ä¿å­˜å‰”é™¤å»ºè®®åˆ°JSONæ–‡ä»¶"""
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        report = {
            'timestamp': str(Path(__file__).stat().st_mtime),
            'total_recommendations': len(recommendations),
            'recommendations': recommendations
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… å‰”é™¤å»ºè®®å·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ§¹ V11.0 RANSAC æ•°æ®æ¸…æ´— - ç¦»ç¾¤ç‚¹æ£€æµ‹")
    print("=" * 80)
    print()
    
    detector = RANSACOutlierDetector()
    
    # åŠ è½½æ¡ˆä¾‹
    cases, case_weights = detector.load_calibration_cases()
    
    # è¿è¡ŒRANSAC
    ransac_result = detector.run_ransac(cases, n_iterations=500, sample_size=20)
    
    # ç”Ÿæˆå‰”é™¤å»ºè®®
    recommendations = detector.generate_removal_recommendations(ransac_result, cases)
    
    # ä¿å­˜ç»“æœ
    output_file = project_root / "data" / "ransac_outlier_recommendations.json"
    detector.save_recommendations(recommendations, output_file)
    
    # æ‰“å°æ‘˜è¦
    print()
    print("=" * 80)
    print("ğŸ“Š RANSACåˆ†æç»“æœæ‘˜è¦")
    print("=" * 80)
    print(f"æ€»æ¡ˆä¾‹æ•°: {ransac_result['total_cases']}")
    print(f"æœ€ä½³åŒ¹é…ç‡: {ransac_result['best_match_rate']:.2%}")
    print(f"è¯†åˆ«å‡ºç¦»ç¾¤ç‚¹: {ransac_result['outlier_count']} ä¸ª")
    print()
    
    print("Top 10 ç¦»ç¾¤ç‚¹å»ºè®®:")
    for i, rec in enumerate(recommendations[:10], 1):
        print(f"  {i}. {rec['case_name']} (ID: {rec['case_id']})")
        print(f"     çœŸå®å€¼: {rec['ground_truth']}, é¢„æµ‹å€¼: {rec['predicted_label']}")
        print(f"     Outlieråˆ†æ•°: {rec['outlier_score']:.2f}")
        print(f"     å»ºè®®: {rec['suggested_action']}")
        print()
    
    print("=" * 80)
    print(f"ğŸ“„ å®Œæ•´å»ºè®®åˆ—è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    print("=" * 80)


if __name__ == '__main__':
    main()

