#!/usr/bin/env python3
"""
æ‰¹é‡æ ¼å±€åŒ¹é…å™¨ï¼ˆå¹¶è¡Œç‰ˆï¼‰
========================
åŠŸèƒ½ï¼šå¯¹å¤§è§„æ¨¡æ ·æœ¬ï¼ˆå¦‚ 51.8 ä¸‡ï¼‰è¿›è¡Œæ ¼å±€åŒ¹é…ï¼Œæ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•è¿›ç¨‹æ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    python3 scripts/batch_pattern_matcher.py --pattern A-03 --data-file core/data/holographic_universe_518k.jsonl --workers 1
    
    # å¤šè¿›ç¨‹æ¨¡å¼ï¼ˆæ¨èï¼‰
    python3 scripts/batch_pattern_matcher.py --pattern A-03 --data-file core/data/holographic_universe_518k.jsonl --workers 8
    
    # ä»…å¤„ç†å‰ N ä¸ªæ ·æœ¬ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    python3 scripts/batch_pattern_matcher.py --pattern A-03 --data-file core/data/holographic_universe_518k.jsonl --limit 1000
"""

import json
import time
import sys
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from multiprocessing import Pool, cpu_count
from functools import partial
import numpy as np

# å°è¯•å¯¼å…¥ tqdmï¼ˆè¿›åº¦æ¡ï¼‰
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    # ç®€å•çš„è¿›åº¦æ¡å®ç°ï¼ˆå¦‚æœæ²¡æœ‰tqdmï¼‰
    class tqdm:
        def __init__(self, iterable=None, total=None, desc=None, unit=None, ncols=None):
            self.iterable = iterable
            self.total = total
            self.desc = desc or ""
            self.unit = unit or "it"
            self.current = 0
            self.start_time = time.time()
            
        def __iter__(self):
            if self.iterable:
                for item in self.iterable:
                    yield item
                    self.current += 1
                    self._update()
            return self
            
        def __enter__(self):
            return self
            
        def __exit__(self, *args):
            self._close()
            
        def update(self, n=1):
            self.current += n
            self._update()
            
        def _update(self):
            if self.total:
                pct = (self.current / self.total) * 100
                elapsed = time.time() - self.start_time
                if self.current > 0:
                    rate = self.current / elapsed
                    eta = (self.total - self.current) / rate if rate > 0 else 0
                    print(f"\r{self.desc}: {self.current}/{self.total} ({pct:.1f}%) | "
                          f"é€Ÿåº¦: {rate:.1f} {self.unit}/s | ETA: {eta:.0f}s", end="", flush=True)
                    
        def _close(self):
            print()  # æ¢è¡Œ

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—ï¼ˆå‡å°‘è¾“å‡ºï¼‰
logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼ˆç”¨äºworkerè¿›ç¨‹ï¼‰
_global_registry_loader = None
_global_pattern_id = None


def init_worker(pattern_id: str):
    """åˆå§‹åŒ–workerè¿›ç¨‹ï¼ˆæ¯ä¸ªè¿›ç¨‹åªåˆå§‹åŒ–ä¸€æ¬¡RegistryLoaderï¼‰"""
    global _global_registry_loader, _global_pattern_id
    from core.registry_loader import RegistryLoader
    _global_registry_loader = RegistryLoader()
    _global_pattern_id = pattern_id


def process_single_sample(args: Tuple[int, Dict]) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬ï¼ˆworkerå‡½æ•°ï¼‰
    
    Args:
        args: (line_number, sample_data) å…ƒç»„
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    global _global_registry_loader, _global_pattern_id
    
    line_number, sample_data = args
    
    try:
        start_time = time.perf_counter()
        
        # æ¨¡å¼1: å¦‚æœæœ‰chartï¼Œä½¿ç”¨å®Œæ•´æµç¨‹
        chart = sample_data.get('chart')
        day_master = sample_data.get('day_master')
        
        if chart:
            # å®Œæ•´åŒ¹é…æµç¨‹ï¼ˆéœ€è¦chartå’Œday_masterï¼‰
            if not day_master:
                # ä»chartæ¨æ–­æ—¥ä¸»ï¼ˆæ—¥æŸ±å¤©å¹²ï¼‰
                if len(chart) >= 2 and len(chart[1]) >= 1:
                    day_master = chart[1][0]
                else:
                    return {
                        'line_number': line_number,
                        'uid': sample_data.get('uid'),
                        'status': 'error',
                        'error': 'Cannot infer day_master'
                    }
            
            result = _global_registry_loader.calculate_tensor_projection_from_registry(
                pattern_id=_global_pattern_id,
                chart=chart,
                day_master=day_master
            )
            
        else:
            # æ¨¡å¼2: å¦‚æœåªæœ‰tensorï¼Œç›´æ¥ä»tensorè®¡ç®—åŒ¹é…åº¦
            tensor_data = sample_data.get('tensor')
            if not tensor_data:
                return {
                    'line_number': line_number,
                    'uid': sample_data.get('uid'),
                    'status': 'error',
                    'error': 'Missing both chart and tensor data'
                }
            
            # å°†tensorè½¬æ¢ä¸ºå‘é‡æ ¼å¼
            if isinstance(tensor_data, dict):
                tensor_vector = np.array([
                    tensor_data.get('E', 0.0),
                    tensor_data.get('O', 0.0),
                    tensor_data.get('M', 0.0),
                    tensor_data.get('S', 0.0),
                    tensor_data.get('R', 0.0)
                ])
            elif isinstance(tensor_data, list) and len(tensor_data) == 5:
                tensor_vector = np.array(tensor_data)
            else:
                return {
                    'line_number': line_number,
                    'uid': sample_data.get('uid'),
                    'status': 'error',
                    'error': f'Invalid tensor format: {type(tensor_data)}'
                }
            
            # ä½¿ç”¨pattern_recognitionç›´æ¥åŒ¹é…
            pattern = _global_registry_loader.get_pattern(_global_pattern_id)
            if not pattern:
                return {
                    'line_number': line_number,
                    'uid': sample_data.get('uid'),
                    'status': 'error',
                    'error': f'Pattern {_global_pattern_id} not found'
                }
            
            # è§£æ@configå¼•ç”¨
            pattern = _global_registry_loader.resolve_config_refs_in_dict(pattern)
            
            # è·å–ç‰¹å¾é”šç‚¹ï¼ˆä½¿ç”¨RegistryLoaderçš„æ–¹æ³•ï¼‰
            feature_anchors = _global_registry_loader.get_feature_anchors(_global_pattern_id)
            
            if not feature_anchors:
                return {
                    'line_number': line_number,
                    'uid': sample_data.get('uid'),
                    'status': 'error',
                    'error': 'No feature anchors found'
                }
            
            # æå–æµå½¢æ•°æ®ï¼ˆæ”¯æŒstandard_manifoldæˆ–ç›´æ¥mean_vectorï¼‰
            manifold = feature_anchors.get('standard_manifold') or feature_anchors
            mean_vector_dict = manifold.get('mean_vector', {})
            
            if not mean_vector_dict:
                return {
                    'line_number': line_number,
                    'uid': sample_data.get('uid'),
                    'status': 'error',
                    'error': 'No mean_vector found in feature anchors'
                }
            
            # è®¡ç®—åŒ¹é…åº¦ï¼ˆä½¿ç”¨pattern_recognitionçš„é€»è¾‘ï¼‰
            from core.math_engine import (
                calculate_mahalanobis_distance,
                calculate_precision_score,
                calculate_cosine_similarity
            )
            
            mean_vector = np.array([
                mean_vector_dict.get('E', 0.0),
                mean_vector_dict.get('O', 0.0),
                mean_vector_dict.get('M', 0.0),
                mean_vector_dict.get('S', 0.0),
                mean_vector_dict.get('R', 0.0)
            ])
            
            # è·å–åæ–¹å·®çŸ©é˜µ
            cov_data = manifold.get('covariance_matrix') or feature_anchors.get('covariance_matrix')
            if cov_data:
                covariance_matrix = np.array(cov_data)
            else:
                # å¦‚æœæ²¡æœ‰åæ–¹å·®çŸ©é˜µï¼Œä½¿ç”¨å•ä½çŸ©é˜µï¼ˆä¼šé€€åŒ–ä¸ºæ¬§å¼è·ç¦»ï¼‰
                covariance_matrix = np.eye(5)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = calculate_cosine_similarity(tensor_vector, mean_vector)
            
            # è®¡ç®—é©¬æ°è·ç¦»
            m_dist = calculate_mahalanobis_distance(tensor_vector, mean_vector, covariance_matrix)
            
            # è®¡ç®—SAIï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨tensorçš„å‡å€¼ä½œä¸ºSAIï¼‰
            sai = np.mean(tensor_vector)
            
            # è®¡ç®—ç²¾ç¡®åº¦è¯„åˆ†
            precision_score = calculate_precision_score(similarity, m_dist, sai)
            
            # è·å–é˜ˆå€¼ï¼ˆä»manifoldæˆ–feature_anchorsï¼‰
            # V3.1ä¿®æ­£ï¼šæé«˜åŒ¹é…é˜ˆå€¼è‡³0.7ï¼Œé¿å…æ³›åŒ–è¿‡åº¦
            thresholds = manifold.get('thresholds', {}) or feature_anchors.get('thresholds', {})
            max_m_dist = thresholds.get('max_mahalanobis_dist', 3.0)
            match_threshold = thresholds.get('match_threshold', 0.7)  # V3.1: ä»0.6æé«˜åˆ°0.7
            
            # åˆ¤æ–­çŠ¶æ€
            if precision_score > match_threshold and m_dist <= max_m_dist:
                status = 'MATCHED'
            elif precision_score < 0.6:
                status = 'BROKEN'
            else:
                status = 'EDGE'
            
            result = {
                'projection': {
                    'E': float(tensor_vector[0]),
                    'O': float(tensor_vector[1]),
                    'M': float(tensor_vector[2]),
                    'S': float(tensor_vector[3]),
                    'R': float(tensor_vector[4])
                },
                'recognition': {
                    'status': status,
                    'precision_score': float(precision_score),
                    'mahalanobis_dist': float(m_dist),
                    'cosine_similarity': float(similarity)
                },
                'sai': float(sai)
            }
        
        elapsed_time = (time.perf_counter() - start_time) * 1000  # è½¬æ¢ä¸ºms
        
        # æå–å…³é”®ä¿¡æ¯
        recognition = result.get('recognition', {})
        precision_score = recognition.get('precision_score', 0.0)
        mahalanobis_dist = recognition.get('mahalanobis_dist', 0.0)
        status = recognition.get('status', 'UNKNOWN')
        
        return {
            'line_number': line_number,
            'uid': sample_data.get('uid'),
            'status': 'success',
            'precision_score': precision_score,
            'mahalanobis_dist': mahalanobis_dist,
            'pattern_status': status,
            'elapsed_ms': elapsed_time,
            'tensor': result.get('projection', {}),
            'sai': result.get('sai', 0.0)
        }
        
    except Exception as e:
        import traceback
        return {
            'line_number': line_number,
            'uid': sample_data.get('uid', 'unknown'),
            'status': 'error',
            'error': str(e),
            'traceback': traceback.format_exc()
        }


def load_samples_from_file(
    file_path: Path,
    limit: Optional[int] = None
) -> List[Tuple[int, Dict]]:
    """
    ä»æ–‡ä»¶åŠ è½½æ ·æœ¬ï¼ˆæƒ°æ€§åŠ è½½ï¼Œåªè¿”å›å…ƒæ•°æ®ï¼‰
    
    Args:
        file_path: JSONLæ–‡ä»¶è·¯å¾„
        limit: é™åˆ¶åŠ è½½æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        
    Returns:
        [(line_number, sample_data), ...] åˆ—è¡¨
    """
    samples = []
    line_count = 0
    
    print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ ·æœ¬: {file_path}")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, start=1):
                if not line.strip():
                    continue
                
                try:
                    data = json.loads(line.strip())
                    
                    # è·³è¿‡metaè¡Œ
                    if 'meta' in data:
                        continue
                    
                    # éªŒè¯å¿…éœ€å­—æ®µï¼ˆæ”¯æŒtensoræˆ–chartï¼‰
                    if 'tensor' in data or 'chart' in data:
                        samples.append((line_num, data))
                        line_count += 1
                        
                        if limit and line_count >= limit:
                            break
                        
                        # è¿›åº¦æç¤º
                        if line_count % 10000 == 0:
                            print(f"  å·²åŠ è½½: {line_count:,} ä¸ªæ ·æœ¬...", end='\r')
                
                except json.JSONDecodeError:
                    continue
                except Exception as e:
                    logger.warning(f"è¡Œ {line_num} è§£æå¤±è´¥: {e}")
                    continue
    
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
        return []
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(samples):,} ä¸ªæ ·æœ¬")
    return samples


def process_batch(
    samples: List[Tuple[int, Dict]],
    pattern_id: str,
    workers: int = None,
    batch_size: int = 1000
) -> Dict[str, Any]:
    """
    æ‰¹é‡å¤„ç†æ ·æœ¬ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
    
    Args:
        samples: æ ·æœ¬åˆ—è¡¨
        pattern_id: æ ¼å±€ID
        workers: è¿›ç¨‹æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨CPUæ ¸å¿ƒæ•°ï¼‰
        batch_size: æ¯æ‰¹å¤„ç†çš„æ ·æœ¬æ•°
        
    Returns:
        ç»Ÿè®¡ç»“æœå­—å…¸
    """
    if workers is None:
        workers = cpu_count()
    
    total_samples = len(samples)
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†: {pattern_id} æ ¼å±€")
    print(f"{'='*80}")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples:,}")
    print(f"å·¥ä½œè¿›ç¨‹æ•°: {workers}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size:,}")
    print(f"{'='*80}\n")
    
    # ç»“æœç»Ÿè®¡
    results = []
    errors = []
    start_time = time.time()
    
    # åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
    n_batches = (total_samples + batch_size - 1) // batch_size
    
    # ä½¿ç”¨è¿›åº¦æ¡
    with tqdm(total=total_samples, desc="ğŸš€ åŒ¹é…è¿›åº¦", unit="æ ·æœ¬", ncols=100) as pbar:
        for batch_idx in range(n_batches):
            batch_start = batch_idx * batch_size
            batch_end = min(batch_start + batch_size, total_samples)
            batch_samples = samples[batch_start:batch_end]
            
            batch_num = batch_idx + 1
            batch_start_time = time.time()
            
            # å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡
            with Pool(processes=workers, initializer=init_worker, initargs=(pattern_id,)) as pool:
                batch_results = pool.map(process_single_sample, batch_samples)
            
            batch_elapsed = time.time() - batch_start_time
            
            # ç»Ÿè®¡æ‰¹æ¬¡ç»“æœ
            batch_success = sum(1 for r in batch_results if r.get('status') == 'success')
            batch_errors = sum(1 for r in batch_results if r.get('status') == 'error')
            batch_matched = sum(1 for r in batch_results if r.get('status') == 'success' and r.get('precision_score', 0) > 0.6)
            
            results.extend(batch_results)
            errors.extend([r for r in batch_results if r.get('status') == 'error'])
            
            # æ›´æ–°è¿›åº¦æ¡
            processed = len(results)
            elapsed_total = time.time() - start_time
            rate = processed / elapsed_total if elapsed_total > 0 else 0
            eta = (total_samples - processed) / rate if rate > 0 else 0
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.set_description(f"ğŸ“¦ æ‰¹æ¬¡ {batch_num}/{n_batches}")
            pbar.update(len(batch_samples))
            
            # åœ¨è¿›åº¦æ¡åæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            pbar.set_postfix({
                'æˆåŠŸ': f"{batch_success}",
                'åŒ¹é…': f"{batch_matched}",
                'é€Ÿåº¦': f"{rate:.0f}/s",
                'ETA': f"{eta/60:.1f}min"
            })
    
    # ç»Ÿè®¡æ±‡æ€»
    total_time = time.time() - start_time
    
    success_results = [r for r in results if r.get('status') == 'success']
    precision_scores = [r.get('precision_score', 0) for r in success_results]
    
    # åŒ¹é…åº¦ç»Ÿè®¡ï¼ˆä¿®æ­£ï¼šåªæœ‰ MATCHED çŠ¶æ€æ‰ç®—çœŸæ­£æˆæ ¼ï¼‰
    matched = [r for r in success_results if r.get('pattern_status') == 'MATCHED']
    edge_cases = [r for r in success_results if r.get('pattern_status') == 'EDGE']
    # V3.1: å¼ºåŒ¹é…é˜ˆå€¼ä¿æŒ0.8ï¼ˆç›¸å¯¹äºæ–°çš„match_threshold=0.7ï¼‰
    strong_matched = [r for r in matched if r.get('precision_score', 0) > 0.8]
    
    stats = {
        'total_samples': total_samples,
        'success_count': len(success_results),
        'error_count': len(errors),
        'matched_count': len(matched),
        'strong_matched_count': len(strong_matched),
        'match_rate': len(matched) / len(success_results) * 100 if success_results else 0,
        'strong_match_rate': len(strong_matched) / len(success_results) * 100 if success_results else 0,
        'avg_precision': np.mean(precision_scores) if precision_scores else 0,
        'median_precision': np.median(precision_scores) if precision_scores else 0,
        'total_time_sec': total_time,
        'total_time_min': total_time / 60,
        'processing_rate': total_samples / total_time if total_time > 0 else 0,
        'precision_distribution': {
            '0.0-0.2': sum(1 for p in precision_scores if 0.0 <= p < 0.2),
            '0.2-0.4': sum(1 for p in precision_scores if 0.2 <= p < 0.4),
            '0.4-0.6': sum(1 for p in precision_scores if 0.4 <= p < 0.6),
            '0.6-0.8': sum(1 for p in precision_scores if 0.6 <= p < 0.8),
            '0.8-1.0': sum(1 for p in precision_scores if 0.8 <= p <= 1.0),
        }
    }
    
    return {
        'stats': stats,
        'results': results,
        'errors': errors
    }


def save_results(
    output_path: Path,
    pattern_id: str,
    stats: Dict,
    results: List[Dict],
    errors: List[Dict]
):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats_path = output_path.with_suffix('.stats.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump({
            'pattern_id': pattern_id,
            'stats': stats,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆä»…ä¿å­˜åŒ¹é…çš„æ ·æœ¬ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
    # V3.1: åªä¿å­˜MATCHEDçŠ¶æ€çš„æ ·æœ¬ï¼ˆå·²ç»æ˜¯é«˜è´¨é‡ç­›é€‰ï¼‰
    matched_results = [r for r in results if r.get('status') == 'success' and r.get('pattern_status') == 'MATCHED']
    
    results_path = output_path.with_suffix('.matched.json')
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump({
            'pattern_id': pattern_id,
            'matched_count': len(matched_results),
            'results': matched_results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜é”™è¯¯æ—¥å¿—
    if errors:
        errors_path = output_path.with_suffix('.errors.json')
        with open(errors_path, 'w', encoding='utf-8') as f:
            json.dump({
                'pattern_id': pattern_id,
                'error_count': len(errors),
                'errors': errors[:100],  # åªä¿å­˜å‰100ä¸ªé”™è¯¯
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
    print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
    print(f"   åŒ¹é…ç»“æœ: {results_path} ({len(matched_results):,} ä¸ªåŒ¹é…æ ·æœ¬)")
    if errors:
        print(f"   é”™è¯¯æ—¥å¿—: {errors_path} ({len(errors)} ä¸ªé”™è¯¯)")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡æ ¼å±€åŒ¹é…å™¨ï¼ˆå¹¶è¡Œç‰ˆï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æµ‹è¯•æ¨¡å¼ï¼ˆå•è¿›ç¨‹ï¼Œ1000ä¸ªæ ·æœ¬ï¼‰
  python3 scripts/batch_pattern_matcher.py --pattern A-03 \\
      --data-file core/data/holographic_universe_518k.jsonl \\
      --workers 1 --limit 1000
  
  # å®Œæ•´è¿è¡Œï¼ˆ8è¿›ç¨‹ï¼‰
  python3 scripts/batch_pattern_matcher.py --pattern A-03 \\
      --data-file core/data/holographic_universe_518k.jsonl \\
      --workers 8
  
  # è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„
  python3 scripts/batch_pattern_matcher.py --pattern A-03 \\
      --data-file core/data/holographic_universe_518k.jsonl \\
      --output results/a03_full_match.json
        """
    )
    
    parser.add_argument(
        '--pattern',
        type=str,
        required=True,
        help='æ ¼å±€ID (å¦‚: A-03, D-01, D-02)'
    )
    parser.add_argument(
        '--data-file',
        type=str,
        required=True,
        help='æ•°æ®æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: results/{pattern}_match.json)'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=None,
        help=f'å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°={cpu_count()})'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='é™åˆ¶å¤„ç†çš„æ ·æœ¬æ•° (ç”¨äºæµ‹è¯•)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=1000,
        help='æ¯æ‰¹å¤„ç†çš„æ ·æœ¬æ•° (é»˜è®¤: 1000)'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è¾“å‡ºè·¯å¾„
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = project_root / "results" / f"{args.pattern}_match.json"
    
    # åŠ è½½æ ·æœ¬
    data_path = Path(args.data_file)
    samples = load_samples_from_file(data_path, limit=args.limit)
    
    if not samples:
        print("âŒ æ²¡æœ‰å¯å¤„ç†çš„æ ·æœ¬")
        return
    
    # æ‰¹é‡å¤„ç†
    result_data = process_batch(
        samples=samples,
        pattern_id=args.pattern,
        workers=args.workers,
        batch_size=args.batch_size
    )
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    stats = result_data['stats']
    print(f"\n{'='*80}")
    print("ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡")
    print(f"{'='*80}")
    print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']:,}")
    print(f"æˆåŠŸå¤„ç†: {stats['success_count']:,}")
    print(f"å¤„ç†å¤±è´¥: {stats['error_count']:,}")
    print(f"\nåŒ¹é…ç»Ÿè®¡:")
    print(f"  åŒ¹é…æ ·æœ¬ (Precision > 0.6): {stats['matched_count']:,} ({stats['match_rate']:.2f}%)")
    print(f"  å¼ºåŒ¹é… (Precision > 0.8): {stats['strong_matched_count']:,} ({stats['strong_match_rate']:.2f}%)")
    print(f"\nç²¾ç¡®åº¦åˆ†å¸ƒ:")
    for range_name, count in stats['precision_distribution'].items():
        pct = count / stats['success_count'] * 100 if stats['success_count'] > 0 else 0
        print(f"  {range_name}: {count:,} ({pct:.1f}%)")
    print(f"\næ€§èƒ½ç»Ÿè®¡:")
    print(f"  å¹³å‡ç²¾ç¡®åº¦: {stats['avg_precision']:.4f}")
    print(f"  ä¸­ä½æ•°ç²¾ç¡®åº¦: {stats['median_precision']:.4f}")
    print(f"  æ€»è€—æ—¶: {stats['total_time_min']:.2f} åˆ†é’Ÿ ({stats['total_time_sec']:.2f} ç§’)")
    print(f"  å¤„ç†é€Ÿåº¦: {stats['processing_rate']:.1f} æ ·æœ¬/ç§’")
    print(f"{'='*80}\n")
    
    # ä¿å­˜ç»“æœ
    save_results(
        output_path=output_path,
        pattern_id=args.pattern,
        stats=stats,
        results=result_data['results'],
        errors=result_data['errors']
    )


if __name__ == "__main__":
    main()

