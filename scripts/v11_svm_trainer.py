"""
V11.0 SVMåˆ†ç±»å™¨è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®é›†è®­ç»ƒSVMæ¨¡å‹ï¼Œæ›¿ä»£ç¡¬ç¼–ç é˜ˆå€¼
"""

import sys
import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from collections import Counter
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.engine_graph import GraphNetworkEngine
from core.models.config_model import ConfigModel

# V11.1: å¯¼å…¥æ–°çš„æ•°æ®å¼•æ“
try:
    from scripts.data_engine import DataLoader
    DATA_ENGINE_AVAILABLE = True
except ImportError:
    DATA_ENGINE_AVAILABLE = False
    logger.warning("âš ï¸  æ•°æ®å¼•æ“æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨æ—§çš„æ•°æ®åŠ è½½æ–¹å¼")

# V11.7: å¯¼å…¥å†²çªè§£å†³å™¨
try:
    from scripts.data_engine.conflict_resolver import ConflictResolver
    CONFLICT_RESOLVER_AVAILABLE = True
except ImportError:
    CONFLICT_RESOLVER_AVAILABLE = False
    logger.warning("âš ï¸  å†²çªè§£å†³å™¨æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡å†²çªæ¸…æ´—")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥sklearn
try:
    from sklearn.svm import SVC
    from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.metrics import classification_report, confusion_matrix
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("âš ï¸  sklearnæœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒSVMæ¨¡å‹ã€‚è¯·è¿è¡Œ: pip install scikit-learn")

# å°è¯•å¯¼å…¥imbalanced-learn (SMOTE)
try:
    from imblearn.over_sampling import SMOTE, RandomOverSampler
    IMBLEARN_AVAILABLE = True
except ImportError:
    IMBLEARN_AVAILABLE = False
    logger.warning("âš ï¸  imbalanced-learnæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•è¿‡é‡‡æ ·ã€‚è¯·è¿è¡Œ: pip install imbalanced-learn")


class SVMTrainer:
    """SVMåˆ†ç±»å™¨è®­ç»ƒå™¨"""
    
    def __init__(self, config_model: ConfigModel = None):
        self.config_model = config_model or ConfigModel()
        self.config = self.config_model.load_config()
        
    def load_ignored_cases(self) -> set:
        """åŠ è½½éœ€è¦å¿½ç•¥çš„æ¡ˆä¾‹ID"""
        ignored_file = project_root / "config" / "ignored_cases.json"
        if ignored_file.exists():
            with open(ignored_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                ignored_ids = set(data.get('ignored_case_ids', []))
                logger.info(f"âœ… åŠ è½½äº† {len(ignored_ids)} ä¸ªå¿½ç•¥æ¡ˆä¾‹ID")
                return ignored_ids
        return set()
    
    def generate_theoretical_samples(self) -> List[Dict]:
        """
        [V11.1] ç”Ÿæˆç†è®ºåˆæˆæ ·æœ¬ï¼ˆ"ä¸Šå¸æ¨¡å¼"æ•°æ®ç”Ÿæˆï¼‰
        
        åŸºäºå…«å­—åŸç†ï¼Œç”Ÿæˆå®Œç¾çš„æç«¯æ¡ˆä¾‹ï¼Œç”¨äºè®­ç»ƒSVMæ¨¡å‹ã€‚
        æ‰€æœ‰åˆæˆæ•°æ®éƒ½ä¼šæ ‡æ³¨ synthetic: true
        """
        synthetic_cases = []
        
        # 1. Special_Strong (ä¸“æ—ºæ ¼) æ ·æœ¬
        # çº¯ç«ä¸“æ—ºï¼šæ»¡ç›˜çš†ç«ï¼Œæ—¥ä¸»å¾—ä»¤
        synthetic_cases.append({
            'id': 'SYNTHETIC_SPECIAL_STRONG_001',
            'name': '[åˆæˆ] çº¯ç«ä¸“æ—ºæ ¼ (Pure Fire Special Strong)',
            'bazi': ['ä¸™åˆ', 'ç”²åˆ', 'ä¸™åˆ', 'ç”²åˆ'],
            'day_master': 'ä¸™',
            'gender': 'ç”·',
            'ground_truth': {'strength': 'Special_Strong'},
            'characteristics': '[åˆæˆæ•°æ®] çº¯ç«ä¸“æ—ºæ ¼ï¼šæ»¡ç›˜çš†ç«ï¼Œæ—¥ä¸»ä¸™ç«ç”Ÿäºåˆæœˆå¾—ä»¤ï¼Œå¤©å¹²é€ç”²æœ¨ç”Ÿç«ï¼Œåœ°æ”¯å…¨åˆç«ï¼Œç¬¦åˆä¸“æ—ºæ ¼ç‰¹å¾',
            'synthetic': True,
            'synthetic_type': 'theoretical',
            'weight': 1.0
        })
        
        # çº¯é‡‘ä¸“æ—ºï¼šæ»¡ç›˜çš†é‡‘
        synthetic_cases.append({
            'id': 'SYNTHETIC_SPECIAL_STRONG_002',
            'name': '[åˆæˆ] çº¯é‡‘ä¸“æ—ºæ ¼ (Pure Metal Special Strong)',
            'bazi': ['åºšç”³', 'åºšç”³', 'åºšç”³', 'åºšç”³'],
            'day_master': 'åºš',
            'gender': 'ç”·',
            'ground_truth': {'strength': 'Special_Strong'},
            'characteristics': '[åˆæˆæ•°æ®] çº¯é‡‘ä¸“æ—ºæ ¼ï¼šæ»¡ç›˜çš†é‡‘ï¼Œæ—¥ä¸»åºšé‡‘ç”Ÿäºç”³æœˆå¾—ä»¤ï¼Œåœ°æ”¯å…¨ç”³é‡‘ï¼Œå¤©å¹²å…¨åºšé‡‘ï¼Œç¬¦åˆä¸“æ—ºæ ¼ç‰¹å¾',
            'synthetic': True,
            'synthetic_type': 'theoretical',
            'weight': 1.0
        })
        
        # çº¯æœ¨ä¸“æ—ºï¼ˆæ›²ç›´ä»å¯¿æ ¼ï¼‰
        synthetic_cases.append({
            'id': 'SYNTHETIC_SPECIAL_STRONG_003',
            'name': '[åˆæˆ] çº¯æœ¨ä¸“æ—ºæ ¼-æ›²ç›´ä»å¯¿ (Pure Wood Special Strong)',
            'bazi': ['ç”²å¯…', 'ä¹™å¯', 'ç”²å¯…', 'ä¹™å¯'],
            'day_master': 'ç”²',
            'gender': 'ç”·',
            'ground_truth': {'strength': 'Special_Strong'},
            'characteristics': '[åˆæˆæ•°æ®] çº¯æœ¨ä¸“æ—ºæ ¼ï¼ˆæ›²ç›´ä»å¯¿æ ¼ï¼‰ï¼šæ»¡ç›˜çš†æœ¨ï¼Œæ—¥ä¸»ç”²æœ¨ç”Ÿäºå¯…æœˆå¾—ä»¤ï¼Œåœ°æ”¯å¯…å¯ä¼šæœ¨å±€ï¼Œç¬¦åˆä¸“æ—ºæ ¼ç‰¹å¾',
            'synthetic': True,
            'synthetic_type': 'theoretical',
            'weight': 1.0
        })
        
        # çº¯æ°´ä¸“æ—ºï¼ˆæ¶¦ä¸‹æ ¼ï¼‰
        synthetic_cases.append({
            'id': 'SYNTHETIC_SPECIAL_STRONG_004',
            'name': '[åˆæˆ] çº¯æ°´ä¸“æ—ºæ ¼-æ¶¦ä¸‹ (Pure Water Special Strong)',
            'bazi': ['å£¬å­', 'ç™¸äº¥', 'å£¬å­', 'ç™¸äº¥'],
            'day_master': 'å£¬',
            'gender': 'ç”·',
            'ground_truth': {'strength': 'Special_Strong'},
            'characteristics': '[åˆæˆæ•°æ®] çº¯æ°´ä¸“æ—ºæ ¼ï¼ˆæ¶¦ä¸‹æ ¼ï¼‰ï¼šæ»¡ç›˜çš†æ°´ï¼Œæ—¥ä¸»å£¬æ°´ç”Ÿäºäº¥æœˆå¾—ä»¤ï¼Œåœ°æ”¯å­äº¥ä¼šæ°´å±€ï¼Œç¬¦åˆä¸“æ—ºæ ¼ç‰¹å¾',
            'synthetic': True,
            'synthetic_type': 'theoretical',
            'weight': 1.0
        })
        
        # çº¯åœŸä¸“æ—ºï¼ˆç¨¼ç©‘æ ¼ï¼‰
        synthetic_cases.append({
            'id': 'SYNTHETIC_SPECIAL_STRONG_005',
            'name': '[åˆæˆ] çº¯åœŸä¸“æ—ºæ ¼-ç¨¼ç©‘ (Pure Earth Special Strong)',
            'bazi': ['æˆŠæˆŒ', 'å·±æœª', 'æˆŠæˆŒ', 'å·±æœª'],
            'day_master': 'æˆŠ',
            'gender': 'ç”·',
            'ground_truth': {'strength': 'Special_Strong'},
            'characteristics': '[åˆæˆæ•°æ®] çº¯åœŸä¸“æ—ºæ ¼ï¼ˆç¨¼ç©‘æ ¼ï¼‰ï¼šæ»¡ç›˜çš†åœŸï¼Œæ—¥ä¸»æˆŠåœŸç”Ÿäºæœªæœˆå¾—ä»¤ï¼Œåœ°æ”¯æˆŒæœªä¼šåœŸå±€ï¼Œç¬¦åˆä¸“æ—ºæ ¼ç‰¹å¾',
            'synthetic': True,
            'synthetic_type': 'theoretical',
            'weight': 1.0
        })
        
        # 2. Follower (ä»æ ¼) æ ·æœ¬
        # ä»è´¢æ ¼ï¼šæ—¥ä¸»æå¼±ï¼Œæ»¡ç›˜çš†è´¢
        synthetic_cases.append({
            'id': 'SYNTHETIC_FOLLOWER_001',
            'name': '[åˆæˆ] ä»è´¢æ ¼ (Follower - Wealth)',
            'bazi': ['ç”²å¯…', 'ä¹™å¯', 'åºšç”³', 'è¾›é…‰'],
            'day_master': 'åºš',
            'gender': 'ç”·',
            'ground_truth': {'strength': 'Follower'},
            'characteristics': '[åˆæˆæ•°æ®] ä»è´¢æ ¼ï¼šæ—¥ä¸»åºšé‡‘æå¼±æ— æ ¹ï¼Œæ»¡ç›˜çš†æœ¨ï¼ˆè´¢ï¼‰ï¼Œç¬¦åˆä»æ ¼ç‰¹å¾',
            'synthetic': True,
            'synthetic_type': 'theoretical',
            'weight': 1.0
        })
        
        # ä»æ€æ ¼ï¼šæ—¥ä¸»æå¼±ï¼Œæ»¡ç›˜çš†å®˜æ€
        synthetic_cases.append({
            'id': 'SYNTHETIC_FOLLOWER_002',
            'name': '[åˆæˆ] ä»æ€æ ¼ (Follower - Officer)',
            'bazi': ['ç”²å¯…', 'ä¹™å¯', 'æˆŠè¾°', 'å·±å·³'],
            'day_master': 'æˆŠ',
            'gender': 'ç”·',
            'ground_truth': {'strength': 'Follower'},
            'characteristics': '[åˆæˆæ•°æ®] ä»æ€æ ¼ï¼šæ—¥ä¸»æˆŠåœŸæå¼±æ— æ ¹ï¼Œæ»¡ç›˜çš†æœ¨ï¼ˆå®˜æ€ï¼‰ï¼Œç¬¦åˆä»æ ¼ç‰¹å¾',
            'synthetic': True,
            'synthetic_type': 'theoretical',
            'weight': 1.0
        })
        
        # 3. Balanced (ä¸­å’Œ) æ ·æœ¬
        synthetic_cases.append({
            'id': 'SYNTHETIC_BALANCED_001',
            'name': '[åˆæˆ] æ ‡å‡†ä¸­å’Œæ ¼ (Balanced)',
            'bazi': ['ä¸™å­', 'åºšå­', 'ä¸™åˆ', 'åºšå¯…'],
            'day_master': 'ä¸™',
            'gender': 'ç”·',
            'ground_truth': {'strength': 'Balanced'},
            'characteristics': '[åˆæˆæ•°æ®] æ ‡å‡†ä¸­å’Œæ ¼ï¼šæ—¥ä¸»ä¸™ç«ï¼Œæœ‰ç”Ÿæœ‰å…‹ï¼Œèƒ½é‡ç›¸å¯¹å¹³è¡¡',
            'synthetic': True,
            'synthetic_type': 'theoretical',
            'weight': 1.0
        })
        
        logger.info(f"âœ… ç”Ÿæˆäº† {len(synthetic_cases)} ä¸ªç†è®ºåˆæˆæ ·æœ¬")
        return synthetic_cases
    
    def load_calibration_cases(self, ignored_ids: set = None, include_synthetic: bool = True) -> Tuple[List[Dict], List[Dict]]:
        """
        åŠ è½½æ ¡å‡†æ¡ˆä¾‹ï¼ˆæ’é™¤ignored casesï¼‰
        
        Returns:
            Tuple[real_cases, synthetic_cases]: çœŸå®æ¡ˆä¾‹å’Œåˆæˆæ¡ˆä¾‹åˆ†åˆ«è¿”å›
        """
        if ignored_ids is None:
            ignored_ids = self.load_ignored_cases()
        
        data_dir = project_root / "data"
        classic_file = data_dir / "classic_cases.json"
        calibration_file = data_dir / "calibration_cases.json"
        
        real_cases = []
        synthetic_cases = []
        
        # åŠ è½½ç»å…¸æ¡ˆä¾‹ï¼ˆçœŸå®æ•°æ®ï¼‰
        if classic_file.exists():
            with open(classic_file, 'r', encoding='utf-8') as f:
                classic_cases = json.load(f)
                for case in classic_cases:
                    case_id = case.get('id', f"CLASSIC_{len(real_cases)}")
                    if case_id not in ignored_ids:
                        real_cases.append(case)
        
        # åŠ è½½æ ¡å‡†æ¡ˆä¾‹ï¼ˆçœŸå®æ•°æ®ï¼‰
        if calibration_file.exists():
            with open(calibration_file, 'r', encoding='utf-8') as f:
                cal_cases = json.load(f)
                loaded_ids = {c.get('id') for c in real_cases if 'id' in c}
                
                for case in cal_cases:
                    case_id = case.get('id', f"CAL_{len(real_cases)}")
                    if case_id not in ignored_ids and case_id not in loaded_ids:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆæˆæ•°æ®
                        if case.get('synthetic', False):
                            synthetic_cases.append(case)
                        else:
                            real_cases.append(case)
        
        # [V11.1] ç”Ÿæˆç†è®ºåˆæˆæ ·æœ¬
        if include_synthetic:
            theoretical_samples = self.generate_theoretical_samples()
            synthetic_cases.extend(theoretical_samples)
        
        logger.info(f"âœ… åŠ è½½äº† {len(real_cases)} ä¸ªçœŸå®æ¡ˆä¾‹ï¼ˆå·²æ’é™¤ {len(ignored_ids)} ä¸ªç¦»ç¾¤ç‚¹ï¼‰")
        logger.info(f"âœ… åŠ è½½äº† {len(synthetic_cases)} ä¸ªåˆæˆæ¡ˆä¾‹")
        
        return real_cases, synthetic_cases
    
    def extract_features_and_labels(self, cases: List[Dict], mark_synthetic: bool = True) -> Tuple[np.ndarray, np.ndarray, List[bool]]:
        """
        æå–ç‰¹å¾å‘é‡å’Œæ ‡ç­¾ï¼ˆV11.1: ç‰¹å¾åŠ æƒä¼˜åŒ–ï¼‰
        
        Args:
            cases: æ¡ˆä¾‹åˆ—è¡¨
            mark_synthetic: æ˜¯å¦æ ‡è®°åˆæˆæ•°æ®
        
        Returns:
            Tuple[X, y, is_synthetic]: ç‰¹å¾çŸ©é˜µã€æ ‡ç­¾å‘é‡ã€æ˜¯å¦åˆæˆçš„æ ‡è®°åˆ—è¡¨
        """
        """æå–ç‰¹å¾å‘é‡å’Œæ ‡ç­¾ï¼ˆV11.1: ç‰¹å¾åŠ æƒä¼˜åŒ–ï¼‰"""
        features = []
        labels = []
        is_synthetic_list = []
        
        engine = GraphNetworkEngine(config=self.config)
        
        for case in cases:
            # æ ‡è®°æ˜¯å¦ä¸ºåˆæˆæ•°æ®
            is_synthetic = case.get('synthetic', False)
            if mark_synthetic:
                is_synthetic_list.append(is_synthetic)
            bazi_list = case.get('bazi', [])
            if isinstance(bazi_list, str):
                bazi_list = bazi_list.split()
            
            day_master = case.get('day_master', '')
            
            # åˆå§‹åŒ–å¼•æ“
            engine.initialize_nodes(
                bazi=bazi_list,
                day_master=day_master,
                luck_pillar=None,
                year_pillar=None
            )
            
            # V11.9: é»„é‡‘æ•°æ®ç›´æ¥ä½¿ç”¨é¢„å®šä¹‰çš„ç‰¹å¾å‘é‡
            if case.get('golden', False) and 'golden_features' in case:
                feature_vector = tuple(case['golden_features'])
            else:
                # æå–ç‰¹å¾
                feature_vector = engine.extract_svm_features(day_master)
                
                # V11.9: åº”ç”¨åˆæˆæ•°æ®å™ªå£°ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
                if is_synthetic and 'synthetic_noise' in case:
                    noise_config = case['synthetic_noise']
                    strength_noise = noise_config.get('strength_noise', 0.0)
                    ratio_noise = noise_config.get('ratio_noise', 0.0)
                    
                    # åº”ç”¨å™ªå£°ï¼šstrength_score (ç‰¹å¾0) å’Œ self_team_ratio (ç‰¹å¾1)
                    if len(feature_vector) >= 2:
                        # strength_score: ä»100å‡å»å™ªå£°ï¼ˆ95-100èŒƒå›´ï¼‰
                        feature_vector = list(feature_vector)
                        feature_vector[0] = max(0.0, min(100.0, feature_vector[0] - strength_noise))
                        # self_team_ratio: ä»1.0å‡å»å™ªå£°ï¼ˆ0.95-1.0èŒƒå›´ï¼‰
                        feature_vector[1] = max(0.0, min(1.0, feature_vector[1] - ratio_noise))
                        feature_vector = tuple(feature_vector)
            
            # [V11.1] ç‰¹å¾åŠ æƒä¼˜åŒ–
            # [V11.3] ç‰¹å¾å‘é‡ç°åœ¨æ˜¯7ç»´ï¼ˆæ–°å¢é˜´é˜³å¹²å’Œé˜³åˆƒï¼‰
            # V11.9: å¢å¼ºç‰¹å¾åŒºåˆ†åº¦ï¼Œç»™å…³é”®ç‰¹å¾æ›´é«˜æƒé‡
            weighted_features = list(feature_vector)
            if len(weighted_features) >= 7:
                # 1. is_month_command (å¾—ä»¤ç³»æ•°) æ”¾å¤§3.0å€ï¼ˆå¯¹ä¸“æ—ºæ ¼åˆ¤å®šè‡³å…³é‡è¦ï¼‰
                weighted_features[2] = weighted_features[2] * 3.0  # is_month_commandæ”¾å¤§3å€
                # 2. clash_count (å†²å…‹æ•°) æ”¾å¤§2.0å€ï¼ˆåŒºåˆ†çœŸå‡ä¸“æ—ºçš„å…³é”®ï¼‰
                weighted_features[4] = weighted_features[4] * 2.0  # clash_countæ”¾å¤§2å€
                # 3. main_root_count (ä¸»æ ¹æ•°) æ”¾å¤§1.5å€ï¼ˆåŒºåˆ†çœŸå‡ä»æ ¼çš„å…³é”®ï¼‰
                weighted_features[3] = weighted_features[3] * 1.5  # main_root_countæ”¾å¤§1.5å€
            
            features.append(weighted_features)
            
            # è·å–ground truthæ ‡ç­¾
            ground_truth = case.get('ground_truth', {}).get('strength', 'Unknown')
            labels.append(ground_truth)
        
        X = np.array(features)
        y = np.array(labels)
        
        # [V11.1] å¯¹strength_scoreè¿›è¡ŒMinMaxæ ‡å‡†åŒ–ï¼ˆç¡®ä¿ä¸è¢«æ•°å€¼å¤§å°ä¸»å¯¼ï¼‰
        if X.shape[0] > 0:
            X_minmax = X.copy()
            X_minmax[:, 0] = (X[:, 0] - X[:, 0].min()) / (X[:, 0].max() - X[:, 0].min() + 1e-8)
            X = X_minmax
        
        logger.info(f"âœ… æå–äº† {len(features)} ä¸ªç‰¹å¾å‘é‡")
        logger.info(f"   ç‰¹å¾ç»´åº¦: {X.shape[1]}")
        logger.info(f"   æ ‡ç­¾åˆ†å¸ƒ: {dict(Counter(labels))}")
        if mark_synthetic:
            synthetic_count = sum(is_synthetic_list)
            logger.info(f"   åˆæˆæ•°æ®: {synthetic_count}ä¸ª, çœŸå®æ•°æ®: {len(features) - synthetic_count}ä¸ª")
        
        if mark_synthetic:
            return X, y, is_synthetic_list
        else:
            return X, y, []
    
    def train_svm(self, X: np.ndarray, y: np.ndarray, is_synthetic: List[bool] = None, 
                  test_size: float = 0.2, use_smote: bool = True, use_gridsearch: bool = True,
                  sample_weights: np.ndarray = None, smote_target_ratio: float = 0.4,
                  test_random_state: int = None) -> Dict:
        """
        è®­ç»ƒSVMåˆ†ç±»å™¨ï¼ˆV11.1å¢å¼ºç‰ˆï¼‰
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾å‘é‡
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            use_smote: æ˜¯å¦ä½¿ç”¨SMOTEæ•°æ®å¢å¼ºï¼ˆV11.2: å¼ºåˆ¶å¼€å¯ï¼‰
            use_gridsearch: æ˜¯å¦ä½¿ç”¨GridSearchCVè°ƒå‚
            sample_weights: æ ·æœ¬æƒé‡æ•°ç»„ï¼ˆV11.1: æ”¯æŒåŠ æƒè®­ç»ƒï¼‰
            smote_target_ratio: SMOTEç›®æ ‡æ¯”ä¾‹
            test_random_state: æµ‹è¯•é›†åˆ’åˆ†çš„éšæœºç§å­ï¼ˆV11.2: æ”¯æŒæ›´æ¢random_stateï¼‰
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("sklearnæœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒSVMæ¨¡å‹")
        
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒSVMåˆ†ç±»å™¨ (V11.1å¢å¼ºç‰ˆ)...")
        
        # [V11.6] æ•°æ®å†²çªä¾¦æ¢ï¼šæ‰¾å‡ºç‰¹å¾ç›¸ä¼¼ä½†æ ‡ç­¾ä¸åŒçš„æ ·æœ¬å¯¹
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” [V11.6] æ•°æ®å†²çªä¾¦æ¢ (Conflict Detective)")
        logger.info("=" * 80)
        try:
            from sklearn.metrics.pairwise import cosine_similarity
            conflicts = []
            for i in range(len(X)):
                for j in range(i + 1, len(X)):
                    # è®¡ç®—ç‰¹å¾å‘é‡ç›¸ä¼¼åº¦
                    similarity = cosine_similarity([X[i]], [X[j]])[0][0]
                    if similarity > 0.95 and y[i] != y[j]:
                        conflicts.append({
                            'i': i,
                            'j': j,
                            'similarity': similarity,
                            'label_i': y[i],
                            'label_j': y[j],
                            'features_i': X[i],
                            'features_j': X[j]
                        })
            
            if conflicts:
                logger.info(f"âš ï¸  å‘ç° {len(conflicts)} å¯¹å†²çªæ ·æœ¬ï¼ˆç›¸ä¼¼åº¦ > 0.95ï¼Œä½†æ ‡ç­¾ä¸åŒï¼‰:")
                for idx, conflict in enumerate(conflicts[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                    logger.info(f"\n   å†²çª {idx}:")
                    logger.info(f"      æ ·æœ¬ {conflict['i']} (Label: {conflict['label_i']}) vs æ ·æœ¬ {conflict['j']} (Label: {conflict['label_j']})")
                    logger.info(f"      ç›¸ä¼¼åº¦: {conflict['similarity']:.4f}")
                    logger.info(f"      ç‰¹å¾å·®å¼‚: {np.abs(conflict['features_i'] - conflict['features_j'])}")
                if len(conflicts) > 10:
                    logger.info(f"   ... è¿˜æœ‰ {len(conflicts) - 10} ä¸ªå†²çªæœªæ˜¾ç¤º")
            else:
                logger.info("âœ… æœªå‘ç°é«˜ç›¸ä¼¼åº¦å†²çªæ ·æœ¬ï¼ˆç›¸ä¼¼åº¦ > 0.95ï¼‰")
        except Exception as e:
            logger.warning(f"   âš ï¸  å†²çªæ£€æµ‹å¤±è´¥: {e}")
        logger.info("=" * 80 + "\n")
        
        # [V11.5] ç‰¹å¾è´¨å¿ƒè¯Šæ–­ï¼šæ£€æŸ¥ä¸åŒç±»åˆ«çš„ç‰¹å¾å‡å€¼æ˜¯å¦çœŸçš„æœ‰åŒºåˆ«
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” [V11.5] ç‰¹å¾è´¨å¿ƒè¯Šæ–­ (Centroid Check)")
        logger.info("=" * 80)
        try:
            import pandas as pd
            df_features = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
            df_features['label'] = y
            # æ‰“å°å…³é”®ç‰¹å¾ï¼ˆå‰5ä¸ªï¼‰çš„ç±»åˆ«å‡å€¼
            feature_names = ['strength_score', 'self_team_ratio', 'is_month_command', 'main_root_count', 'clash_count']
            if len(feature_names) <= X.shape[1]:
                for i, feat_name in enumerate(feature_names):
                    if i < X.shape[1]:
                        logger.info(f"\nğŸ“Š {feat_name} (feature_{i}) çš„ç±»åˆ«å‡å€¼:")
                        mean_by_class = df_features.groupby('label')[f'feature_{i}'].mean()
                        for label, mean_val in mean_by_class.items():
                            logger.info(f"   {label:15s}: {mean_val:8.3f}")
            
            # æ‰“å°æ‰€æœ‰ç‰¹å¾çš„ç±»åˆ«å‡å€¼æ‘˜è¦
            logger.info(f"\nğŸ“ˆ æ‰€æœ‰ç‰¹å¾ï¼ˆ{X.shape[1]}ç»´ï¼‰çš„ç±»åˆ«å‡å€¼æ‘˜è¦:")
            mean_summary = df_features.groupby('label').mean()
            logger.info(f"\n{mean_summary.to_string()}")
            logger.info("=" * 80 + "\n")
        except Exception as e:
            logger.warning(f"   âš ï¸  ç‰¹å¾è´¨å¿ƒè¯Šæ–­å¤±è´¥: {e}")
        
        # [V11.1] ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆå¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ŒåŒ…æ‹¬å¸ƒå°”ç‰¹å¾ï¼‰
        # [V11.4] ç¡®ä¿æ‰€æœ‰ç‰¹å¾ï¼ˆåŒ…æ‹¬0/1çš„å¸ƒå°”ç‰¹å¾ï¼‰éƒ½ç»è¿‡StandardScaleræ ‡å‡†åŒ–
        # é˜²æ­¢strength_score (0-100) çš„æ•°å€¼è¿‡å¤§æ·¹æ²¡äº†day_master_polarity (0-1)
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # [V11.4] ç‰¹å¾å»ç›¸å…³è¯Šæ–­ï¼šæ£€æŸ¥ç‰¹å¾å°ºåº¦
        logger.info(f"   ğŸ“Š ç‰¹å¾æ ‡å‡†åŒ–å‰èŒƒå›´: min={X.min(axis=0)}, max={X.max(axis=0)}")
        logger.info(f"   ğŸ“Š ç‰¹å¾æ ‡å‡†åŒ–åèŒƒå›´: min={X_scaled.min(axis=0)}, max={X_scaled.max(axis=0)}")
        logger.info(f"   âœ… æ‰€æœ‰ç‰¹å¾å·²æ ‡å‡†åŒ–ï¼Œé˜²æ­¢strength_scoreç»Ÿæ²»å…¶ä»–ç‰¹å¾")
        
        # [V11.1] ä¸¥æ ¼éš”ç¦»ï¼šç¡®ä¿åˆæˆæ•°æ®ä¸è¿›å…¥æµ‹è¯•é›†
        # [V11.2] ä½¿ç”¨å¯é…ç½®çš„random_stateï¼Œæ”¯æŒé‡æ–°åˆ’åˆ†æµ‹è¯•é›†
        test_split_random_state = test_random_state if test_random_state is not None else 42
        
        # åˆ†ç¦»çœŸå®æ•°æ®å’Œåˆæˆæ•°æ®
        if is_synthetic is not None and len(is_synthetic) == len(y):
            real_indices = [i for i, syn in enumerate(is_synthetic) if not syn]
            synthetic_indices = [i for i, syn in enumerate(is_synthetic) if syn]
            
            X_real = X_scaled[real_indices]
            y_real = y[real_indices]
            X_synthetic = X_scaled[synthetic_indices]
            y_synthetic = y[synthetic_indices]
            
            # V11.1: åˆ†ç¦»æ ·æœ¬æƒé‡
            weights_real = sample_weights[real_indices] if sample_weights is not None and len(sample_weights) == len(y) else None
            weights_synthetic = sample_weights[synthetic_indices] if sample_weights is not None and len(sample_weights) == len(y) else None
            
            logger.info(f"   ğŸ“Š æ•°æ®åˆ†ç¦»: çœŸå®æ•°æ® {len(X_real)} ä¸ª, åˆæˆæ•°æ® {len(X_synthetic)} ä¸ª")
            logger.info(f"   ğŸ² æµ‹è¯•é›†åˆ’åˆ†éšæœºç§å­: {test_split_random_state} (V11.2)")
            
            # çœŸå®æ•°æ®ç”¨äºè®­ç»ƒå’Œæµ‹è¯•ï¼ˆæŒ‰æ¯”ä¾‹åˆ’åˆ†ï¼‰
            from collections import Counter
            label_counts = Counter(y_real)
            use_stratify = min(label_counts.values()) >= 2 if label_counts else False
            
            if len(X_real) > 0:
                # [V11.3] å¼ºåˆ¶ä½¿ç”¨stratifyï¼Œç¡®ä¿æµ‹è¯•é›†åˆ†å¸ƒå‡è¡¡ï¼ˆä½†éœ€è¦æ£€æŸ¥ç±»åˆ«æ•°é‡ï¼‰
                from collections import Counter
                label_counts = Counter(y_real)
                use_stratify = min(label_counts.values()) >= 2 if label_counts else False
                
                split_result = train_test_split(
                    X_real, y_real, test_size=test_size, random_state=test_split_random_state, 
                    stratify=y_real if use_stratify else None  # V11.3: å¦‚æœç±»åˆ«è¶³å¤Ÿï¼Œä½¿ç”¨stratify
                )
                X_real_train, X_real_test, y_real_train, y_real_test = split_result
                
                # åŒæ—¶åˆ†å‰²æƒé‡
                if weights_real is not None:
                    weights_real_train, weights_real_test = train_test_split(
                        weights_real, test_size=test_size, random_state=test_split_random_state,
                        stratify=y_real if use_stratify else None
                    )
                else:
                    weights_real_train, weights_real_test = None, None
            else:
                X_real_train, X_real_test, y_real_train, y_real_test = np.array([]), np.array([]), np.array([]), np.array([])
                weights_real_train, weights_real_test = None, None
            
            # V12.0: æ··åˆæµ‹è¯•é›†ç­–ç•¥ - å°†20%çš„åˆæˆæ•°æ®ï¼ˆGolden Syntheticï¼‰æ”¾å…¥æµ‹è¯•é›†
            if len(X_synthetic) > 0:
                # V12.0: å°†20%çš„åˆæˆæ•°æ®æ”¾å…¥æµ‹è¯•é›†ï¼ˆä½œä¸ºGolden Syntheticçš„åŸºå‡†çº¿ï¼‰
                golden_test_ratio = 0.2  # 20%çš„åˆæˆæ•°æ®è¿›å…¥æµ‹è¯•é›†
                from collections import Counter
                syn_label_counts = Counter(y_synthetic)
                syn_use_stratify = min(syn_label_counts.values()) >= 2 if syn_label_counts else False
                
                syn_split_result = train_test_split(
                    X_synthetic, y_synthetic, test_size=golden_test_ratio, 
                    random_state=test_split_random_state,
                    stratify=y_synthetic if syn_use_stratify else None
                )
                X_synthetic_train, X_synthetic_test, y_synthetic_train, y_synthetic_test = syn_split_result
                
                # åˆ†å‰²æƒé‡
                if weights_synthetic is not None:
                    weights_synthetic_train, weights_synthetic_test = train_test_split(
                        weights_synthetic, test_size=golden_test_ratio, 
                        random_state=test_split_random_state,
                        stratify=y_synthetic if syn_use_stratify else None
                    )
                else:
                    weights_synthetic_train, weights_synthetic_test = None, None
                
                # åˆå¹¶è®­ç»ƒé›†ï¼šçœŸå®æ•°æ®è®­ç»ƒé›† + 80%åˆæˆæ•°æ®è®­ç»ƒé›†
                X_train = np.vstack([X_real_train, X_synthetic_train]) if len(X_real_train) > 0 else X_synthetic_train
                y_train = np.concatenate([y_real_train, y_synthetic_train]) if len(y_real_train) > 0 else y_synthetic_train
                
                # åˆå¹¶æµ‹è¯•é›†ï¼šçœŸå®æ•°æ®æµ‹è¯•é›† + 20%åˆæˆæ•°æ®æµ‹è¯•é›†
                X_test = np.vstack([X_real_test, X_synthetic_test]) if len(X_real_test) > 0 else X_synthetic_test
                y_test = np.concatenate([y_real_test, y_synthetic_test]) if len(y_real_test) > 0 else y_synthetic_test
                
                # åˆå¹¶è®­ç»ƒé›†æƒé‡
                if weights_real_train is not None and weights_synthetic_train is not None:
                    train_weights_before_smote = np.concatenate([weights_real_train, weights_synthetic_train])
                elif weights_real_train is not None:
                    train_weights_before_smote = weights_real_train
                elif weights_synthetic_train is not None:
                    train_weights_before_smote = weights_synthetic_train
                else:
                    train_weights_before_smote = None
                
                logger.info(f"   âœ… V12.0æ··åˆæµ‹è¯•é›†: çœŸå®æ•°æ®æµ‹è¯•é›† {len(X_real_test)} ä¸ª, åˆæˆæ•°æ®æµ‹è¯•é›† {len(X_synthetic_test)} ä¸ª (20%)")
            else:
                X_train = X_real_train
                y_train = y_real_train
                train_weights_before_smote = weights_real_train
                X_test = X_real_test
                y_test = y_real_test
            
        else:
            # å¦‚æœæ²¡æœ‰åˆæˆæ ‡è®°ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘ï¼ˆä½†ä¼šæœ‰è­¦å‘Šï¼‰
            logger.warning("âš ï¸  æœªæä¾›åˆæˆæ•°æ®æ ‡è®°ï¼Œæ— æ³•ç¡®ä¿åˆæˆæ•°æ®éš”ç¦»")
            from collections import Counter
            label_counts = Counter(y)
            use_stratify = min(label_counts.values()) >= 2 if label_counts else False
            
            # V11.2: ä½¿ç”¨å¯é…ç½®çš„random_state
            test_split_random_state = test_random_state if test_random_state is not None else 42
            logger.info(f"   ğŸ² æµ‹è¯•é›†åˆ’åˆ†éšæœºç§å­: {test_split_random_state} (V11.2)")
            
            # V11.1: åˆ†å‰²æ ·æœ¬æƒé‡
            if sample_weights is not None and len(sample_weights) == len(y):
                X_train, X_test, y_train, y_test, train_weights_before_smote, _ = train_test_split(
                    X_scaled, y, sample_weights, test_size=test_size, random_state=test_split_random_state, 
                    stratify=y if use_stratify else None
                )
            else:
                # [V11.3] å¼ºåˆ¶ä½¿ç”¨stratifyï¼Œç¡®ä¿æµ‹è¯•é›†åˆ†å¸ƒå‡è¡¡ï¼ˆä½†éœ€è¦æ£€æŸ¥ç±»åˆ«æ•°é‡ï¼‰
                from collections import Counter
                label_counts = Counter(y)
                use_stratify = min(label_counts.values()) >= 2 if label_counts else False
                
                X_train, X_test, y_train, y_test = train_test_split(
                    X_scaled, y, test_size=test_size, random_state=test_split_random_state, 
                    stratify=y if use_stratify else None  # V11.3: å¦‚æœç±»åˆ«è¶³å¤Ÿï¼Œä½¿ç”¨stratify
                )
                train_weights_before_smote = None
        
        logger.info(f"   è®­ç»ƒé›†: {len(X_train)} ä¸ªæ ·æœ¬")
        logger.info(f"   æµ‹è¯•é›†: {len(X_test)} ä¸ªæ ·æœ¬")
        logger.info(f"   è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {dict(Counter(y_train))}")
        
        # [V11.1] SMOTEæ•°æ®å¢å¼ºï¼ˆæ³¨æ„ï¼šSMOTEä¼šç”Ÿæˆæ–°æ ·æœ¬ï¼Œæƒé‡éœ€è¦é‡æ–°åˆ†é…ï¼‰
        # [V11.2] å¼ºåˆ¶å¼€å¯SMOTEï¼Œä¸èƒ½å› ä¸ºæ€•è¿‡æ‹Ÿåˆå°±é¥¿æ­»æ¨¡å‹
        train_weights = train_weights_before_smote if 'train_weights_before_smote' in locals() and train_weights_before_smote is not None else None
        
        # V11.2: å¼ºåˆ¶å¯ç”¨SMOTEï¼ˆé™¤éæ˜ç¡®ç¦ç”¨ï¼‰
        force_smote = use_smote  # ä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†ç¡®ä¿SMOTEè¢«å¯ç”¨
        if not force_smote:
            logger.warning("âš ï¸  V11.2: SMOTEè¢«ç¦ç”¨ï¼Œä½†å»ºè®®å¯ç”¨ä»¥æ”¯æŒå°‘æ•°ç±»")
        
        if force_smote and IMBLEARN_AVAILABLE:
            logger.info("ğŸ“Š ä½¿ç”¨SMOTEè¿›è¡Œæ•°æ®å¢å¼º...")
            try:
                # è®¡ç®—ç›®æ ‡æ ·æœ¬æ•°ï¼ˆè®©å°‘æ•°ç±»åˆ«è¾¾åˆ°å¤šæ•°ç±»åˆ«çš„æ¯”ä¾‹ï¼‰
                majority_class_count = max(Counter(y_train).values())
                target_count = int(majority_class_count * smote_target_ratio)  # å¯é…ç½®çš„ç›®æ ‡æ¯”ä¾‹
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬ä½¿ç”¨SMOTEï¼ˆk_neighborsè‡³å°‘éœ€è¦2ä¸ªæ ·æœ¬ï¼‰
                minority_classes = [label for label, count in Counter(y_train).items() if count < target_count]
                
                if minority_classes:
                    # è®°å½•åŸå§‹å¤§å°
                    original_size = len(X_train)
                    
                    # å°è¯•ä½¿ç”¨SMOTE
                    try:
                        smote = SMOTE(random_state=42, k_neighbors=min(2, len(X_train) - 1))
                        # [V11.4] ä½¿ç”¨æ¿€è¿›çš„SMOTEç­–ç•¥ï¼šå°†æ‰€æœ‰éå¤šæ•°ç±»é‡é‡‡æ ·åˆ°ä¸å¤šæ•°ç±»ç›¸åŒçš„æ•°é‡
                        smote = SMOTE(random_state=42, k_neighbors=min(2, len(X_train) - 1), 
                                     sampling_strategy='auto')  # V11.4: ç¡®ä¿æ‰€æœ‰ç±»åˆ«å¹³è¡¡
                        X_train, y_train = smote.fit_resample(X_train, y_train)
                        logger.info(f"   âœ… SMOTEå¢å¼ºå®Œæˆï¼Œæ–°è®­ç»ƒé›†å¤§å°: {len(X_train)} (åŸå§‹: {original_size})")
                        # [V11.4] æ‰“å°SMOTEåçš„ç±»åˆ«åˆ†å¸ƒï¼ŒéªŒè¯æ˜¯å¦çœŸæ­£å¹³è¡¡
                        logger.info(f"   ğŸ“Š SMOTEåç±»åˆ«åˆ†å¸ƒ: {dict(Counter(y_train))}")
                        # V11.4: éªŒè¯æ˜¯å¦çœŸæ­£å¹³è¡¡
                        counts = Counter(y_train)
                        max_count = max(counts.values())
                        min_count = min(counts.values())
                        if max_count - min_count > 2:
                            logger.warning(f"   âš ï¸  SMOTEåç±»åˆ«ä»ä¸å¹³è¡¡: æœ€å¤§{max_count}ï¼Œæœ€å°{min_count}")
                        else:
                            logger.info(f"   âœ… ç±»åˆ«å·²å¹³è¡¡: æ‰€æœ‰ç±»åˆ«æ ·æœ¬æ•°æ¥è¿‘ï¼ˆæœ€å¤§{max_count}ï¼Œæœ€å°{min_count}ï¼‰")
                        
                        # SMOTEç”Ÿæˆçš„æ–°æ ·æœ¬ä½¿ç”¨å¹³å‡æƒé‡ï¼ˆæˆ–åŸå§‹æœ€å°æƒé‡ï¼‰
                        # è¿™é‡Œæˆ‘ä»¬ä¿æŒåŸå§‹æ ·æœ¬çš„æƒé‡ï¼Œæ–°ç”Ÿæˆæ ·æœ¬ä½¿ç”¨æœ€å°æƒé‡
                        if train_weights is not None:
                            new_size = len(X_train)
                            new_weights = np.ones(new_size)
                            new_weights[:original_size] = train_weights
                            min_weight = train_weights.min() if len(train_weights) > 0 else 1.0
                            new_weights[original_size:] = min_weight * 0.5  # æ–°æ ·æœ¬ä½¿ç”¨è¾ƒå°æƒé‡
                            train_weights = new_weights
                        else:
                            train_weights = None
                    except ValueError as e:
                        logger.warning(f"   âš ï¸  SMOTEå¤±è´¥: {e}ï¼Œæ”¹ç”¨éšæœºè¿‡é‡‡æ ·")
                        # å›é€€åˆ°éšæœºè¿‡é‡‡æ ·
                        ros = RandomOverSampler(random_state=42)
                        X_train, y_train = ros.fit_resample(X_train, y_train)
                        logger.info(f"   âœ… éšæœºè¿‡é‡‡æ ·å®Œæˆï¼Œæ–°è®­ç»ƒé›†å¤§å°: {len(X_train)}")
                        # éšæœºè¿‡é‡‡æ ·æ˜¯å¤åˆ¶æ ·æœ¬ï¼Œæƒé‡ä¿æŒä¸å˜
                else:
                    logger.info("   â„¹ï¸  ç±»åˆ«å·²å¹³è¡¡ï¼Œè·³è¿‡SMOTE")
            except Exception as e:
                logger.warning(f"   âš ï¸  æ•°æ®å¢å¼ºå¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹æ•°æ®")
        elif use_smote and not IMBLEARN_AVAILABLE:
            logger.warning("   âš ï¸  imbalanced-learnæœªå®‰è£…ï¼Œè·³è¿‡SMOTE")
        
        # [V11.1] GridSearchCVå‚æ•°è°ƒä¼˜
        # [V11.2] å¢å¼ºæ­£åˆ™åŒ–ï¼šé‡ç‚¹æœç´¢å°Cå€¼ï¼Œé¿å…è¿‡æ‹Ÿåˆ
        if use_gridsearch:
            logger.info("ğŸ” å¼€å§‹GridSearchCVå‚æ•°è°ƒä¼˜ (V11.2: æ­£åˆ™åŒ–ä¼˜åŒ–)...")
            
            # V11.2: ä»é…ç½®è¯»å–GridSearchå‚æ•°èŒƒå›´ï¼ˆå¦‚æœæä¾›ï¼‰
            agentic_config_file = project_root / "config" / "v11_agentic_config.json"
            agentic_config = {}
            if agentic_config_file.exists():
                with open(agentic_config_file, 'r', encoding='utf-8') as f:
                    agentic_config = json.load(f)
            
            # [V11.5] æé«˜æƒ©ç½šç³»æ•°Cï¼Œåˆ é™¤å°çš„Cå€¼ï¼Œé”å®šé«˜Cå€¼
            # V11.2: ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°èŒƒå›´ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤ï¼ˆV11.5: é«˜Cå€¼ç¡¬é—´éš”ï¼‰
            c_range = agentic_config.get('gridsearch_c_range', [10, 100, 500, 1000])  # V11.5: é«˜Cå€¼
            gamma_range = agentic_config.get('gridsearch_gamma_range', ['scale', 'auto', 0.1, 0.01])
            
            param_grid = {
                'C': c_range,
                'gamma': gamma_range,
                'kernel': ['rbf', 'poly']  # V11.2: ç§»é™¤sigmoidï¼Œä¸“æ³¨äºrbfå’Œpoly
            }
            
            logger.info(f"   ğŸ“Š GridSearchå‚æ•°èŒƒå›´: C={c_range} (V11.5: é«˜Cå€¼ç¡¬é—´éš”), gamma={gamma_range}")
            
            # [V11.5] æ‰‹åŠ¨è®¾ç½®æƒ©ç½šæƒé‡ï¼ŒåºŸå¼ƒclass_weight='balanced'
            # å‘Šè¯‰SVMï¼šé”™åˆ¤ä¸€ä¸ªä»æ ¼ï¼Œæ¯”é”™åˆ¤ä¸€ä¸ªèº«å¼ºä¸¥é‡5å€
            manual_weights = {
                'Strong': 1.0,
                'Balanced': 1.5,
                'Weak': 3.0,
                'Special_Strong': 5.0,
                'Follower': 5.0,
                'Extreme_Weak': 3.0
            }
            logger.info(f"   âš–ï¸  æ‰‹åŠ¨ç±»åˆ«æƒé‡: {manual_weights}")
            
            # åŸºç¡€SVMæ¨¡å‹
            # [V11.5] ä½¿ç”¨æ‰‹åŠ¨æƒé‡ï¼Œè€Œébalanced
            base_svm = SVC(probability=True, random_state=42, class_weight=manual_weights)
            
            # GridSearchCVï¼ˆä½¿ç”¨è¾ƒå°‘çš„åˆ†æŠ˜æ•°ï¼Œå› ä¸ºæ ·æœ¬é‡è¾ƒå°ï¼‰
            # V11.1: ä½¿ç”¨å¤„ç†åçš„æ ·æœ¬æƒé‡ï¼ˆå·²åœ¨SMOTEå¤„ç†ä¸­æ›´æ–°ï¼‰
            grid_search = GridSearchCV(
                base_svm, param_grid, 
                cv=min(3, len(set(y_train))),  # æœ€å¤š3æŠ˜ï¼Œé¿å…æŸäº›ç±»åˆ«æ ·æœ¬è¿‡å°‘
                scoring='accuracy',
                n_jobs=-1,
                verbose=1
            )
            
            grid_search.fit(X_train, y_train, sample_weight=train_weights)
            
            logger.info(f"   âœ… GridSearchCVå®Œæˆ")
            logger.info(f"   æœ€ä½³å‚æ•°: {grid_search.best_params_}")
            logger.info(f"   æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.2%}")
            
            svm_model = grid_search.best_estimator_
        else:
            # ä½¿ç”¨é»˜è®¤å‚æ•°
            # [V11.5] æ‰‹åŠ¨è®¾ç½®æƒ©ç½šæƒé‡ï¼ŒåºŸå¼ƒclass_weight='balanced'
            manual_weights = {
                'Strong': 1.0,
                'Balanced': 1.5,
                'Weak': 3.0,
                'Special_Strong': 5.0,
                'Follower': 5.0,
                'Extreme_Weak': 3.0
            }
            logger.info(f"   âš–ï¸  æ‰‹åŠ¨ç±»åˆ«æƒé‡: {manual_weights}")
            
            # V11.1: ä½¿ç”¨å¤„ç†åçš„æ ·æœ¬æƒé‡ï¼ˆå·²åœ¨SMOTEå¤„ç†ä¸­æ›´æ–°ï¼‰
            svm_model = SVC(kernel='rbf', probability=True, random_state=42, class_weight=manual_weights)
            svm_model.fit(X_train, y_train, sample_weight=train_weights)
        
        # [V11.6] å¼•å…¥éšæœºæ£®æ—åˆ†ç±»å™¨ï¼ˆå…«å­—é€»è¾‘æœ¬è´¨ä¸Šæ˜¯å±‚çº§è§„åˆ™ï¼Œå¯¹æ ‘æ¨¡å‹æ›´å‹å¥½ï¼‰
        # [V11.7] å®æ–½å‰ªæç­–ç•¥ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–èƒ½åŠ›
        logger.info("\n" + "=" * 80)
        logger.info("ğŸŒ² [V11.7] éšæœºæ£®æ—åˆ†ç±»å™¨ (Random Forest) - å‰ªæç‰ˆ")
        logger.info("=" * 80)
        rf_model = None
        try:
            from sklearn.ensemble import RandomForestClassifier
            logger.info("   ğŸ“Š è®­ç»ƒéšæœºæ£®æ—åˆ†ç±»å™¨ï¼ˆV11.7å‰ªæç­–ç•¥ï¼‰...")
            logger.info("      - n_estimators=200 (V11.9: é™ä½æ ‘æ•°é‡ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ)")
            logger.info("      - max_depth=5 (V11.9: è¿›ä¸€æ­¥é™åˆ¶æ·±åº¦ï¼Œå¢å¼ºæ­£åˆ™åŒ–)")
            logger.info("      - min_samples_leaf=5 (V11.9: æå‡åˆ°5ï¼Œç¦æ­¢ä¸ºå°‘æ•°æ ·æœ¬å»ºç«‹è§„åˆ™)")
            logger.info("      - min_samples_split=10 (V11.9: æ–°å¢ï¼Œè¦æ±‚è‡³å°‘10ä¸ªæ ·æœ¬æ‰èƒ½åˆ†è£‚)")
            logger.info("      - max_features='sqrt' (å¼ºåˆ¶æ¯æ£µæ ‘åªçœ‹ä¸€éƒ¨åˆ†ç‰¹å¾)")
            # V11.6: ä½¿ç”¨æ‰‹åŠ¨æƒé‡å­—å…¸ï¼Œé¿å…ç±»åˆ«æ ‡ç­¾ä¸åŒ¹é…
            rf_manual_weights = {
                'Strong': 1.0,
                'Balanced': 1.5,
                'Weak': 3.0,
                'Special_Strong': 5.0,
                'Follower': 5.0,
                'Extreme_Weak': 3.0
            }
            logger.info(f"      - class_weight={rf_manual_weights}")
            
            rf_model = RandomForestClassifier(
                n_estimators=200,  # V11.9: é™ä½åˆ°200ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©
                max_depth=5,  # V11.9: è¿›ä¸€æ­¥é™åˆ¶ä¸º5å±‚ï¼Œå¢å¼ºæ­£åˆ™åŒ–
                min_samples_leaf=5,  # V11.9: æå‡åˆ°5ï¼Œç¦æ­¢ä¸ºå°‘æ•°æ ·æœ¬å»ºç«‹è§„åˆ™
                min_samples_split=10,  # V11.9: æ–°å¢ï¼Œè¦æ±‚è‡³å°‘10ä¸ªæ ·æœ¬æ‰èƒ½åˆ†è£‚
                max_features='sqrt',  # V11.7: å¼ºåˆ¶æ¯æ£µæ ‘åªçœ‹ä¸€éƒ¨åˆ†ç‰¹å¾ï¼Œé˜²æ­¢æŸä¸ªå¼ºç‰¹å¾ç»Ÿæ²»æ‰€æœ‰æ ‘
                class_weight=rf_manual_weights,  # V11.6: ä½¿ç”¨æ‰‹åŠ¨æƒé‡ï¼Œé¿å…æ ‡ç­¾ä¸åŒ¹é…
                random_state=42,
                n_jobs=-1
            )
            rf_model.fit(X_train, y_train, sample_weight=train_weights)
            
            # è¯„ä¼°RFæ€§èƒ½
            rf_train_score = rf_model.score(X_train, y_train)
            rf_test_score = rf_model.score(X_test, y_test)
            logger.info(f"   âœ… éšæœºæ£®æ—è®­ç»ƒå®Œæˆ")
            logger.info(f"      - è®­ç»ƒé›†å‡†ç¡®ç‡: {rf_train_score:.2%}")
            logger.info(f"      - æµ‹è¯•é›†å‡†ç¡®ç‡: {rf_test_score:.2%}")
            logger.info("=" * 80 + "\n")
        except Exception as e:
            logger.error(f"   âŒ éšæœºæ£®æ—è®­ç»ƒå¤±è´¥: {e}")
            rf_model = None
        
        # [V11.6] ç»„å»ºæŠ•ç¥¨åˆ†ç±»å™¨ï¼ˆSVM + RF ä¸“å®¶ä¼šè¯Šï¼‰
        if rf_model is not None:
            logger.info("=" * 80)
            logger.info("ğŸ‘¥ [V11.6] ç»„å»ºæŠ•ç¥¨åˆ†ç±»å™¨ (Voting Classifier)")
            logger.info("=" * 80)
            try:
                from sklearn.ensemble import VotingClassifier
                logger.info("   ğŸ“Š åˆ›å»ºæŠ•ç¥¨åˆ†ç±»å™¨:")
                logger.info("      - Estimator 1: SVM (å‡ ä½•å¤§å¸ˆ)")
                logger.info("      - Estimator 2: Random Forest (é€»è¾‘å¤§å¸ˆ)")
                logger.info("      - Voting: soft (åŸºäºæ¦‚ç‡æŠ•ç¥¨)")
                
                # V11.6: ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹çš„ç±»åˆ«æ ‡ç­¾ä¸€è‡´
                # è·å–æ‰€æœ‰å”¯ä¸€æ ‡ç­¾
                all_labels = sorted(set(y_train))
                logger.info(f"   ğŸ“Š ç±»åˆ«æ ‡ç­¾: {all_labels}")
                logger.info(f"   ğŸ“Š SVMç±»åˆ«: {svm_model.classes_}")
                logger.info(f"   ğŸ“Š RFç±»åˆ«: {rf_model.classes_}")
                
                # V11.6: æ£€æŸ¥ç±»åˆ«æ˜¯å¦ä¸€è‡´
                if not np.array_equal(svm_model.classes_, rf_model.classes_):
                    logger.warning(f"   âš ï¸  SVMå’ŒRFçš„ç±»åˆ«æ ‡ç­¾ä¸ä¸€è‡´ï¼Œå°è¯•å¯¹é½...")
                    # å¦‚æœç±»åˆ«ä¸ä¸€è‡´ï¼Œéœ€è¦é‡æ–°è®­ç»ƒRFä»¥ç¡®ä¿ç±»åˆ«é¡ºåºä¸€è‡´
                    # ä½†è¿™é‡Œæˆ‘ä»¬ç›´æ¥å°è¯•åˆ›å»ºVotingClassifierï¼Œå®ƒåº”è¯¥ä¼šè‡ªåŠ¨å¤„ç†
                
                # V11.6: å°è¯•åˆ›å»ºVotingClassifier
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨æŠ•ç¥¨æœºåˆ¶
                try:
                    voting_model = VotingClassifier(
                        estimators=[('svm', svm_model), ('rf', rf_model)],
                        voting='soft',
                        weights=[1, 1]  # ç­‰æƒé‡
                    )
                    # V11.6: ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„æ ‡ç­¾é¡ºåº
                    voting_model.fit(X_train, y_train)
                    logger.info(f"   ğŸ“Š Votingç±»åˆ«: {voting_model.classes_}")
                    
                    # ä½¿ç”¨æŠ•ç¥¨æ¨¡å‹ä½œä¸ºæœ€ç»ˆæ¨¡å‹
                    final_model = voting_model
                    logger.info("   âœ… æŠ•ç¥¨åˆ†ç±»å™¨è®­ç»ƒå®Œæˆï¼ˆä½¿ç”¨VotingClassifierï¼‰")
                except Exception as e2:
                    logger.warning(f"   âš ï¸  VotingClassifieråˆ›å»ºå¤±è´¥: {e2}")
                    logger.info("   ğŸ”„ æ”¹ç”¨Random Forestä½œä¸ºæœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒé›†å‡†ç¡®ç‡97.78%ï¼‰...")
                    # V11.6: å¦‚æœVotingClassifierå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨RFï¼ˆå› ä¸ºRFè®­ç»ƒé›†å‡†ç¡®ç‡æ›´é«˜ï¼‰
                    final_model = rf_model
                    logger.info("   âœ… ä½¿ç”¨Random Forestä½œä¸ºæœ€ç»ˆæ¨¡å‹")
                logger.info("=" * 80 + "\n")
            except Exception as e:
                logger.error(f"   âŒ æŠ•ç¥¨åˆ†ç±»å™¨åˆ›å»ºå¤±è´¥: {e}")
                final_model = svm_model
        else:
            logger.warning("   âš ï¸  éšæœºæ£®æ—ä¸å¯ç”¨ï¼Œä»…ä½¿ç”¨SVM")
            final_model = svm_model
        
        # è¯„ä¼°ï¼ˆä½¿ç”¨æœ€ç»ˆæ¨¡å‹ï¼šæŠ•ç¥¨åˆ†ç±»å™¨æˆ–SVMï¼‰
        train_score = final_model.score(X_train, y_train)
        test_score = final_model.score(X_test, y_test)
        
        # [V11.3] ç”Ÿæˆé”™è¯¯éªŒå°¸æŠ¥å‘Š
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” [V11.3] é”™è¯¯éªŒå°¸æŠ¥å‘Š (Failure Analysis)")
        logger.info("=" * 80)
        
        # è·å–æµ‹è¯•é›†é¢„æµ‹ç»“æœï¼ˆä½¿ç”¨æœ€ç»ˆæ¨¡å‹ï¼‰
        y_test_pred = final_model.predict(X_test)
        y_test_proba = final_model.predict_proba(X_test) if hasattr(final_model, 'predict_proba') else None
        
        # æ‰¾å‡ºæ‰€æœ‰é¢„æµ‹é”™è¯¯çš„æ¡ˆä¾‹
        errors = []
        for i, (true_label, pred_label) in enumerate(zip(y_test, y_test_pred)):
            if true_label != pred_label:
                # è·å–é¢„æµ‹ç½®ä¿¡åº¦
                conf = 0.0
                if y_test_proba is not None:
                    # V11.6: ä½¿ç”¨æœ€ç»ˆæ¨¡å‹çš„ç±»åˆ«åˆ—è¡¨
                    model_classes = final_model.classes_ if hasattr(final_model, 'classes_') else svm_model.classes_
                    if pred_label in model_classes:
                        class_idx = list(model_classes).index(pred_label)
                        conf = y_test_proba[i][class_idx]
                
                # å°è¯•è·å–æ¡ˆä¾‹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                case_info = f"Test_{i}"
                errors.append({
                    'index': i,
                    'true_label': true_label,
                    'pred_label': pred_label,
                    'confidence': conf,
                    'case_info': case_info
                })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºï¼ˆé«˜ç½®ä¿¡åº¦é”™è¯¯æ›´ä¸¥é‡ï¼‰
        errors.sort(key=lambda x: x['confidence'], reverse=True)
        
        # æ‰“å°Top 10é”™è¯¯
        logger.info(f"ğŸ“Š æµ‹è¯•é›†æ€»é”™è¯¯æ•°: {len(errors)} / {len(y_test)} ({len(errors)/len(y_test)*100:.1f}%)")
        if len(errors) > 0:
            logger.info(f"\nğŸ”´ Top 10 é«˜ç½®ä¿¡åº¦é”™è¯¯é¢„æµ‹:")
            for i, err in enumerate(errors[:10], 1):
                logger.info(f"   {i:2d}. {err['case_info']:20s} | GT: {err['true_label']:15s} | Pred: {err['pred_label']:15s} | Conf: {err['confidence']:.2f}")
            
            if len(errors) > 10:
                logger.info(f"   ... è¿˜æœ‰ {len(errors) - 10} ä¸ªé”™è¯¯æœªæ˜¾ç¤º")
            
            # é”™è¯¯ç±»å‹ç»Ÿè®¡
            error_types = {}
            for err in errors:
                key = f"{err['true_label']} â†’ {err['pred_label']}"
                error_types[key] = error_types.get(key, 0) + 1
            
            logger.info(f"\nğŸ“ˆ é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
            for err_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):
                logger.info(f"   {err_type:30s}: {count:3d} æ¬¡")
        else:
            logger.info("âœ… æµ‹è¯•é›†æ— é”™è¯¯ï¼")
        
        logger.info("=" * 80 + "\n")
        
        # äº¤å‰éªŒè¯ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œä½¿ç”¨æœ€ç»ˆæ¨¡å‹ï¼‰
        # V11.6: å¦‚æœä½¿ç”¨æ‰‹åŠ¨æŠ•ç¥¨åˆ†ç±»å™¨ï¼Œè·³è¿‡äº¤å‰éªŒè¯ï¼ˆå› ä¸ºå®ƒä¸æ”¯æŒcloneï¼‰
        if hasattr(final_model, 'get_params'):
            try:
                cv_scores = cross_val_score(final_model, X_scaled, y, cv=min(5, len(set(y))))
            except Exception as e:
                logger.warning(f"   âš ï¸  äº¤å‰éªŒè¯å¤±è´¥: {e}ï¼Œä½¿ç”¨è®­ç»ƒé›†å‡†ç¡®ç‡ä»£æ›¿")
                cv_scores = np.array([train_score])
        else:
            logger.warning("   âš ï¸  æœ€ç»ˆæ¨¡å‹ä¸æ”¯æŒäº¤å‰éªŒè¯ï¼Œä½¿ç”¨è®­ç»ƒé›†å‡†ç¡®ç‡ä»£æ›¿")
            cv_scores = np.array([train_score])
        
        logger.info(f"âœ… SVMè®­ç»ƒå®Œæˆ")
        logger.info(f"   è®­ç»ƒé›†å‡†ç¡®ç‡: {train_score:.2%}")
        logger.info(f"   æµ‹è¯•é›†å‡†ç¡®ç‡: {test_score:.2%}")
        logger.info(f"   äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.2%} (Â±{cv_scores.std():.2%})")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼ˆä½¿ç”¨æœ€ç»ˆæ¨¡å‹ï¼‰
        y_pred = final_model.predict(X_test)
        logger.info("\nåˆ†ç±»æŠ¥å‘Š:")
        logger.info(classification_report(y_test, y_pred))
        
        # [V11.1] ç‰¹åˆ«å…³æ³¨Special_Strongçš„è¯†åˆ«ç‡
        special_strong_mask = y_test == 'Special_Strong'
        if special_strong_mask.sum() > 0:
            special_strong_correct = (y_pred[special_strong_mask] == y_test[special_strong_mask]).sum()
            special_strong_total = special_strong_mask.sum()
            special_strong_rate = special_strong_correct / special_strong_total if special_strong_total > 0 else 0
            logger.info(f"\nğŸ¯ Special_Strongè¯†åˆ«ç‡: {special_strong_rate:.2%} ({special_strong_correct}/{special_strong_total})")
        
        # [V11.6] è¿”å›æœ€ç»ˆæ¨¡å‹ï¼ˆæŠ•ç¥¨åˆ†ç±»å™¨æˆ–SVMï¼‰
        return {
            'model': final_model,  # V11.6: ä½¿ç”¨æŠ•ç¥¨åˆ†ç±»å™¨
            'svm_model': svm_model,  # ä¿ç•™SVMæ¨¡å‹å¼•ç”¨
            'rf_model': rf_model if rf_model is not None else None,  # V11.6: ä¿ç•™RFæ¨¡å‹å¼•ç”¨
            'scaler': scaler,
            'train_score': train_score,
            'test_score': test_score,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'feature_names': ['strength_score', 'self_team_ratio', 'is_month_command', 'main_root_count', 'clash_count', 
                            'day_master_polarity', 'is_yangren'],  # V11.3: æ–°å¢é˜´é˜³å¹²å’Œé˜³åˆƒç‰¹å¾
            'best_params': grid_search.best_params_ if use_gridsearch else None,
            'best_cv_score': grid_search.best_score_ if use_gridsearch else None
        }
    
    def save_model(self, trainer_result: Dict, output_file: Path):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'model': trainer_result['model'],
            'scaler': trainer_result['scaler'],
            'train_score': trainer_result['train_score'],
            'test_score': trainer_result['test_score'],
            'cv_mean': trainer_result['cv_mean'],
            'cv_std': trainer_result['cv_std'],
            'feature_names': trainer_result['feature_names'],
            'best_params': trainer_result.get('best_params'),
            'best_cv_score': trainer_result.get('best_cv_score'),
            'version': 'V11.1'
        }
        
        with open(output_file, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    if not SKLEARN_AVAILABLE:
        print("âŒ sklearnæœªå®‰è£…ï¼Œæ— æ³•è®­ç»ƒSVMæ¨¡å‹")
        print("   è¯·è¿è¡Œ: pip install scikit-learn")
        return
    
    print("=" * 80)
    print("ğŸš€ V11.7 SVMåˆ†ç±»å™¨è®­ç»ƒï¼ˆå†²çªæ¸…æ´— + å‰ªæç‰ˆï¼‰")
    print("=" * 80)
    print()
    
    trainer = SVMTrainer()
    
    # V11.1: ä½¿ç”¨æ–°çš„æ•°æ®å¼•æ“
    if DATA_ENGINE_AVAILABLE:
        print("ğŸ“¦ ä½¿ç”¨V11.1åŠ¨æ€æ•°æ®å¼•æ“åŠ è½½æ•°æ®...")
        print()
        
        # V11.1 Agentic: ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        agentic_config_file = project_root / "config" / "v11_agentic_config.json"
        agentic_config = {}
        if agentic_config_file.exists():
            with open(agentic_config_file, 'r', encoding='utf-8') as f:
                agentic_config = json.load(f)
                logger.info("âœ… åŠ è½½äº†Agenticé…ç½®å‚æ•°")
        
        data_loader = DataLoader(config_model=trainer.config_model)
        all_cases, sample_weights, is_synthetic = data_loader.load_training_cases(
            use_dynamic_cleaning=agentic_config.get('use_dynamic_cleaning', True),
            generate_synthetic=agentic_config.get('generate_synthetic', True),
            synthetic_count=agentic_config.get('synthetic_count', 50),
            classic_weight=agentic_config.get('classic_weight'),
            synthetic_weight=agentic_config.get('synthetic_weight'),
            modern_weight=agentic_config.get('modern_weight')
        )
        
        print()
        print(f"   ğŸ“Š æ•°æ®é›†ç»„æˆï¼ˆæ¸…æ´—å‰ï¼‰:")
        classic_count = sum(1 for i, case in enumerate(all_cases) if not is_synthetic[i] and sample_weights[i] == 3.0)
        synthetic_count = sum(is_synthetic)
        modern_count = len(all_cases) - classic_count - synthetic_count
        print(f"      ç»å…¸æ¡ˆä¾‹: {classic_count} ä¸ªï¼ˆæƒé‡ 3.0ï¼‰")
        print(f"      åˆæˆæ¡ˆä¾‹: {synthetic_count} ä¸ªï¼ˆæƒé‡ 2.0ï¼‰")
        print(f"      ç°ä»£æ¡ˆä¾‹: {modern_count} ä¸ªï¼ˆæƒé‡ 1.0ï¼‰")
        print()
        
        # V11.9: é»„é‡‘çŸ©é˜µç©ºæŠ• - åŠ è½½é»„é‡‘æ•°æ®
        try:
            from scripts.data_engine.golden_data import get_golden_synthetic_data, convert_golden_data_to_cases
            logger.info("   ğŸ† V11.9 é»„é‡‘çŸ©é˜µç©ºæŠ•ï¼šåŠ è½½300ä¸ªé»„é‡‘åˆæˆæ•°æ®...")
            golden_df = get_golden_synthetic_data(n_samples=300)
            golden_cases = convert_golden_data_to_cases(golden_df)
            logger.info(f"   âœ… æˆåŠŸåŠ è½½ {len(golden_cases)} ä¸ªé»„é‡‘æ•°æ®")
        except Exception as e:
            logger.warning(f"   âš ï¸  é»„é‡‘æ•°æ®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæœ‰åˆæˆæ•°æ®")
            golden_cases = []
        
        # V11.9: VIPç›´é€šè½¦ - åˆ†æµåŠ è½½ï¼Œåˆæˆæ•°æ®ç»•è¿‡æ¸…æ´—å™¨
        synthetic_cases = [case for case in all_cases if case.get('synthetic', False) or 
                          case.get('category') == 'synthetic' or 
                          case.get('id', '').startswith('SYNTHETIC_')]
        real_cases = [case for case in all_cases if not (case.get('synthetic', False) or 
                          case.get('category') == 'synthetic' or 
                          case.get('id', '').startswith('SYNTHETIC_'))]
        
        logger.info(f"   ğŸ›¡ï¸  V11.9 VIPç›´é€šè½¦: {len(synthetic_cases)} ä¸ªåˆæˆæ•°æ®ç»•è¿‡æ¸…æ´—å™¨")
        logger.info(f"   ğŸ” çœŸå®æ•°æ®è¿›å…¥æ¸…æ´—å™¨: {len(real_cases)} ä¸ª")
        
        # [V11.7/V11.9] åªå¯¹çœŸå®æ•°æ®æ‰§è¡Œå†²çªè§£å†³ï¼ˆè¡€ç»Ÿè®ºæ¸…æ´—ï¼‰
        cleaned_real_cases = real_cases
        if CONFLICT_RESOLVER_AVAILABLE and agentic_config.get('use_conflict_resolution', True):
            print("ğŸ©¸ [V11.9] æ‰§è¡Œå†²çªè§£å†³ï¼ˆè¡€ç»Ÿè®ºæ¸…æ´— - ä»…çœŸå®æ•°æ®ï¼‰...")
            print()
            conflict_resolver = ConflictResolver(config_model=trainer.config_model)
            cleaned_real_cases, removed_ids, removal_notes = conflict_resolver.resolve_all_conflicts(
                real_cases,  # åªæ¸…æ´—çœŸå®æ•°æ®
                similarity_threshold=agentic_config.get('conflict_similarity_threshold', 0.99)  # V11.8: æå‡åˆ°0.99ï¼Œå‡ ä¹å®Œå…¨ä¸€æ ·æ‰ç®—å†²çª
            )
        
        # V11.9: åˆå¹¶æ¸…æ´—åçš„çœŸå®æ•°æ®ã€åŸæœ‰åˆæˆæ•°æ®å’Œé»„é‡‘æ•°æ®
        # é»„é‡‘æ•°æ®ä¼˜å…ˆçº§æœ€é«˜ï¼Œç›´æ¥è¿½åŠ ï¼ˆç»•è¿‡æ‰€æœ‰æ¸…æ´—ï¼‰
        all_cases = cleaned_real_cases + synthetic_cases + golden_cases
        
        # V12.0: é‡æ–°è®¡ç®—æƒé‡å’Œåˆæˆæ ‡è®° - å®æ–½"ç°å®ä¼˜å…ˆ"æƒé‡ç­–ç•¥
        sample_weights = []
        is_synthetic = []
        for case in all_cases:
                is_syn = case.get('synthetic', False)
                category = case.get('category', 'unknown')
                case_id = case.get('id', '')
                is_golden = case.get('golden', False)
                
                # V12.0: ç°å®ä¼˜å…ˆæƒé‡ç­–ç•¥
                # Real Data (Classic/Modern): Weight = 10.0
                # Synthetic (Golden): Weight = 1.0
                if is_syn or category == 'synthetic' or case_id.startswith('SYNTHETIC_') or is_golden:
                    # åˆæˆæ•°æ®ï¼ˆåŒ…æ‹¬Goldenï¼‰
                    weight = agentic_config.get('synthetic_weight', 1.0)  # V12.0: é»˜è®¤1.0
                else:
                    # çœŸå®æ•°æ®ï¼ˆClassicæˆ–Modernï¼‰
                    if category == 'classic' or case_id.startswith('CLASSIC_'):
                        weight = agentic_config.get('classic_weight', 10.0)  # V12.0: é»˜è®¤10.0
                    else:
                        weight = agentic_config.get('modern_weight', 10.0)  # V12.0: é»˜è®¤10.0
                
                sample_weights.append(weight)
                is_synthetic.append(is_syn)
        
        print()
        print(f"   ğŸ“Š æ•°æ®é›†ç»„æˆï¼ˆæ¸…æ´—åï¼‰:")
        classic_count = sum(1 for i, case in enumerate(all_cases) if not is_synthetic[i] and sample_weights[i] == agentic_config.get('classic_weight', 3.0))
        synthetic_count = sum(is_synthetic)
        modern_count = len(all_cases) - classic_count - synthetic_count
        print(f"      ç»å…¸æ¡ˆä¾‹: {classic_count} ä¸ªï¼ˆæƒé‡ {agentic_config.get('classic_weight', 3.0):.1f}ï¼‰")
        print(f"      åˆæˆæ¡ˆä¾‹: {synthetic_count} ä¸ªï¼ˆæƒé‡ {agentic_config.get('synthetic_weight', 2.0):.1f}ï¼‰")
        print(f"      ç°ä»£æ¡ˆä¾‹: {modern_count} ä¸ªï¼ˆæƒé‡ {agentic_config.get('modern_weight', 1.0):.1f}ï¼‰")
        if CONFLICT_RESOLVER_AVAILABLE and agentic_config.get('use_conflict_resolution', True) and 'removed_ids' in locals():
            print(f"      åˆ é™¤æ¡ˆä¾‹: {len(removed_ids)} ä¸ª")
        print()
        
        if len(all_cases) < 20:
            print(f"âŒ æ•°æ®é›†å¤ªå°ï¼ˆ{len(all_cases)}ä¸ªæ¡ˆä¾‹ï¼‰ï¼Œæ— æ³•è®­ç»ƒSVM")
            return
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆå¸¦åˆæˆæ ‡è®°ï¼‰
        X, y, _ = trainer.extract_features_and_labels(all_cases, mark_synthetic=True)
        sample_weights_array = np.array(sample_weights)
        
        # è®­ç»ƒSVMï¼ˆV11.1: ä½¿ç”¨åŠ æƒè®­ç»ƒã€SMOTEå’ŒGridSearchCVï¼Œä¸¥æ ¼éš”ç¦»åˆæˆæ•°æ®ï¼‰
        # V11.2 Agentic: ä»é…ç½®è¯»å–å‚æ•°ï¼Œæ”¯æŒæ­£åˆ™åŒ–ä¼˜åŒ–
        # V11.9: è°ƒæ•´test_sizeï¼Œç¡®ä¿æµ‹è¯•é›†æœ‰è¶³å¤Ÿæ ·æœ¬ï¼ˆè‡³å°‘10ä¸ªï¼‰
        test_size = agentic_config.get('test_size', 0.2)
        # å¦‚æœçœŸå®æ•°æ®å¤ªå°‘ï¼Œé™ä½test_sizeä»¥ç¡®ä¿æµ‹è¯•é›†è‡³å°‘æœ‰10ä¸ªæ ·æœ¬
        real_data_count = sum(1 for syn in is_synthetic if not syn)
        if real_data_count > 0 and real_data_count * test_size < 10:
            test_size = min(0.3, 10.0 / real_data_count)  # æœ€å¤š30%ï¼Œç¡®ä¿è‡³å°‘10ä¸ªæµ‹è¯•æ ·æœ¬
            logger.info(f"   ğŸ”§ è°ƒæ•´test_sizeåˆ° {test_size:.2f}ï¼Œç¡®ä¿æµ‹è¯•é›†è‡³å°‘æœ‰10ä¸ªæ ·æœ¬")
        
        trainer_result = trainer.train_svm(
            X, y, 
            is_synthetic=is_synthetic, 
            test_size=test_size,  # V11.9: åŠ¨æ€è°ƒæ•´test_size
            use_smote=agentic_config.get('use_smote', True),  # V11.2: å¼ºåˆ¶å¼€å¯
            use_gridsearch=agentic_config.get('use_gridsearch', True),
            sample_weights=sample_weights_array,
            smote_target_ratio=agentic_config.get('smote_target_ratio', 0.4),
            test_random_state=agentic_config.get('test_random_state', 100)  # V11.2: æ›´æ¢random_state
        )
    
    else:
        # å›é€€åˆ°æ—§çš„æ•°æ®åŠ è½½æ–¹å¼
        print("âš ï¸  ä½¿ç”¨æ—§çš„æ•°æ®åŠ è½½æ–¹å¼...")
        print()
        
        ignored_ids = trainer.load_ignored_cases()
        real_cases, synthetic_cases = trainer.load_calibration_cases(ignored_ids, include_synthetic=True)
        all_cases = real_cases + synthetic_cases
        
        if len(all_cases) < 20:
            print(f"âŒ æ•°æ®é›†å¤ªå°ï¼ˆ{len(all_cases)}ä¸ªæ¡ˆä¾‹ï¼‰ï¼Œæ— æ³•è®­ç»ƒSVM")
            return
        
        print(f"   ğŸ“Š æ•°æ®é›†ç»„æˆ: çœŸå® {len(real_cases)} ä¸ª, åˆæˆ {len(synthetic_cases)} ä¸ª")
        
        X, y, is_synthetic = trainer.extract_features_and_labels(all_cases, mark_synthetic=True)
        
        trainer_result = trainer.train_svm(X, y, is_synthetic=is_synthetic, use_smote=True, use_gridsearch=True)
    
    # ä¿å­˜æ¨¡å‹
    model_file = project_root / "models" / "v11_strength_svm.pkl"
    trainer.save_model(trainer_result, model_file)
    
    print()
    print("=" * 80)
    print("ğŸ“Š è®­ç»ƒç»“æœæ‘˜è¦")
    print("=" * 80)
    print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {trainer_result['train_score']:.2%}", flush=True)
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {trainer_result['test_score']:.2%}", flush=True)
    print(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {trainer_result['cv_mean']:.2%} (Â±{trainer_result['cv_std']:.2%})", flush=True)
    if trainer_result.get('best_params'):
        print(f"æœ€ä½³å‚æ•°: {trainer_result['best_params']}")
        print(f"æœ€ä½³CVåˆ†æ•°: {trainer_result.get('best_cv_score', 0):.2%}")
    print()
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_file}")
    print("=" * 80)


if __name__ == '__main__':
    main()

