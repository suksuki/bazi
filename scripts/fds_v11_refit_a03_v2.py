#!/usr/bin/env python3
"""
FDS-V1.1 é‡æ–°æ‹Ÿåˆè„šæœ¬ï¼šA-03 ç¾Šåˆƒæ¶æ€ï¼ˆå¯¹é½V2.0è§„èŒƒï¼‰
æ‰§è¡ŒStep 2-5çš„å®Œæ•´æ‹Ÿåˆå·¥ä½œï¼Œé‡ç‚¹ï¼š
- Step 3: è®¡ç®—è´¨å¿ƒå¹¶å½’ä¸€åŒ–
- Step 5: æ›´æ–°feature_anchorsåˆ°æ³¨å†Œè¡¨
"""

import sys
from pathlib import Path
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from controllers.holographic_pattern_controller import HolographicPatternController
from core.trinity.core.unified_arbitrator_master import QuantumUniversalFramework
from core.registry_loader import RegistryLoader
from core.math_engine import (
    calculate_centroid,
    tensor_normalize,
    calculate_cosine_similarity
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FDSV11RefitEngine:
    """
    FDS-V1.1 é‡æ–°æ‹Ÿåˆå¼•æ“ï¼ˆV2.0å¯¹é½ç‰ˆï¼‰
    æ‰§è¡ŒStep 2-5çš„æ‹Ÿåˆå·¥ä½œï¼Œé‡ç‚¹è®¡ç®—è´¨å¿ƒå’Œæ›´æ–°feature_anchors
    """
    
    def __init__(self, pattern_id: str = 'A-03'):
        self.pattern_id = pattern_id
        self.controller = HolographicPatternController()
        self.framework = QuantumUniversalFramework()
        self.registry_loader = RegistryLoader()
        self.pattern = self.controller.get_pattern_by_id(pattern_id)
        
        if not self.pattern:
            raise ValueError(f"æ ¼å±€ {pattern_id} ä¸å­˜åœ¨")
        
        logger.info(f"åˆå§‹åŒ–FDS-V1.1é‡æ–°æ‹Ÿåˆå¼•æ“: {pattern_id}")
    
    def load_existing_fitting_data(self) -> Dict[str, Any]:
        """
        åŠ è½½ç°æœ‰çš„æ‹Ÿåˆæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        
        Returns:
            æ‹Ÿåˆæ•°æ®å­—å…¸ï¼ŒåŒ…å«step3çš„results
        """
        fitting_file = project_root / "data" / "holographic_pattern" / "A-03_FDS_Fitting_Results.json"
        
        if fitting_file.exists():
            logger.info(f"åŠ è½½ç°æœ‰æ‹Ÿåˆæ•°æ®: {fitting_file}")
            with open(fitting_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            logger.warning("æœªæ‰¾åˆ°ç°æœ‰æ‹Ÿåˆæ•°æ®ï¼Œéœ€è¦é‡æ–°æ‰§è¡ŒStep 2")
            return {}
    
    def step3_calculate_centroids(self, fitting_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Step 3: å¤šç»´ç‰¹å¾æå–ä¸é”šç‚¹é”å®š
        è®¡ç®—Tier Aå’ŒTier Xçš„è´¨å¿ƒ
        
        Args:
            fitting_data: ç°æœ‰æ‹Ÿåˆæ•°æ®ï¼ˆåŒ…å«step3.resultsï¼‰
            
        Returns:
            è´¨å¿ƒè®¡ç®—ç»“æœ
        """
        logger.info("=" * 70)
        logger.info("Step 3: å¤šç»´ç‰¹å¾æå–ä¸é”šç‚¹é”å®šï¼ˆè®¡ç®—è´¨å¿ƒï¼‰")
        logger.info("=" * 70)
        
        step3_data = fitting_data.get('step3', {})
        results = step3_data.get('results', [])
        
        if not results:
            raise ValueError("æœªæ‰¾åˆ°Step 3çš„æ‹Ÿåˆç»“æœï¼Œè¯·å…ˆæ‰§è¡ŒStep 2å’ŒStep 3")
        
        logger.info(f"å¤„ç† {len(results)} ä¸ªæ ·æœ¬...")
        
        # 1. æå–Tier Aæ ·æœ¬çš„5ç»´æŠ•å½±å€¼ï¼ˆéœ€è¦å½’ä¸€åŒ–ï¼‰
        tier_a_projections = []
        tier_x_projections = []
        
        for result in results:
            # ä¼˜å…ˆä½¿ç”¨frequency_vectoré‡æ–°è®¡ç®—projectionï¼ˆåŸºäºå®é™…èƒ½é‡åˆ†å¸ƒï¼‰
            # å¦‚æœfrequency_vectorä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨åŸå§‹projection
            frequency_vector = result.get('frequency_vector', {})
            
            if frequency_vector:
                # åŸºäºfrequency_vectorè®¡ç®—5ç»´æŠ•å½±
                # æ˜ å°„å…³ç³»ï¼š
                # E (èƒ½çº§è½´): æ¯”åŠ« + å°æ­ï¼ˆè‡ªæˆ‘å›¢é˜Ÿ+èµ„æºæ”¯æŒ = æ ¹åŸºï¼‰
                # O (ç§©åºè½´): å®˜æ€ï¼ˆæƒåŠ›ã€åœ°ä½ï¼‰
                # M (ç‰©è´¨è½´): è´¢æ˜Ÿï¼ˆè´¢å¯Œã€èµ„äº§ï¼‰
                # S (åº”åŠ›è½´): å®˜æ€ - å°æ­ï¼ˆå‹åŠ› - æ”¯æ’‘ = åº”åŠ›ï¼‰
                # R (å…³è”è½´): é£Ÿä¼¤ï¼ˆè¾“å‡ºã€è¡¨è¾¾ = äººé™…ï¼‰
                
                bi_jie = frequency_vector.get('æ¯”åŠ«', 0.0)
                shi_shang = frequency_vector.get('é£Ÿä¼¤', 0.0)
                cai_xing = frequency_vector.get('è´¢æ˜Ÿ', 0.0)
                guan_sha = frequency_vector.get('å®˜æ€', 0.0)
                yin_xiao = frequency_vector.get('å°æ­', 0.0)
                
                # è®¡ç®—æ€»èƒ½é‡ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
                total_energy = bi_jie + shi_shang + cai_xing + guan_sha + yin_xiao
                
                if total_energy > 0:
                    # è®¡ç®—5ç»´æŠ•å½±ï¼ˆå½’ä¸€åŒ–ï¼‰
                    projection = {
                        'E': (bi_jie + yin_xiao) / total_energy,  # èƒ½çº§è½´ï¼šæ ¹åŸº
                        'O': guan_sha / total_energy,              # ç§©åºè½´ï¼šæƒåŠ›
                        'M': cai_xing / total_energy,              # ç‰©è´¨è½´ï¼šè´¢å¯Œ
                        'S': max(0, (guan_sha - yin_xiao)) / total_energy if total_energy > 0 else 0,  # åº”åŠ›è½´ï¼šå‹åŠ›-æ”¯æ’‘
                        'R': shi_shang / total_energy              # å…³è”è½´ï¼šè¾“å‡º
                    }
                    # ç¡®ä¿å½’ä¸€åŒ–
                    normalized_projection = tensor_normalize(projection)
                else:
                    # å¦‚æœæ€»èƒ½é‡ä¸º0ï¼Œä½¿ç”¨åŸå§‹projection
                    projection = result.get('projection', {})
                    normalized_projection = tensor_normalize(projection) if projection else None
            else:
                # ä½¿ç”¨åŸå§‹projection
                projection = result.get('projection', {})
                normalized_projection = tensor_normalize(projection) if projection else None
            
            if not normalized_projection:
                continue
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºTier Xï¼ˆæ ¹æ®è§„èŒƒï¼šæå€¼æ³•åˆ™ã€ç›¸å˜æ³•åˆ™ã€ç®—æ³•å¤±æ•ˆæ³•åˆ™ï¼‰
            sai = result.get('sai', 0.0)
            s_value = normalized_projection.get('S', 0.0)
            e_value = normalized_projection.get('E', 0.0)
            
            # Tier Xåˆ¤æ–­ï¼š
            # 1. æå€¼æ³•åˆ™ï¼šSè½´æˆ–Eè½´å¼‚å¸¸é«˜ï¼ˆ>0.5ï¼‰
            # 2. ç›¸å˜æ³•åˆ™ï¼šSè½´æé«˜ä¸”Eè½´æä½ï¼ˆç»“æ„é«˜å‹ï¼‰
            is_tier_x = False
            if s_value > 0.5:  # åº”åŠ›è½´å¼‚å¸¸é«˜
                is_tier_x = True
            elif e_value > 0.5 and s_value > 0.3:  # èƒ½çº§æº¢å‡ºä¸”é«˜åº”åŠ›
                is_tier_x = True
            elif sai > 2.0:  # SAIå¼‚å¸¸é«˜
                is_tier_x = True
            
            if is_tier_x:
                tier_x_projections.append(normalized_projection)
            else:
                tier_a_projections.append(normalized_projection)
        
        logger.info(f"Tier Aæ ·æœ¬æ•°: {len(tier_a_projections)}")
        logger.info(f"Tier Xæ ·æœ¬æ•°: {len(tier_x_projections)}")
        
        # 2. è®¡ç®—Tier Aè´¨å¿ƒ
        standard_centroid = None
        if tier_a_projections:
            standard_centroid = calculate_centroid(tier_a_projections)
            logger.info(f"âœ… Tier Aè´¨å¿ƒè®¡ç®—å®Œæˆ: {standard_centroid}")
        
        # 3. è®¡ç®—Tier Xè´¨å¿ƒï¼ˆå¦‚æœæœ‰å¤šä¸ªå˜ä½“ï¼Œéœ€è¦åˆ†ç»„ï¼‰
        singularity_centroids = []
        if tier_x_projections:
            # ç®€åŒ–ï¼šå°†æ‰€æœ‰Tier Xæ ·æœ¬ä½œä¸ºä¸€ä¸ªæ•´ä½“è®¡ç®—è´¨å¿ƒ
            # å®é™…åº”è¯¥æ ¹æ®ä¸åŒçš„å¥‡ç‚¹ç±»å‹åˆ†ç»„ï¼ˆå¦‚A-03-X1, A-03-X2ï¼‰
            x1_projections = []
            x2_projections = []
            
            for proj in tier_x_projections:
                # æ ¹æ®åº”åŠ›è½´Så€¼åˆ¤æ–­æ˜¯X1ï¼ˆé«˜èƒ½æº¢å‡ºï¼‰è¿˜æ˜¯X2ï¼ˆé«˜å‹å±ˆæœï¼‰
                if proj.get('S', 0.0) > 0.4:
                    x2_projections.append(proj)  # é«˜å‹å±ˆæœå‹
                else:
                    x1_projections.append(proj)  # èšå˜ä¸´ç•Œå‹
            
            if x1_projections:
                x1_centroid = calculate_centroid(x1_projections)
                singularity_centroids.append({
                    'sub_id': 'A-03-X1',
                    'description': 'èšå˜ä¸´ç•Œå‹ (Tier X1 Mean)',
                    'vector': x1_centroid,
                    'match_threshold': 0.90,
                    'risk_level': 'CRITICAL',
                    'special_instruction': 'Enable Vent Logic (Disable Balance Check)',
                    'sample_count': len(x1_projections)
                })
                logger.info(f"âœ… Tier X1è´¨å¿ƒè®¡ç®—å®Œæˆ: {x1_centroid}")
            
            if x2_projections:
                x2_centroid = calculate_centroid(x2_projections)
                singularity_centroids.append({
                    'sub_id': 'A-03-X2',
                    'description': 'ç»“æ„é«˜å‹å±ˆæœå‹ (Tier X2 Mean)',
                    'vector': x2_centroid,
                    'match_threshold': 0.90,
                    'risk_level': 'CRITICAL',
                    'special_instruction': 'Disable Balance Check',
                    'sample_count': len(x2_projections)
                })
                logger.info(f"âœ… Tier X2è´¨å¿ƒè®¡ç®—å®Œæˆ: {x2_centroid}")
        
        return {
            'standard_centroid': standard_centroid,
            'singularity_centroids': singularity_centroids,
            'tier_a_count': len(tier_a_projections),
            'tier_x_count': len(tier_x_projections)
        }
    
    def step5_update_registry(self, centroids: Dict[str, Any]) -> bool:
        """
        Step 5: ä¸“é¢˜å°å·ä¸å…¨æ¯æ³¨å†Œ
        æ›´æ–°feature_anchorsåˆ°æ³¨å†Œè¡¨
        
        Args:
            centroids: Step 3è®¡ç®—çš„è´¨å¿ƒç»“æœ
            
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        logger.info("=" * 70)
        logger.info("Step 5: ä¸“é¢˜å°å·ä¸å…¨æ¯æ³¨å†Œï¼ˆæ›´æ–°feature_anchorsï¼‰")
        logger.info("=" * 70)
        
        registry_path = project_root / "core" / "subjects" / "holographic_pattern" / "registry.json"
        
        # åŠ è½½æ³¨å†Œè¡¨
        with open(registry_path, 'r', encoding='utf-8') as f:
            registry = json.load(f)
        
        pattern = registry['patterns'].get(self.pattern_id)
        if not pattern:
            logger.error(f"æ ¼å±€ {self.pattern_id} ä¸å­˜åœ¨äºæ³¨å†Œè¡¨")
            return False
        
        # æ›´æ–°feature_anchors
        standard_centroid = centroids.get('standard_centroid')
        singularity_centroids = centroids.get('singularity_centroids', [])
        
        if not standard_centroid:
            logger.error("ç¼ºå°‘standard_centroidï¼Œæ— æ³•æ›´æ–°æ³¨å†Œè¡¨")
            return False
        
        # æ„å»ºfeature_anchorsç»“æ„
        feature_anchors = {
            'description': 'åŸºäºå¤§æ•°æ®æ‹Ÿåˆç®—å‡ºçš„ç©ºé—´è´¨å¿ƒåæ ‡ (The DNA of Fate)',
            'standard_centroid': {
                'description': 'æ ‡å‡†æ’æ˜Ÿé”šç‚¹ (Tier A Mean)',
                'vector': standard_centroid,
                'match_threshold': 0.80,
                'perfect_threshold': 0.92
            },
            'singularity_centroids': singularity_centroids
        }
        
        # æ›´æ–°æ³¨å†Œè¡¨
        pattern['feature_anchors'] = feature_anchors
        
        # æ›´æ–°ç‰ˆæœ¬å†å²
        if 'audit_trail' not in pattern:
            pattern['audit_trail'] = {}
        
        if 'version_history' not in pattern['audit_trail']:
            pattern['audit_trail']['version_history'] = []
        
        pattern['audit_trail']['version_history'].append({
            'version': '2.0',
            'date': datetime.now().strftime('%Y-%m-%d'),
            'source': 'FDS-V1.1 Refit (V2.0 Alignment)',
            'description': 'é‡æ–°æ‹Ÿåˆï¼Œè®¡ç®—è´¨å¿ƒé”šç‚¹å¹¶æ›´æ–°feature_anchors',
            'fds_steps': {
                'step3': f'è®¡ç®—è´¨å¿ƒï¼ˆTier A: {centroids.get("tier_a_count", 0)}, Tier X: {centroids.get("tier_x_count", 0)}ï¼‰',
                'step5': 'æ›´æ–°feature_anchorsåˆ°æ³¨å†Œè¡¨'
            }
        })
        
        # ä¿å­˜æ³¨å†Œè¡¨
        with open(registry_path, 'w', encoding='utf-8') as f:
            json.dump(registry, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æ³¨å†Œè¡¨å·²æ›´æ–°: {registry_path}")
        logger.info(f"   - standard_centroid: {standard_centroid}")
        logger.info(f"   - singularity_centroids: {len(singularity_centroids)} ä¸ª")
        
        return True
    
    def run_full_refit(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„é‡æ–°æ‹Ÿåˆæµç¨‹
        
        Returns:
            æ‹Ÿåˆç»“æœå­—å…¸
        """
        logger.info("=" * 70)
        logger.info("ğŸš€ å¼€å§‹A-03ç¾Šåˆƒæ¶æ€é‡æ–°æ‹Ÿåˆï¼ˆå¯¹é½V2.0è§„èŒƒï¼‰")
        logger.info("=" * 70)
        
        # 1. åŠ è½½ç°æœ‰æ‹Ÿåˆæ•°æ®
        fitting_data = self.load_existing_fitting_data()
        
        if not fitting_data.get('step3', {}).get('results'):
            logger.error("æœªæ‰¾åˆ°Step 3çš„æ‹Ÿåˆç»“æœï¼Œè¯·å…ˆæ‰§è¡ŒStep 2å’ŒStep 3")
            return {'error': 'ç¼ºå°‘Step 3æ‹Ÿåˆæ•°æ®'}
        
        # 2. Step 3: è®¡ç®—è´¨å¿ƒ
        centroids = self.step3_calculate_centroids(fitting_data)
        
        # 3. Step 5: æ›´æ–°æ³¨å†Œè¡¨
        update_success = self.step5_update_registry(centroids)
        
        if not update_success:
            return {'error': 'æ³¨å†Œè¡¨æ›´æ–°å¤±è´¥'}
        
        # 4. éªŒè¯æ›´æ–°ç»“æœ
        updated_pattern = self.registry_loader.get_pattern(self.pattern_id)
        feature_anchors = updated_pattern.get('feature_anchors', {})
        
        result = {
            'pattern_id': self.pattern_id,
            'refit_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'fds_version': 'V1.1 (V2.0 Alignment)',
            'step3_centroids': centroids,
            'step5_registry_updated': update_success,
            'verification': {
                'feature_anchors_exists': 'feature_anchors' in updated_pattern,
                'standard_centroid_exists': 'standard_centroid' in feature_anchors,
                'singularity_centroids_count': len(feature_anchors.get('singularity_centroids', []))
            }
        }
        
        logger.info("=" * 70)
        logger.info("âœ… é‡æ–°æ‹Ÿåˆå®Œæˆï¼")
        logger.info("=" * 70)
        logger.info(f"   - Tier Aè´¨å¿ƒ: {centroids.get('standard_centroid')}")
        logger.info(f"   - Tier Xè´¨å¿ƒæ•°: {len(centroids.get('singularity_centroids', []))}")
        logger.info(f"   - æ³¨å†Œè¡¨å·²æ›´æ–°: {update_success}")
        
        return result


if __name__ == '__main__':
    engine = FDSV11RefitEngine('A-03')
    result = engine.run_full_refit()
    
    # ä¿å­˜ç»“æœ
    output_file = project_root / "data" / "holographic_pattern" / "A-03_Refit_V2_Results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

