#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RLHF å¼ºåŒ–å­¦ä¹ åé¦ˆæµ‹è¯•è„šæœ¬
========================

æµ‹è¯• V10.0 æ–°å¢çš„ RLHF åŠŸèƒ½ï¼ŒéªŒè¯åŸºäºçœŸå®æ¡ˆä¾‹åé¦ˆçš„è‡ªé€‚åº”è¿›åŒ–ã€‚
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.rlhf_feedback import RewardModel, AdaptiveParameterTuner, RLHFTrainer
from controllers.wealth_verification_controller import WealthVerificationController
from core.engine_graph import GraphNetworkEngine

def print_section(title: str, char: str = "="):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print(f"\n{char * 80}")
    print(f"  {title}")
    print(f"{char * 80}\n")

def test_rlhf_feedback():
    """æµ‹è¯• RLHF åé¦ˆåŠŸèƒ½"""
    print_section("ğŸ”¬ RLHF å¼ºåŒ–å­¦ä¹ åé¦ˆæµ‹è¯•", "=")
    
    # åˆå§‹åŒ–ç»„ä»¶
    from core.config_schema import DEFAULT_FULL_ALGO_PARAMS
    controller = WealthVerificationController()
    engine = GraphNetworkEngine(config=DEFAULT_FULL_ALGO_PARAMS)
    rlhf_trainer = RLHFTrainer()
    
    # è·å– Jason æ¡ˆä¾‹
    all_cases = controller.get_all_cases()
    jason_cases = [c for c in all_cases if hasattr(c, 'id') and c.id.startswith('JASON_')]
    
    print(f"æ‰¾åˆ° {len(jason_cases)} ä¸ª Jason æ¡ˆä¾‹ç”¨äº RLHF è®­ç»ƒ")
    print()
    
    # æµ‹è¯• 1: å¥–åŠ±æ¨¡å‹
    print_section("æµ‹è¯• 1: å¥–åŠ±æ¨¡å‹è®¡ç®—", "-")
    
    reward_model = RewardModel()
    
    # æµ‹è¯•ä¸åŒè¯¯å·®çš„å¥–åŠ±
    test_cases = [
        (100.0, 100.0, "å®Œç¾åŒ¹é…"),
        (100.0, 95.0, "è¯¯å·® 5.0"),
        (100.0, 85.0, "è¯¯å·® 15.0"),
        (100.0, 75.0, "è¯¯å·® 25.0"),
        (100.0, 50.0, "è¯¯å·® 50.0")
    ]
    
    for pred, real, desc in test_cases:
        reward = reward_model.calculate_reward(pred, real)
        error = abs(pred - real)
        print(f"  {desc}: é¢„æµ‹={pred:.1f}, çœŸå®={real:.1f}, è¯¯å·®={error:.1f}, å¥–åŠ±={reward:.2f}")
    print()
    
    # æµ‹è¯• 2: æ‰¹é‡å¥–åŠ±è®¡ç®—
    print_section("æµ‹è¯• 2: æ‰¹é‡å¥–åŠ±è®¡ç®—", "-")
    
    # ä½¿ç”¨ Jason D æ¡ˆä¾‹
    jason_d = next((c for c in jason_cases if c.id == 'JASON_D_T1961_1010'), None)
    
    if jason_d:
        predictions = []
        reals = []
        
        for event in jason_d.timeline or []:
            year = event.year
            ganzhi = event.ganzhi if hasattr(event, 'ganzhi') else ''
            dayun = event.dayun if hasattr(event, 'dayun') else ''
            real_magnitude = event.real_magnitude if hasattr(event, 'real_magnitude') else 0.0
            
            if not ganzhi:
                continue
            
            # è®¡ç®—é¢„æµ‹å€¼
            wealth_result = engine.calculate_wealth_index(
                bazi=jason_d.bazi,
                day_master=jason_d.day_master,
                gender=jason_d.gender,
                luck_pillar=dayun,
                year_pillar=ganzhi
            )
            
            predicted = wealth_result.get('wealth_index', 0.0)
            predictions.append(predicted)
            reals.append(real_magnitude)
        
        # è®¡ç®—æ‰¹é‡å¥–åŠ±
        batch_reward = reward_model.calculate_batch_reward(predictions, reals)
        
        print(f"  æ¡ˆä¾‹: {jason_d.name}")
        print(f"  äº‹ä»¶æ•°: {batch_reward['total_count']}")
        print(f"  æ­£ç¡®æ•°: {batch_reward['correct_count']}")
        print(f"  å‘½ä¸­ç‡: {batch_reward['hit_rate'] * 100:.1f}%")
        print(f"  æ€»å¥–åŠ±: {batch_reward['total_reward']:.2f}")
        print(f"  å¹³å‡å¥–åŠ±: {batch_reward['avg_reward']:.2f}")
        print(f"  å‘½ä¸­ç‡åŠ æˆ: {batch_reward['hit_rate_bonus']:.2f}")
        print(f"  æœ€ç»ˆå¥–åŠ±: {batch_reward['final_reward']:.2f}")
        print()
    
    # æµ‹è¯• 3: è‡ªé€‚åº”å‚æ•°è°ƒä¼˜
    print_section("æµ‹è¯• 3: è‡ªé€‚åº”å‚æ•°è°ƒä¼˜", "-")
    
    tuner = AdaptiveParameterTuner()
    
    # åˆå§‹å‚æ•°
    current_params = {
        'threshold': 0.5,
        'scale': 10.0,
        'phase_point': 0.5
    }
    
    parameter_ranges = {
        'threshold': (0.4, 0.6),
        'scale': (5.0, 15.0),
        'phase_point': (0.4, 0.6)
    }
    
    print(f"  åˆå§‹å‚æ•°: {current_params}")
    
    # æ¨¡æ‹Ÿå¤šæ¬¡è°ƒä¼˜
    for i in range(5):
        # æ¨¡æ‹Ÿå¥–åŠ±ï¼ˆæ­£å¥–åŠ±è¡¨ç¤ºå¥½ï¼Œè´Ÿå¥–åŠ±è¡¨ç¤ºå·®ï¼‰
        reward = 1.0 if i < 3 else -0.5
        
        current_params = tuner.tune_parameters(
            current_params=current_params,
            reward=reward,
            parameter_ranges=parameter_ranges
        )
        
        print(f"  è¿­ä»£ {i+1}: å¥–åŠ±={reward:.2f}, å‚æ•°={current_params}")
    
    # è·å–æœ€ä½³å‚æ•°
    best_params = tuner.get_best_parameters()
    print(f"  å†å²æœ€ä½³å‚æ•°: {best_params}")
    print()
    
    # æµ‹è¯• 4: RLHF è®­ç»ƒå™¨
    print_section("æµ‹è¯• 4: RLHF è®­ç»ƒå™¨", "-")
    
    if jason_d:
        # å‡†å¤‡é¢„æµ‹å’ŒçœŸå®æ•°æ®
        predictions_data = []
        reals_data = []
        
        for event in jason_d.timeline or []:
            year = event.year
            ganzhi = event.ganzhi if hasattr(event, 'ganzhi') else ''
            dayun = event.dayun if hasattr(event, 'dayun') else ''
            real_magnitude = event.real_magnitude if hasattr(event, 'real_magnitude') else 0.0
            
            if not ganzhi:
                continue
            
            wealth_result = engine.calculate_wealth_index(
                bazi=jason_d.bazi,
                day_master=jason_d.day_master,
                gender=jason_d.gender,
                luck_pillar=dayun,
                year_pillar=ganzhi
            )
            
            predictions_data.append({
                'year': year,
                'wealth_index': wealth_result.get('wealth_index', 0.0)
            })
            
            reals_data.append({
                'year': year,
                'real_magnitude': real_magnitude
            })
        
        # ä»åé¦ˆä¸­å­¦ä¹ 
        learning_result = rlhf_trainer.learn_from_feedback(
            case_id=jason_d.id,
            predictions=predictions_data,
            reals=reals_data
        )
        
        print(f"  æ¡ˆä¾‹: {jason_d.name}")
        print(f"  å­¦ä¹ ç»“æœ: {learning_result}")
        print(f"  åé¦ˆå†å²è®°å½•æ•°: {len(rlhf_trainer.feedback_history)}")
        print()
    
    print("âœ… RLHF å¼ºåŒ–å­¦ä¹ åé¦ˆæµ‹è¯•å®Œæˆï¼")
    
    return {
        'reward_model': reward_model,
        'tuner': tuner,
        'rlhf_trainer': rlhf_trainer
    }

if __name__ == '__main__':
    try:
        result = test_rlhf_feedback()
        print(f"\nâœ… è„šæœ¬æ‰§è¡ŒæˆåŠŸï¼")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

