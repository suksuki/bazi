"""
V11.1 åŠ¨æ€æ¸…æ´—å™¨ (Dynamic Cleaner)
ä»£è°¢æ¨¡ç»„ï¼šä½¿ç”¨RANSACæ€æƒ³åŠ¨æ€æ¸…æ´—è„æ•°æ®

æœºåˆ¶ï¼š
1. æ¯æ¬¡è®­ç»ƒå‰ï¼Œå…ˆç”¨Classicå’ŒSyntheticæ•°æ®è®­ç»ƒä¸€ä¸ªåŸºå‡†æ¨¡å‹
2. ç”¨åŸºå‡†æ¨¡å‹å»é¢„æµ‹æ‰€æœ‰Modernæ•°æ®
3. åå·®æ£€æµ‹ï¼šå¦‚æœæŸModernæ¡ˆä¾‹çš„é¢„æµ‹ç»“æœä¸Ground Truthåç¦»åº¦ > é˜ˆå€¼ï¼Œæ ‡è®°ä¸ºDirty
4. å¤„ç½®ï¼šè‡ªåŠ¨å°†å…¶IDè¿½åŠ åˆ°config/ignored_cases.jsonï¼ˆæˆ–åœ¨å†…å­˜ä¸­å‰”é™¤ï¼‰
"""

import sys
import json
import logging
import numpy as np
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from core.engine_graph import GraphNetworkEngine
from core.models.config_model import ConfigModel

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥sklearn
try:
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger.warning("âš ï¸  sklearnæœªå®‰è£…ï¼ŒåŠ¨æ€æ¸…æ´—å™¨å°†ä½¿ç”¨è§„åˆ™æ–¹æ³•")


class DynamicCleaner:
    """åŠ¨æ€æ¸…æ´—å™¨ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶æ ‡è®°è„æ•°æ®"""
    
    def __init__(self, config_model: ConfigModel = None):
        self.config_model = config_model or ConfigModel()
        self.config = self.config_model.load_config()
        self.ignored_cases_file = project_root / "config" / "ignored_cases.json"
    
    def load_ignored_cases(self) -> Set[str]:
        """åŠ è½½å·²å¿½ç•¥çš„æ¡ˆä¾‹ID"""
        if self.ignored_cases_file.exists():
            with open(self.ignored_cases_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                ignored_ids = set(data.get('ignored_case_ids', []))
                logger.info(f"âœ… åŠ è½½äº† {len(ignored_ids)} ä¸ªå·²å¿½ç•¥æ¡ˆä¾‹ID")
                return ignored_ids
        return set()
    
    def save_ignored_cases(self, ignored_ids: Set[str], notes: Dict[str, str] = None):
        """ä¿å­˜å¿½ç•¥æ¡ˆä¾‹åˆ—è¡¨"""
        self.ignored_cases_file.parent.mkdir(parents=True, exist_ok=True)
        
        data = {
            "version": "V11.1",
            "description": "åŠ¨æ€æ¸…æ´—å™¨è¯†åˆ«çš„ç¦»ç¾¤ç‚¹ï¼ˆè„æ•°æ®ï¼‰ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°æ—¶æ’é™¤",
            "ignored_case_ids": sorted(list(ignored_ids)),
            "notes": notes or {},
            "generated_from": "Dynamic Cleaner (V11.1)",
            "total_outliers": len(ignored_ids)
        }
        
        with open(self.ignored_cases_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… å·²ä¿å­˜ {len(ignored_ids)} ä¸ªå¿½ç•¥æ¡ˆä¾‹åˆ°: {self.ignored_cases_file}")
    
    def filter_outliers(
        self, 
        classic_cases: List[Dict], 
        synthetic_cases: List[Dict],
        modern_cases: List[Dict],
        confidence_threshold: float = 0.90,
        use_svm: bool = True
    ) -> Tuple[List[Dict], Set[str]]:
        """
        ä½¿ç”¨åŸºå‡†æ¨¡å‹è¿‡æ»¤ç¦»ç¾¤ç‚¹
        
        Args:
            classic_cases: ç»å…¸æ¡ˆä¾‹ï¼ˆåŸºå‡†è®­ç»ƒæ•°æ®ï¼‰
            synthetic_cases: åˆæˆæ¡ˆä¾‹ï¼ˆåŸºå‡†è®­ç»ƒæ•°æ®ï¼‰
            modern_cases: ç°ä»£æ¡ˆä¾‹ï¼ˆå¾…æ¸…æ´—æ•°æ®ï¼‰
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé¢„æµ‹ä¸ground_truthç›¸åä¸”ç½®ä¿¡åº¦>é˜ˆå€¼æ—¶æ ‡è®°ä¸ºDirtyï¼‰
            use_svm: æ˜¯å¦ä½¿ç”¨SVMä½œä¸ºåŸºå‡†æ¨¡å‹ï¼ˆå¦åˆ™ä½¿ç”¨è§„åˆ™æ–¹æ³•ï¼‰
        
        Returns:
            Tuple[cleaned_modern_cases, new_dirty_ids]: æ¸…æ´—åçš„ç°ä»£æ¡ˆä¾‹åˆ—è¡¨å’Œæ–°å¢çš„è„æ•°æ®IDé›†åˆ
        """
        logger.info("ğŸ§¹ å¼€å§‹åŠ¨æ€æ¸…æ´—...")
        logger.info(f"   åŸºå‡†æ•°æ®: {len(classic_cases)} ä¸ªç»å…¸æ¡ˆä¾‹ + {len(synthetic_cases)} ä¸ªåˆæˆæ¡ˆä¾‹")
        logger.info(f"   å¾…æ¸…æ´—æ•°æ®: {len(modern_cases)} ä¸ªç°ä»£æ¡ˆä¾‹")
        
        # åŠ è½½å·²æœ‰çš„å¿½ç•¥åˆ—è¡¨
        existing_ignored = self.load_ignored_cases()
        
        # åŸºå‡†è®­ç»ƒæ•°æ®ï¼ˆClassic + Syntheticï¼‰
        reference_cases = classic_cases + synthetic_cases
        
        if len(reference_cases) < 5:
            logger.warning("âš ï¸  åŸºå‡†æ•°æ®å¤ªå°‘ï¼Œè·³è¿‡åŠ¨æ€æ¸…æ´—")
            return modern_cases, set()
        
        # è®­ç»ƒåŸºå‡†æ¨¡å‹
        if use_svm and SKLEARN_AVAILABLE:
            reference_model, scaler = self._train_reference_svm(reference_cases)
        else:
            reference_model = None
            scaler = None
            logger.info("   ä½¿ç”¨è§„åˆ™æ–¹æ³•ä½œä¸ºåŸºå‡†æ¨¡å‹")
        
        # è¯„ä¼°ç°ä»£æ¡ˆä¾‹
        dirty_ids = set()
        cleaned_modern_cases = []
        
        engine = GraphNetworkEngine(config=self.config)
        
        for case in modern_cases:
            case_id = case.get('id', '')
            
            # è·³è¿‡å·²ç»å¿½ç•¥çš„æ¡ˆä¾‹
            if case_id in existing_ignored:
                continue
            
            # æå–ç‰¹å¾
            try:
                bazi_list = case.get('bazi', [])
                if isinstance(bazi_list, str):
                    bazi_list = bazi_list.split()
                
                day_master = case.get('day_master', '')
                
                engine.initialize_nodes(
                    bazi=bazi_list,
                    day_master=day_master,
                    luck_pillar=None,
                    year_pillar=None
                )
                
                if use_svm and reference_model is not None:
                    # ä½¿ç”¨SVMé¢„æµ‹
                    feature_vector = engine.extract_svm_features(day_master)
                    X = np.array([feature_vector])
                    X_scaled = scaler.transform(X)
                    
                    predicted_label = reference_model.predict(X_scaled)[0]
                    prediction_proba = reference_model.predict_proba(X_scaled)[0]
                    confidence = max(prediction_proba)
                    
                    ground_truth = case.get('ground_truth', {}).get('strength', 'Unknown')
                    
                    # åå·®æ£€æµ‹ï¼šé¢„æµ‹ä¸ground_truthç›¸åä¸”ç½®ä¿¡åº¦å¾ˆé«˜
                    if predicted_label != ground_truth and confidence > confidence_threshold:
                        dirty_ids.add(case_id)
                        logger.warning(f"   ğŸš« è¯†åˆ«ä¸ºè„æ•°æ®: {case_id} ({case.get('name', 'Unknown')})")
                        logger.warning(f"      Ground Truth: {ground_truth}, é¢„æµ‹: {predicted_label}, ç½®ä¿¡åº¦: {confidence:.2%}")
                    else:
                        cleaned_modern_cases.append(case)
                
                else:
                    # ä½¿ç”¨è§„åˆ™æ–¹æ³•ï¼ˆåŸºäºcalculate_strength_scoreï¼‰
                    result = engine.calculate_strength_score(day_master)
                    predicted_label = result.get('strength_label', 'Unknown')
                    strength_score = result.get('strength_score', 0.0)
                    
                    ground_truth = case.get('ground_truth', {}).get('strength', 'Unknown')
                    
                    # è§„åˆ™åå·®æ£€æµ‹ï¼šé¢„æµ‹ä¸ground_truthå®Œå…¨ç›¸åï¼Œä¸”åˆ†æ•°æç«¯
                    # ä¾‹å¦‚ï¼šground_truth=Strongä½†é¢„æµ‹=Weakä¸”score<20ï¼Œæˆ–ground_truth=Weakä½†é¢„æµ‹=Strongä¸”score>80
                    is_extreme_mismatch = False
                    if ground_truth == 'Strong' and predicted_label in ['Weak', 'Extreme_Weak'] and strength_score < 20.0:
                        is_extreme_mismatch = True
                    elif ground_truth == 'Weak' and predicted_label == 'Special_Strong' and strength_score > 80.0:
                        is_extreme_mismatch = True
                    elif ground_truth == 'Follower' and predicted_label == 'Special_Strong' and strength_score > 70.0:
                        is_extreme_mismatch = True
                    elif ground_truth == 'Special_Strong' and predicted_label in ['Weak', 'Follower'] and strength_score < 30.0:
                        is_extreme_mismatch = True
                    
                    if is_extreme_mismatch:
                        dirty_ids.add(case_id)
                        logger.warning(f"   ğŸš« è¯†åˆ«ä¸ºè„æ•°æ®: {case_id} ({case.get('name', 'Unknown')})")
                        logger.warning(f"      Ground Truth: {ground_truth}, é¢„æµ‹: {predicted_label}, Score: {strength_score:.1f}")
                    else:
                        cleaned_modern_cases.append(case)
            
            except Exception as e:
                logger.error(f"   âŒ å¤„ç†æ¡ˆä¾‹ {case_id} æ—¶å‡ºé”™: {e}")
                # å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼Œä¸æ ‡è®°ä¸ºè„æ•°æ®
                cleaned_modern_cases.append(case)
        
        logger.info(f"âœ… åŠ¨æ€æ¸…æ´—å®Œæˆ")
        logger.info(f"   æ¸…æ´—åç°ä»£æ¡ˆä¾‹: {len(cleaned_modern_cases)} ä¸ª")
        logger.info(f"   è¯†åˆ«å‡ºæ–°çš„è„æ•°æ®: {len(dirty_ids)} ä¸ª")
        
        # åˆå¹¶åˆ°å·²æœ‰çš„å¿½ç•¥åˆ—è¡¨
        all_ignored = existing_ignored | dirty_ids
        
        # ä¿å­˜æ›´æ–°åçš„å¿½ç•¥åˆ—è¡¨
        if dirty_ids:
            notes = {}
            for case_id in dirty_ids:
                case = next((c for c in modern_cases if c.get('id') == case_id), None)
                if case:
                    notes[case_id] = f"{case.get('name', case_id)}: åŠ¨æ€æ¸…æ´—å™¨è¯†åˆ«ä¸ºç¦»ç¾¤ç‚¹"
            self.save_ignored_cases(all_ignored, notes)
        
        return cleaned_modern_cases, dirty_ids
    
    def _train_reference_svm(self, reference_cases: List[Dict]) -> Tuple[SVC, StandardScaler]:
        """è®­ç»ƒåŸºå‡†SVMæ¨¡å‹"""
        logger.info("   ğŸ”¨ è®­ç»ƒåŸºå‡†SVMæ¨¡å‹...")
        
        engine = GraphNetworkEngine(config=self.config)
        features = []
        labels = []
        
        for case in reference_cases:
            bazi_list = case.get('bazi', [])
            if isinstance(bazi_list, str):
                bazi_list = bazi_list.split()
            
            day_master = case.get('day_master', '')
            
            engine.initialize_nodes(
                bazi=bazi_list,
                day_master=day_master,
                luck_pillar=None,
                year_pillar=None
            )
            
            feature_vector = engine.extract_svm_features(day_master)
            features.append(feature_vector)
            
            ground_truth = case.get('ground_truth', {}).get('strength', 'Unknown')
            labels.append(ground_truth)
        
        X = np.array(features)
        y = np.array(labels)
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # è®­ç»ƒSVMï¼ˆä½¿ç”¨ç®€å•å‚æ•°ï¼‰
        svm_model = SVC(kernel='rbf', probability=True, random_state=42)
        svm_model.fit(X_scaled, y)
        
        logger.info(f"   âœ… åŸºå‡†SVMæ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆ{len(reference_cases)} ä¸ªæ ·æœ¬ï¼‰")
        
        return svm_model, scaler


if __name__ == '__main__':
    """æµ‹è¯•åŠ¨æ€æ¸…æ´—å™¨"""
    logging.basicConfig(level=logging.INFO)
    
    # åŠ è½½æ•°æ®
    data_dir = project_root / "data"
    classic_file = data_dir / "classic_cases.json"
    calibration_file = data_dir / "calibration_cases.json"
    
    classic_cases = []
    modern_cases = []
    
    if classic_file.exists():
        with open(classic_file, 'r', encoding='utf-8') as f:
            classic_cases = json.load(f)
    
    if calibration_file.exists():
        with open(calibration_file, 'r', encoding='utf-8') as f:
            cal_cases = json.load(f)
            # å‡è®¾calibration_casesä¸­éclassicçš„ä¸ºmodern
            classic_ids = {c.get('id') for c in classic_cases}
            modern_cases = [c for c in cal_cases if c.get('id') not in classic_ids and not c.get('synthetic', False)]
    
    # åˆ›å»ºåŠ¨æ€æ¸…æ´—å™¨
    cleaner = DynamicCleaner()
    
    # ç”Ÿæˆåˆæˆæ•°æ®ï¼ˆç”¨äºåŸºå‡†è®­ç»ƒï¼‰
    from .synthetic_factory import SyntheticDataFactory
    factory = SyntheticDataFactory()
    synthetic_cases = factory.generate_perfect_cases(target_count=30)
    
    # æ‰§è¡ŒåŠ¨æ€æ¸…æ´—
    cleaned_cases, dirty_ids = cleaner.filter_outliers(
        classic_cases=classic_cases,
        synthetic_cases=synthetic_cases,
        modern_cases=modern_cases,
        confidence_threshold=0.90,
        use_svm=True
    )
    
    print(f"\nâœ… åŠ¨æ€æ¸…æ´—å®Œæˆ")
    print(f"   æ¸…æ´—åç°ä»£æ¡ˆä¾‹: {len(cleaned_cases)} ä¸ª")
    print(f"   è¯†åˆ«å‡ºè„æ•°æ®: {len(dirty_ids)} ä¸ª")
    if dirty_ids:
        print(f"\n   è„æ•°æ®IDåˆ—è¡¨: {sorted(dirty_ids)}")

