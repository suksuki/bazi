"""
V11.1 æ•°æ®åŠ è½½å™¨ (Data Loader)
èåˆæ¨¡ç»„ï¼šåŠ æƒæ··åˆä¸åŒç±»å‹çš„æ•°æ®

è®­ç»ƒé›†é…æ¯”ï¼š
- æ ¸å¿ƒå±‚ (Core): ç»å…¸å¤ç±æ¡ˆä¾‹ï¼Œæƒé‡ 3.0 â€”â€” ä¸å¯åŠ¨æ‘‡çš„å®ªæ³•
- éª¨æ¶å±‚ (Skeleton): åˆæˆç†è®ºæ•°æ®ï¼Œæƒé‡ 2.0 â€”â€” æ’‘èµ·æ¨¡å‹çš„éª¨æ¶
- è‚Œè‚‰å±‚ (Muscle): æ¸…æ´—åçš„ç°ä»£æ•°æ®ï¼Œæƒé‡ 1.0 â€”â€” å¢åŠ æ³›åŒ–èƒ½åŠ›

éªŒè¯é›† (Validation Set):
- ä¸¥ç¦åŒ…å«åˆæˆæ•°æ®ï¼Œå¿…é¡»æ˜¯ 100% çœŸå®æ¡ˆä¾‹ï¼ˆ"ç»ƒå‡æ‰“çœŸ"ï¼‰
"""

import sys
import json
import logging
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from core.models.config_model import ConfigModel
from .synthetic_factory import SyntheticDataFactory
from .dynamic_cleaner import DynamicCleaner

logger = logging.getLogger(__name__)

# æ•°æ®æƒé‡é…ç½®ï¼ˆå¯é€šè¿‡é…ç½®æ–‡ä»¶è¦†ç›–ï¼‰
DEFAULT_WEIGHTS = {
    'classic': 3.0,      # æ ¸å¿ƒå±‚ï¼šç»å…¸æ¡ˆä¾‹æƒé‡æœ€é«˜
    'synthetic': 2.0,    # éª¨æ¶å±‚ï¼šåˆæˆæ•°æ®æƒé‡ä¸­ç­‰
    'modern': 1.0        # è‚Œè‚‰å±‚ï¼šç°ä»£æ•°æ®æƒé‡æœ€ä½
}


class DataLoader:
    """æ•°æ®åŠ è½½å™¨ï¼šåŠ æƒæ··åˆä¸åŒç±»å‹çš„æ•°æ®"""
    
    def __init__(self, config_model: ConfigModel = None):
        self.config_model = config_model or ConfigModel()
        self.data_dir = project_root / "data"
        self.synthetic_factory = SyntheticDataFactory()
        self.dynamic_cleaner = DynamicCleaner(config_model=config_model)
    
    def load_training_data(
        self,
        use_dynamic_cleaning: bool = True,
        generate_synthetic: bool = True,
        synthetic_count: int = 50
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[bool], List[Dict]]:
        """
        åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆåŒ…å«æ ·æœ¬æƒé‡ï¼‰
        
        Args:
            use_dynamic_cleaning: æ˜¯å¦ä½¿ç”¨åŠ¨æ€æ¸…æ´—
            generate_synthetic: æ˜¯å¦ç”Ÿæˆåˆæˆæ•°æ®
            synthetic_count: åˆæˆæ•°æ®ç”Ÿæˆæ•°é‡
        
        Returns:
            Tuple[X, y, sample_weights, is_synthetic, metadata_list]:
            - X: ç‰¹å¾çŸ©é˜µ
            - y: æ ‡ç­¾å‘é‡
            - sample_weights: æ ·æœ¬æƒé‡æ•°ç»„
            - is_synthetic: æ˜¯å¦åˆæˆçš„æ ‡è®°åˆ—è¡¨
            - metadata_list: æ¡ˆä¾‹å…ƒæ•°æ®åˆ—è¡¨ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰
        """
        logger.info("ğŸ“¦ å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®...")
        
        # 1. åŠ è½½ç»å…¸æ¡ˆä¾‹ï¼ˆCore Layerï¼‰
        classic_cases = self._load_classic_cases()
        logger.info(f"   âœ… åŠ è½½äº† {len(classic_cases)} ä¸ªç»å…¸æ¡ˆä¾‹ï¼ˆæƒé‡ {WEIGHTS['classic']:.1f}ï¼‰")
        
        # 2. ç”Ÿæˆåˆæˆæ•°æ®ï¼ˆSkeleton Layerï¼‰
        synthetic_cases = []
        if generate_synthetic:
            synthetic_cases = self.synthetic_factory.generate_perfect_cases(target_count=synthetic_count)
            logger.info(f"   âœ… ç”Ÿæˆäº† {len(synthetic_cases)} ä¸ªåˆæˆæ¡ˆä¾‹ï¼ˆæƒé‡ {WEIGHTS['synthetic']:.1f}ï¼‰")
        
        # 3. åŠ è½½ç°ä»£æ¡ˆä¾‹ï¼ˆMuscle Layerï¼‰
        modern_cases = self._load_modern_cases()
        logger.info(f"   âœ… åŠ è½½äº† {len(modern_cases)} ä¸ªç°ä»£æ¡ˆä¾‹ï¼ˆåŸå§‹ï¼‰")
        
        # 4. åŠ¨æ€æ¸…æ´—ç°ä»£æ¡ˆä¾‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        cleaned_modern_cases = modern_cases
        if use_dynamic_cleaning and len(classic_cases) > 0:
            cleaned_modern_cases, dirty_ids = self.dynamic_cleaner.filter_outliers(
                classic_cases=classic_cases,
                synthetic_cases=synthetic_cases,
                modern_cases=modern_cases,
                confidence_threshold=0.90,
                use_svm=True
            )
            logger.info(f"   âœ… åŠ¨æ€æ¸…æ´—åå‰©ä½™ {len(cleaned_modern_cases)} ä¸ªç°ä»£æ¡ˆä¾‹ï¼ˆæƒé‡ {WEIGHTS['modern']:.1f}ï¼‰")
            if dirty_ids:
                logger.info(f"   ğŸš« è¯†åˆ«å¹¶æ’é™¤äº† {len(dirty_ids)} ä¸ªè„æ•°æ®")
        
        # 5. åˆå¹¶æ‰€æœ‰æ•°æ®
        all_cases = classic_cases + synthetic_cases + cleaned_modern_cases
        
        # 6. æ ‡è®°æ•°æ®æ¥æº
        is_synthetic_list = []
        sample_weights = []
        metadata_list = []
        
        for case in all_cases:
            # åˆ¤æ–­æ•°æ®æ¥æº
            is_synthetic = case.get('synthetic', False)
            category = case.get('category', 'unknown')
            
            # ç¡®å®šæƒé‡
            if category == 'classic' or case.get('id', '').startswith('CLASSIC_'):
                weight = WEIGHTS['classic']
                source = 'classic'
            elif is_synthetic or category == 'synthetic':
                weight = WEIGHTS['synthetic']
                source = 'synthetic'
            else:
                weight = WEIGHTS['modern']
                source = 'modern'
            
            is_synthetic_list.append(is_synthetic)
            sample_weights.append(weight)
            metadata_list.append({
                'id': case.get('id', ''),
                'name': case.get('name', ''),
                'source': source,
                'weight': weight,
                'synthetic': is_synthetic
            })
        
        logger.info(f"   ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        logger.info(f"      ç»å…¸æ¡ˆä¾‹: {sum(1 for m in metadata_list if m['source'] == 'classic')} ä¸ªï¼ˆæƒé‡ {WEIGHTS['classic']:.1f}ï¼‰")
        logger.info(f"      åˆæˆæ¡ˆä¾‹: {sum(1 for m in metadata_list if m['source'] == 'synthetic')} ä¸ªï¼ˆæƒé‡ {WEIGHTS['synthetic']:.1f}ï¼‰")
        logger.info(f"      ç°ä»£æ¡ˆä¾‹: {sum(1 for m in metadata_list if m['source'] == 'modern')} ä¸ªï¼ˆæƒé‡ {WEIGHTS['modern']:.1f}ï¼‰")
        
        # 7. æå–ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆéœ€è¦ä¼ å…¥engineï¼‰
        # è¿™é‡Œåªè¿”å›æ•°æ®å’Œå…ƒæ•°æ®ï¼Œç‰¹å¾æå–åœ¨trainerä¸­å®Œæˆ
        # è¿”å›å ä½ç¬¦ï¼Œå®é™…ç‰¹å¾æå–åœ¨è°ƒç”¨æ–¹å®Œæˆ
        X_placeholder = np.array([])  # å ä½ç¬¦ï¼Œå®é™…åº”åœ¨trainerä¸­æå–
        y_placeholder = np.array([])  # å ä½ç¬¦ï¼Œå®é™…åº”åœ¨trainerä¸­æå–
        
        return X_placeholder, y_placeholder, np.array(sample_weights), is_synthetic_list, metadata_list
    
    def load_training_cases(
        self,
        use_dynamic_cleaning: bool = True,
        generate_synthetic: bool = True,
        synthetic_count: int = 50,
        classic_weight: float = None,
        synthetic_weight: float = None,
        modern_weight: float = None
    ) -> Tuple[List[Dict], List[float], List[bool]]:
        """
        åŠ è½½è®­ç»ƒæ¡ˆä¾‹åˆ—è¡¨ï¼ˆç”¨äºç‰¹å¾æå–ï¼‰
        
        Returns:
            Tuple[cases, sample_weights, is_synthetic]:
            - cases: æ¡ˆä¾‹åˆ—è¡¨
            - sample_weights: æ ·æœ¬æƒé‡åˆ—è¡¨
            - is_synthetic: æ˜¯å¦åˆæˆçš„æ ‡è®°åˆ—è¡¨
        """
        logger.info("ğŸ“¦ å¼€å§‹åŠ è½½è®­ç»ƒæ¡ˆä¾‹...")
        
        # 1. åŠ è½½ç»å…¸æ¡ˆä¾‹
        classic_cases = self._load_classic_cases()
        logger.info(f"   âœ… åŠ è½½äº† {len(classic_cases)} ä¸ªç»å…¸æ¡ˆä¾‹")
        
        # 2. ç”Ÿæˆåˆæˆæ•°æ®
        synthetic_cases = []
        if generate_synthetic:
            synthetic_cases = self.synthetic_factory.generate_perfect_cases(target_count=synthetic_count)
            logger.info(f"   âœ… ç”Ÿæˆäº† {len(synthetic_cases)} ä¸ªåˆæˆæ¡ˆä¾‹")
        
        # 3. åŠ è½½å¹¶æ¸…æ´—ç°ä»£æ¡ˆä¾‹
        modern_cases = self._load_modern_cases()
        cleaned_modern_cases = modern_cases
        if use_dynamic_cleaning and len(classic_cases) > 0:
            cleaned_modern_cases, dirty_ids = self.dynamic_cleaner.filter_outliers(
                classic_cases=classic_cases,
                synthetic_cases=synthetic_cases,
                modern_cases=modern_cases,
                confidence_threshold=0.90,
                use_svm=True
            )
            if dirty_ids:
                logger.info(f"   ğŸš« è¯†åˆ«å¹¶æ’é™¤äº† {len(dirty_ids)} ä¸ªè„æ•°æ®")
        
        # 4. åˆå¹¶å¹¶æ ‡è®°
        all_cases = classic_cases + synthetic_cases + cleaned_modern_cases
        
        # ä½¿ç”¨ä¼ å…¥çš„æƒé‡æˆ–é»˜è®¤æƒé‡
        weights = {
            'classic': classic_weight if classic_weight is not None else DEFAULT_WEIGHTS['classic'],
            'synthetic': synthetic_weight if synthetic_weight is not None else DEFAULT_WEIGHTS['synthetic'],
            'modern': modern_weight if modern_weight is not None else DEFAULT_WEIGHTS['modern']
        }
        
        sample_weights = []
        is_synthetic_list = []
        
        for case in all_cases:
            is_synthetic = case.get('synthetic', False)
            category = case.get('category', 'unknown')
            
            if category == 'classic' or case.get('id', '').startswith('CLASSIC_'):
                weight = weights['classic']
            elif is_synthetic or category == 'synthetic':
                weight = weights['synthetic']
            else:
                weight = weights['modern']
            
            sample_weights.append(weight)
            is_synthetic_list.append(is_synthetic)
        
        logger.info(f"   ğŸ“Š æ€»è®¡: {len(all_cases)} ä¸ªæ¡ˆä¾‹")
        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        classic_weight = weights['classic']
        classic_count = sum(1 for i, s in enumerate(is_synthetic_list) if not s and sample_weights[i] == classic_weight)
        synthetic_count = sum(is_synthetic_list)
        modern_count = len(all_cases) - classic_count - synthetic_count
        logger.info(f"      ç»å…¸: {classic_count} ä¸ª")
        logger.info(f"      åˆæˆ: {synthetic_count} ä¸ª")
        logger.info(f"      ç°ä»£: {modern_count} ä¸ª")
        
        return all_cases, sample_weights, is_synthetic_list
    
    def _load_classic_cases(self) -> List[Dict]:
        """åŠ è½½ç»å…¸æ¡ˆä¾‹"""
        classic_file = self.data_dir / "classic_cases.json"
        classic_cases = []
        
        if classic_file.exists():
            with open(classic_file, 'r', encoding='utf-8') as f:
                classic_cases = json.load(f)
                # ç¡®ä¿æ ‡è®°ä¸ºclassic
                for case in classic_cases:
                    case['category'] = 'classic'
                    case['synthetic'] = False
        
        return classic_cases
    
    def _load_modern_cases(self) -> List[Dict]:
        """åŠ è½½ç°ä»£æ¡ˆä¾‹ï¼ˆæ’é™¤å·²å¿½ç•¥çš„ï¼‰"""
        ignored_ids = self.dynamic_cleaner.load_ignored_cases()
        
        calibration_file = self.data_dir / "calibration_cases.json"
        modern_cases = []
        
        if calibration_file.exists():
            with open(calibration_file, 'r', encoding='utf-8') as f:
                cal_cases = json.load(f)
                
                # è¿‡æ»¤ï¼šæ’é™¤classicã€syntheticå’Œå·²å¿½ç•¥çš„
                classic_ids = set()
                classic_file = self.data_dir / "classic_cases.json"
                if classic_file.exists():
                    with open(classic_file, 'r', encoding='utf-8') as f:
                        classic_cases = json.load(f)
                        classic_ids = {c.get('id') for c in classic_cases}
                
                for case in cal_cases:
                    case_id = case.get('id', '')
                    # æ’é™¤ï¼šå·²å¿½ç•¥çš„ã€classicçš„ã€syntheticçš„
                    if (case_id not in ignored_ids and 
                        case_id not in classic_ids and 
                        not case.get('synthetic', False)):
                        case['category'] = 'modern'
                        modern_cases.append(case)
        
        return modern_cases


if __name__ == '__main__':
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    logging.basicConfig(level=logging.INFO)
    
    loader = DataLoader()
    
    cases, weights, is_synthetic = loader.load_training_cases(
        use_dynamic_cleaning=True,
        generate_synthetic=True,
        synthetic_count=30
    )
    
    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   æ€»æ¡ˆä¾‹æ•°: {len(cases)}")
    print(f"   æƒé‡åˆ†å¸ƒ: {dict(Counter([f'{w:.1f}' for w in weights]))}")
    print(f"   åˆæˆæ•°æ®: {sum(is_synthetic)} ä¸ª")
    print(f"   çœŸå®æ•°æ®: {len(cases) - sum(is_synthetic)} ä¸ª")

