"""
A-03 å¥‡ç‚¹ç­›é€‰ä¸è‡ªåŠ¨åŒ–å­˜è¯è„šæœ¬
============================
ä» 518k æ ·æœ¬åº“ä¸­ç­›é€‰é«˜è´¨é‡å¥‡ç‚¹ï¼Œç¡®ä¿ï¼š
1. ç‰©ç†å¤šæ ·æ€§ï¼šè¦†ç›–ä¸åŒåç§»æ–¹å‘ï¼ˆé«˜Eæçƒ­ã€é«˜Sæå¯’ã€é«˜Rè”ç›Ÿç­‰ï¼‰
2. é©¬æ°è·ç¦»ï¼šåç¦»æ ‡å‡†æµå½¢ D_M > 2.5
3. äººç”Ÿè½¨è¿¹å…³è”ï¼šæç«¯ y_true å€¼

ä½¿ç”¨æ–¹æ³•:
    source venv/bin/activate && python scripts/mine_a03_singularities.py
"""

import os
import sys
import json
import logging
from typing import Dict, List, Any
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from core.vault_manager import VaultManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# A-03 æ ‡å‡†æµå½¢å‚æ•° (åŸºäº FDS-V3.0 æ‹Ÿåˆç»“æœ)
A03_STANDARD_MEAN = np.array([0.60, 0.35, 0.15, 0.55, 0.35])  # [E, O, M, S, R]
A03_COVARIANCE = np.array([
    [0.02, 0.005, -0.008, 0.01, 0.003],
    [0.005, 0.015, 0.002, 0.005, 0.004],
    [-0.008, 0.002, 0.01, -0.003, 0.002],
    [0.01, 0.005, -0.003, 0.025, 0.006],
    [0.003, 0.004, 0.002, 0.006, 0.018]
])

# ç‰©ç†å¤šæ ·æ€§åˆ†åŒº (ç¡®ä¿é€‰å‡ºçš„å¥‡ç‚¹è¦†ç›–ä¸åŒæ–¹å‘)
DIVERSITY_ZONES = {
    "HIGH_E_HOT": {"axis": "E", "threshold": 0.75, "direction": "gt", "description": "é«˜èƒ½é‡æçƒ­å‹"},
    "LOW_E_WEAK": {"axis": "E", "threshold": 0.40, "direction": "lt", "description": "ä½èƒ½é‡èº«å¼±å‹"},
    "HIGH_S_STRESS": {"axis": "S", "threshold": 0.70, "direction": "gt", "description": "é«˜åº”åŠ›å‹"},
    "NEGATIVE_S_SHIELD": {"axis": "S", "threshold": 0.20, "direction": "lt", "description": "è´Ÿå‹å±è”½å‹"},
    "HIGH_R_ALLIANCE": {"axis": "R", "threshold": 0.60, "direction": "gt", "description": "é«˜å…³è”è”ç›Ÿå‹"},
    "LOW_M_POOR": {"axis": "M", "threshold": 0.10, "direction": "lt", "description": "æè´«è´¢åŠ¡å‹"},
    "HIGH_O_POWER": {"axis": "O", "threshold": 0.60, "direction": "gt", "description": "é«˜ç§©åºæƒåŠ›å‹"},
    "EXTREME_Y_HIGH": {"axis": "y_true", "threshold": 0.9, "direction": "gt", "description": "æç«¯é«˜æˆå°±"},
    "EXTREME_Y_LOW": {"axis": "y_true", "threshold": 0.3, "direction": "lt", "description": "æç«¯ä½æˆå°±"},
}


def calculate_mahalanobis_distance(tensor: np.ndarray) -> float:
    """è®¡ç®—é©¬æ°è·ç¦»"""
    try:
        diff = tensor - A03_STANDARD_MEAN
        inv_cov = np.linalg.inv(A03_COVARIANCE)
        return float(np.sqrt(np.dot(np.dot(diff, inv_cov), diff)))
    except:
        # å¦‚æœåæ–¹å·®çŸ©é˜µä¸å¯é€†ï¼Œä½¿ç”¨æ¬§æ°è·ç¦»
        return float(np.linalg.norm(tensor - A03_STANDARD_MEAN))


def classify_zone(tensor: np.ndarray, y_true: float) -> List[str]:
    """åˆ¤æ–­æ ·æœ¬å±äºå“ªäº›å¤šæ ·æ€§åˆ†åŒº"""
    zones = []
    axis_map = {"E": 0, "O": 1, "M": 2, "S": 3, "R": 4}
    
    for zone_id, zone_def in DIVERSITY_ZONES.items():
        axis = zone_def["axis"]
        threshold = zone_def["threshold"]
        direction = zone_def["direction"]
        
        if axis == "y_true":
            value = y_true
        else:
            value = tensor[axis_map[axis]]
        
        if direction == "gt" and value > threshold:
            zones.append(zone_id)
        elif direction == "lt" and value < threshold:
            zones.append(zone_id)
    
    return zones


def load_a03_matched_samples() -> List[Dict]:
    """åŠ è½½ A-03 å·²åŒ¹é…æ ·æœ¬"""
    results_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "results")
    universe_path = os.path.join(
        os.path.dirname(os.path.dirname(__file__)), 
        "core", "data", "holographic_universe_518k.jsonl"
    )
    
    # æ–¹æ¡ˆ1: ç›´æ¥ä» 518k åº“ç­›é€‰ç¬¦åˆ A-03 ç‰¹å¾çš„æ ·æœ¬
    # A-03 ç‰¹å¾: E > 0.5, S > 0.4 (ç¾Šåˆƒæ¶æ€åŸºæœ¬ç‰¹å¾)
    
    samples = []
    logger.info(f"æ­£åœ¨ä» 518k æ ·æœ¬åº“ä¸­ç­›é€‰ A-03 å€™é€‰...")
    
    with open(universe_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i == 0:  # è·³è¿‡å…ƒæ•°æ®è¡Œ
                continue
            
            try:
                data = json.loads(line.strip())
                tensor = data.get("tensor", {})
                y_true = data.get("y_true", 0.5)
                
                # A-03 åŸºæœ¬ç­›é€‰æ¡ä»¶
                E = tensor.get("E", 0)
                S = tensor.get("S", 0)
                O = tensor.get("O", 0)
                
                # ç¾Šåˆƒæ¶æ€ç‰¹å¾: é«˜èƒ½é‡ + ä¸­é«˜åº”åŠ› + æœ‰ä¸€å®šç§©åº
                if E > 0.45 and S > 0.35 and O > 0.20:
                    samples.append({
                        "uid": data.get("uid"),
                        "tensor": [E, O, tensor.get("M", 0), S, tensor.get("R", 0)],
                        "y_true": y_true
                    })
                    
            except Exception as e:
                continue
    
    logger.info(f"ä» 518k æ ·æœ¬ä¸­ç­›é€‰å‡º {len(samples)} ä¸ª A-03 å€™é€‰æ ·æœ¬")
    return samples


def mine_singularities(samples: List[Dict], top_n: int = 50, diversity_quota: int = 5) -> List[Dict]:
    """
    ç­›é€‰é«˜è´¨é‡å¥‡ç‚¹
    
    Args:
        samples: å€™é€‰æ ·æœ¬åˆ—è¡¨
        top_n: æœ€ç»ˆé€‰å–æ•°é‡
        diversity_quota: æ¯ä¸ªåˆ†åŒºé…é¢
        
    Returns:
        ç­›é€‰åçš„å¥‡ç‚¹åˆ—è¡¨
    """
    candidates = []
    
    for sample in samples:
        tensor = np.array(sample["tensor"])
        y_true = sample["y_true"]
        
        # è®¡ç®—é©¬æ°è·ç¦»
        m_dist = calculate_mahalanobis_distance(tensor)
        
        # åªä¿ç•™åç¦»è¾ƒå¤§çš„æ ·æœ¬ (D_M > 2.0)
        if m_dist < 2.0:
            continue
        
        # åˆ†ç±»å¤šæ ·æ€§åˆ†åŒº
        zones = classify_zone(tensor, y_true)
        
        candidates.append({
            "uid": sample["uid"],
            "tensor": sample["tensor"],
            "y_true": y_true,
            "mahalanobis_distance": m_dist,
            "zones": zones,
            "zone_count": len(zones)
        })
    
    logger.info(f"é©¬æ°è·ç¦» > 2.0 çš„å€™é€‰: {len(candidates)} ä¸ª")
    
    # æŒ‰å¤šæ ·æ€§åˆ†åŒºé…é¢ç­›é€‰
    selected = []
    zone_counts = {zone: 0 for zone in DIVERSITY_ZONES}
    
    # ä¼˜å…ˆé€‰æ‹©å±äºå¤šä¸ªåˆ†åŒºçš„æ ·æœ¬ï¼ˆæ›´æç«¯ï¼‰
    candidates.sort(key=lambda x: (-x["zone_count"], -x["mahalanobis_distance"]))
    
    for candidate in candidates:
        if len(selected) >= top_n:
            break
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰åˆ†åŒºé…é¢
        can_add = False
        for zone in candidate["zones"]:
            if zone_counts[zone] < diversity_quota:
                can_add = True
                break
        
        # å¦‚æœæ‰€æœ‰åˆ†åŒºéƒ½æ»¡äº†ï¼Œä½†æ€»æ•°è¿˜ä¸å¤Ÿï¼Œä¹Ÿæ·»åŠ 
        if not can_add and len(selected) < top_n // 2:
            can_add = True
        
        if can_add:
            selected.append(candidate)
            for zone in candidate["zones"]:
                zone_counts[zone] += 1
    
    # å¦‚æœè¿˜ä¸å¤Ÿï¼ŒæŒ‰é©¬æ°è·ç¦»è¡¥å……
    if len(selected) < top_n:
        remaining = [c for c in candidates if c not in selected]
        remaining.sort(key=lambda x: -x["mahalanobis_distance"])
        selected.extend(remaining[:top_n - len(selected)])
    
    return selected[:top_n]


def ingest_singularities(vault: VaultManager, singularities: List[Dict]) -> Dict[str, int]:
    """æ‰¹é‡å­˜è¯å¥‡ç‚¹åˆ°çŸ¥è¯†åº“"""
    stats = {"success": 0, "error": 0}
    
    for sing in singularities:
        try:
            case_id = f"A03_SING_{sing['uid']:06d}"
            
            # ç¡®å®šå­æ ¼å±€ç±»å‹
            zones = sing.get("zones", [])
            if "HIGH_R_ALLIANCE" in zones:
                sub_pattern = "SP_A03_ALLIANCE"
            elif "HIGH_S_STRESS" in zones:
                sub_pattern = "SP_A03_EXTREME_STRESS"
            elif "HIGH_E_HOT" in zones:
                sub_pattern = "SP_A03_HOT_BLADE"
            else:
                sub_pattern = "SP_A03_SINGULARITY"
            
            # ç”Ÿæˆæè¿°
            zone_desc = ", ".join([DIVERSITY_ZONES[z]["description"] for z in zones[:3]])
            
            metadata = {
                "pattern_id": "A-03",
                "sub_pattern": sub_pattern,
                "distance_to_manifold": sing["mahalanobis_distance"],
                "y_true": sing["y_true"],
                "zones": ",".join(zones),  # å°†åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”å­—ç¬¦ä¸²
                "zone_count": len(zones),
                "description": f"å¥‡ç‚¹æ ·æœ¬: {zone_desc}"
            }
            
            vault.add_singularity(
                case_id=case_id,
                tensor_5d=sing["tensor"],
                metadata=metadata
            )
            stats["success"] += 1
            
        except Exception as e:
            logger.error(f"å­˜è¯å¤±è´¥ (uid={sing.get('uid')}): {e}")
            stats["error"] += 1
    
    return stats


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸ”¬ A-03 å¥‡ç‚¹ç­›é€‰ä¸è‡ªåŠ¨åŒ–å­˜è¯")
    logger.info("=" * 60)
    
    # 1. åˆå§‹åŒ– VaultManager
    try:
        vault = VaultManager()
        logger.info(f"âœ… VaultManager åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   å½“å‰å¥‡ç‚¹åº“æ ·æœ¬æ•°: {vault.singularity_vault.count()}")
    except Exception as e:
        logger.error(f"âŒ VaultManager åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 2. åŠ è½½ A-03 å€™é€‰æ ·æœ¬
    samples = load_a03_matched_samples()
    if len(samples) < 50:
        logger.error(f"å€™é€‰æ ·æœ¬æ•°ä¸è¶³: {len(samples)}")
        return
    
    # 3. ç­›é€‰é«˜è´¨é‡å¥‡ç‚¹ï¼ˆç¡®ä¿å¤šæ ·æ€§ï¼‰
    logger.info("\nğŸ” ç­›é€‰é«˜è´¨é‡å¥‡ç‚¹ (ç¡®ä¿ç‰©ç†å¤šæ ·æ€§)...")
    singularities = mine_singularities(samples, top_n=50, diversity_quota=6)
    logger.info(f"ç­›é€‰å®Œæˆ: {len(singularities)} ä¸ªå¥‡ç‚¹")
    
    # æ˜¾ç¤ºå¤šæ ·æ€§åˆ†å¸ƒ
    zone_distribution = {}
    for sing in singularities:
        for zone in sing.get("zones", []):
            zone_distribution[zone] = zone_distribution.get(zone, 0) + 1
    
    logger.info("\nğŸ“Š å¤šæ ·æ€§åˆ†å¸ƒ:")
    for zone, count in sorted(zone_distribution.items(), key=lambda x: -x[1]):
        logger.info(f"   {zone}: {count} æ ·æœ¬")
    
    # 4. æ‰¹é‡å­˜è¯
    logger.info("\nâš›ï¸ æ‰¹é‡å­˜è¯åˆ°å¥‡ç‚¹åº“...")
    stats = ingest_singularities(vault, singularities)
    logger.info(f"   æˆåŠŸ: {stats['success']}, å¤±è´¥: {stats['error']}")
    
    # 5. éªŒè¯
    final_stats = vault.get_vault_stats()
    logger.info(f"\nğŸ“š æœ€ç»ˆçŸ¥è¯†åº“çŠ¶æ€:")
    logger.info(f"   è¯­ä¹‰åº“æ–‡æ¡£æ•°: {final_stats['semantic_count']}")
    logger.info(f"   å¥‡ç‚¹åº“æ ·æœ¬æ•°: {final_stats['singularity_count']}")
    
    # æµ‹è¯• KNN æ£€ç´¢å¤šæ ·æ€§
    logger.info("\nğŸ§ª æµ‹è¯• KNN æ£€ç´¢å¤šæ ·æ€§...")
    test_tensors = [
        ([0.80, 0.30, 0.10, 0.75, 0.20], "é«˜èƒ½é‡é«˜åº”åŠ›"),
        ([0.55, 0.25, 0.05, 0.60, 0.55], "è”ç›Ÿå‹"),
        ([0.35, 0.40, 0.15, 0.50, 0.30], "èº«å¼±å‹"),
    ]
    
    for tensor, desc in test_tensors:
        results = vault.query_singularities(tensor, n_results=3)
        nearest = results["ids"][0] if results["ids"] else "None"
        dist = results["distances"][0] if results["distances"] else 0
        logger.info(f"   {desc}: æœ€è¿‘é‚» {nearest} (è·ç¦»: {dist:.4f})")
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ… A-03 å¥‡ç‚¹ç­›é€‰ä¸å­˜è¯å®Œæˆï¼")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
