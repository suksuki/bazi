import streamlit as st
import ollama

def render_system_config(config_manager):
    """
    Renders the System Config page.
    Args:
        config_manager: Instance of ConfigManager.
    """
    st.header("âš™ï¸ ç³»ç»Ÿæ§åˆ¶å° (System Console)")
    
    # ==================== å­¦ä¹ ä»»åŠ¡é…ç½® ====================
    st.subheader("ğŸ“š å­¦ä¹ ä»»åŠ¡å¼•æ“ (Learning Engine)")
    
    col_learn_1, col_learn_2 = st.columns([1, 1])
    
    with col_learn_1:
        # å¹¶å‘ä»»åŠ¡æ•°è®¾ç½®
        cur_limit = int(config_manager.get('max_concurrent_jobs', 3))
        new_limit = st.number_input(
            "âš¡ æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°", 
            min_value=1, 
            max_value=10, 
            value=cur_limit,
            help="åŒæ—¶å¤„ç†çš„å­¦ä¹ ä»»åŠ¡æ•°é‡ã€‚å»ºè®®3-5ä¸ªï¼Œè¿‡é«˜å¯èƒ½å¯¼è‡´ç³»ç»Ÿèµ„æºç´§å¼ "
        )
        if new_limit != cur_limit:
            config_manager.save_config('max_concurrent_jobs', new_limit)
            st.success(f"âœ… å¹¶å‘æ•°å·²æ›´æ–°ä¸º {new_limit}ï¼Œæ–°ä»»åŠ¡å°†ç«‹å³ç”Ÿæ•ˆï¼")
    
    with col_learn_2:
        # å­—å¹•ä¼˜å…ˆçº§è®¾ç½®
        subtitle_priority = config_manager.get('subtitle_priority', True)
        new_priority = st.checkbox(
            "ğŸ’¬ ä¼˜å…ˆä½¿ç”¨CCå­—å¹•",
            value=subtitle_priority,
            help="å¼€å¯åï¼Œä¼˜å…ˆä¸‹è½½è§†é¢‘å­—å¹•ï¼Œè·³è¿‡Whisperè½¬å½•ï¼Œå¤§å¹…æå‡é€Ÿåº¦"
        )
        if new_priority != subtitle_priority:
            config_manager.save_config('subtitle_priority', new_priority)
            st.success(f"âœ… å­—å¹•ä¼˜å…ˆçº§å·²{'å¼€å¯' if new_priority else 'å…³é—­'}")
    
    # å­—å¹•è¯­è¨€é…ç½®
    with st.expander("ğŸŒ å­—å¹•è¯­è¨€ä¼˜å…ˆçº§", expanded=False):
        st.caption("è§†é¢‘å­¦ä¹ æ—¶ï¼Œå°†æŒ‰æ­¤é¡ºåºå°è¯•ä¸‹è½½å­—å¹•")
        
        default_langs = ['zh-Hans', 'zh-Hant', 'zh-CN', 'zh-TW', 'zh', 'en']
        current_langs = config_manager.get('subtitle_languages', default_langs)
        
        lang_text = st.text_area(
            "è¯­è¨€ä»£ç ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
            value="\n".join(current_langs),
            height=150,
            help="å¸¸ç”¨ä»£ç : zh-Hans(ç®€ä¸­), zh-Hant(ç¹ä¸­), zh(ä¸­æ–‡), en(è‹±æ–‡)"
        )
        
        new_langs = [lang.strip() for lang in lang_text.split('\n') if lang.strip()]
        if new_langs != current_langs:
            config_manager.save_config('subtitle_languages', new_langs)
            st.success(f"âœ… å­—å¹•è¯­è¨€ä¼˜å…ˆçº§å·²æ›´æ–°: {' â†’ '.join(new_langs[:3])}...")
    
    st.divider()
    
    # ==================== LLMé…ç½® ====================
    st.subheader("ğŸ¤– å¤§æ¨¡å‹è„‘æ ¸ (LLM Core)")
    col_llm_1, col_llm_2 = st.columns([1, 1])
    
    with col_llm_1:
         ollama_host = st.text_input("Ollama Server URL", value=st.session_state.get('ollama_host', 'http://localhost:11434'))
    
    # Update config
    if ollama_host != st.session_state.get('ollama_host'):
        st.session_state['ollama_host'] = ollama_host
        config_manager.save_config("ollama_host", ollama_host)

    with st.expander("ğŸ› ï¸ é«˜çº§è¿æ¥è°ƒè¯•", expanded=True):
        if st.button("ğŸ“¡ æµ‹è¯•è¿æ¥ & åˆ·æ–°æ¨¡å‹åˆ—è¡¨"):
            # Check availability (We assume ollama library is available as this function is imported when needed)
            try:
                client = ollama.Client(host=ollama_host)
                resp = client.list()
                # extract model names
                models = []
                model_list = resp.models if hasattr(resp, 'models') else resp.get('models', [])
                
                for m in model_list:
                    if hasattr(m, 'model'):
                        models.append(m.model)
                    elif isinstance(m, dict):
                        models.append(m.get('model') or m.get('name'))
                    else:
                        models.append(str(m))
                        
                st.session_state['ollama_models'] = models
                st.success(f"è¿æ¥æˆåŠŸ! å‘ç° {len(models)} ä¸ªæ¨¡å‹")
                
                # Save host on success
                config_manager.save_config("ollama_host", ollama_host)
                
            except Exception as e:
                st.error(f"è¿æ¥å¤±è´¥: {e}")
    
        # Model Selector
        model_options = st.session_state.get('ollama_models', [])
        
        index = 0
        current_selection = st.session_state.get('selected_model_name', '')
        if current_selection and current_selection in model_options:
            index = model_options.index(current_selection)
        
        saved_model = config_manager.get("selected_model_name") # Read for display info

        if model_options:
            selected_model_name = st.selectbox("é€‰æ‹©æ­¤æœåŠ¡å™¨ä¸Šçš„æ¨¡å‹", model_options, index=index)
            
            if selected_model_name != st.session_state.get('selected_model_name'):
                st.session_state['selected_model_name'] = selected_model_name
                config_manager.save_config("selected_model_name", selected_model_name)
            
            # Quick Test Button
            if st.button("ğŸŸ¢ éªŒè¯æ¨¡å‹å“åº” (Test Run)"):
                try:
                    with st.spinner("æ­£åœ¨å‘é€æµ‹è¯•ä¿¡å·..."):
                        client = ollama.Client(host=ollama_host)
                        res = client.generate(model=selected_model_name, prompt="Say 'Ready' in Chinese", stream=False)
                        st.success(f"æ¨¡å‹å“åº”æ­£å¸¸: {res['response']}")
                except Exception as e:
                    st.error(f"æ¨¡å‹æ— å“åº”: {e}")
        else:
            if saved_model:
                 st.info(f"ä¸Šæ¬¡ä½¿ç”¨çš„æ¨¡å‹: {saved_model} (çŠ¶æ€: æœªè¿æ¥)")
            else:
                 st.info("è¯·å…ˆæµ‹è¯•è¿æ¥ä»¥åŠ è½½æ¨¡å‹åˆ—è¡¨")

