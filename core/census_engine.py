"""
Step 2 å¤å…¸æµ·é€‰å¼•æ“ (Classical Census Engine)
=============================================
åŸºäºçº¯å¹²æ”¯é€»è¾‘æ‰§è¡Œ 518k æ ·æœ¬åº“å¸ƒå°”æ£€ç´¢

Version: 1.0
Compliance: FDS-LKV-CENSUS V1.0
"""

import os
import sys
import json
import logging
from typing import Dict, List, Any, Callable, Optional
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ============================================================
# ç¾Šåˆƒå¯¹ç…§è¡¨
# ============================================================
YANG_REN_MAP = {
    'ç”²': 'å¯', 'ä¸™': 'åˆ', 'æˆŠ': 'åˆ',
    'åºš': 'é…‰', 'å£¬': 'å­'
}

# åç¥è‹±æ–‡æ˜ å°„
SHEN_MAP = {
    'bi_jian': 'æ¯”è‚©', 'jie_cai': 'åŠ«è´¢',
    'shi_shen': 'é£Ÿç¥', 'shang_guan': 'ä¼¤å®˜',
    'zheng_cai': 'æ­£è´¢', 'pian_cai': 'åè´¢',
    'zheng_guan': 'æ­£å®˜', 'qi_sha': 'ä¸ƒæ€',
    'zheng_yin': 'æ­£å°', 'pian_yin': 'åå°'
}


class ClassicalCensusEngine:
    """
    å¤å…¸æµ·é€‰å¼•æ“
    
    åŸºäºçº¯å¹²æ”¯é€»è¾‘ï¼ˆæ— å¼ é‡ä»‹å…¥ï¼‰æ‰§è¡Œæ ·æœ¬ç­›é€‰
    """
    
    def __init__(self, universe_path: str = None):
        """
        åˆå§‹åŒ–å¼•æ“
        
        Args:
            universe_path: 518k æ ·æœ¬åº“è·¯å¾„
        """
        self.universe_path = universe_path or str(
            Path(__file__).parent / "data" / "holographic_universe_518k.jsonl"
        )
        # åˆå§‹åŒ–ç¼–è¯‘å™¨
        from core.logic_compiler import LogicCompiler
        self.compiler = LogicCompiler()
    
    # ================================================================
    # æµ·é€‰æ‰§è¡Œ
    # ================================================================
    
    def census(
        self, 
        pattern_id: str, 
        limit: int = None,
        include_tensor: bool = False
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¤å…¸æµ·é€‰
        
        Args:
            pattern_id: æ ¼å±€ ID (å¦‚ 'A-03' æˆ– 'A-03@å¯…')
            limit: é™åˆ¶æ‰«ææ ·æœ¬æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰
            include_tensor: æ˜¯å¦åŒ…å« 5D å¼ é‡ï¼ˆStep 3 æ‰éœ€è¦ï¼‰
            
        Returns:
            æµ·é€‰ç»“æœ
        """
        try:
            # åŠ¨æ€ç¼–è¯‘è¿‡æ»¤å™¨
            filter_func = self.compiler.compile(pattern_id)
        except ValueError as e:
            raise ValueError(f"ç¼–è¯‘å¤±è´¥: {e}")
        
        matched = []
        total_scanned = 0
        
        logger.info(f"ğŸ” å¼€å§‹ {pattern_id} å¤å…¸æµ·é€‰...")
        
        with open(self.universe_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i == 0:  # è·³è¿‡å…ƒæ•°æ®è¡Œ
                    continue
                
                if limit and i > limit:
                    break
                
                try:
                    sample = json.loads(line.strip())
                    total_scanned += 1
                    
                    # æ¨¡æ‹Ÿå…«å­—æ•°æ®ç»“æ„ï¼ˆå®é™…éœ€è¦ä»æ ·æœ¬ä¸­æå–ï¼‰
                    # è¿™é‡Œä½¿ç”¨ tensor åšç®€åŒ–æ˜ å°„
                    bazi = self._tensor_to_mock_bazi(sample)
                    
                    if filter_func(bazi):
                        result = {"uid": sample.get("uid")}
                        if include_tensor:
                            result["tensor"] = sample.get("tensor")
                        matched.append(result)
                        
                except Exception as e:
                    continue
        
        abundance = len(matched) / total_scanned if total_scanned > 0 else 0
        
        result = {
            "pattern_id": pattern_id,
            "total_scanned": total_scanned,
            "matched_count": len(matched),
            "abundance": abundance,
            "samples": matched
        }
        
        logger.info(f"âœ… æµ·é€‰å®Œæˆ: {len(matched)} / {total_scanned} (ä¸°åº¦: {abundance:.6f})")
        
        return result
    
    def _tensor_to_mock_bazi(self, sample: Dict) -> Dict:
        """
        å°†å¼ é‡æ ·æœ¬è½¬æ¢ä¸ºæ¨¡æ‹Ÿå…«å­—ç»“æ„
        
        æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–çš„æ¼”ç¤ºé€»è¾‘
        å®é™…åº”ç”¨ä¸­éœ€è¦ä»çœŸå®å…«å­—æ•°æ®ä¸­æå–å¹²æ”¯ä¿¡æ¯
        """
        tensor = sample.get("tensor", {})
        uid = sample.get("uid", 0)
        
        # åŸºäº UID æ¨¡æ‹Ÿæ—¥ä¸»ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        day_masters = ['ç”²', 'ä¹™', 'ä¸™', 'ä¸', 'æˆŠ', 'å·±', 'åºš', 'è¾›', 'å£¬', 'ç™¸']
        day_master = day_masters[uid % 10]
        
        # åŸºäºå¼ é‡å€¼æ¨¡æ‹Ÿæœˆä»¤å’Œå¤©å¹²ï¼ˆæ¼”ç¤ºç”¨ï¼‰
        # å®é™…éœ€è¦ä»çœŸå®å…«å­—æ•°æ®ä¸­è¯»å–
        month_branches = ['å­', 'ä¸‘', 'å¯…', 'å¯', 'è¾°', 'å·³', 'åˆ', 'æœª', 'ç”³', 'é…‰', 'æˆŒ', 'äº¥']
        month_branch = month_branches[uid % 12]
        
        # æ¨¡æ‹Ÿæœˆä»¤ä¸»æ°”
        E = tensor.get("E", 0)
        O = tensor.get("O", 0)
        S = tensor.get("S", 0)
        
        if O > 0.5:
            month_main = 'zheng_guan' if uid % 2 == 0 else 'qi_sha'
        elif E > 0.6:
            month_main = 'bi_jian'
        elif S > 0.5:
            month_main = 'shi_shen' if uid % 3 == 0 else 'shang_guan'
        else:
            month_main = 'zheng_cai' if uid % 2 == 0 else 'pian_cai'
        
        # æ¨¡æ‹Ÿå¤©å¹²åç¥
        stems = []
        if O > 0.4:
            stems.append('zheng_guan' if uid % 2 == 0 else 'qi_sha')
        if tensor.get("M", 0) > 0.3:
            stems.append('zheng_cai' if uid % 2 == 0 else 'pian_cai')
        if E > 0.5:
            stems.append('bi_jian')
        if tensor.get("R", 0) > 0.4:
            stems.append('shi_shen')
        if tensor.get("S", 0) > 0.4:
            stems.append('shang_guan')
        
        return {
            "uid": uid,
            "day_master": day_master,
            "month_branch": month_branch,
            "month_main": month_main,
            "stems": stems
        }
    
    def save_results(self, results: Dict, output_path: str = None):
        """ä¿å­˜æµ·é€‰ç»“æœ"""
        if output_path is None:
            output_path = f"results/{results['pattern_id']}_census.matched.json"
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ ç»“æœå·²ä¿å­˜: {output_path}")


# ================================================================
# å…¨å±€å•ä¾‹
# ================================================================
_census_engine: Optional[ClassicalCensusEngine] = None


def get_census_engine() -> ClassicalCensusEngine:
    """è·å– ClassicalCensusEngine å•ä¾‹"""
    global _census_engine
    if _census_engine is None:
        _census_engine = ClassicalCensusEngine()
    return _census_engine


# ================================================================
# å‘½ä»¤è¡Œå…¥å£
# ================================================================
if __name__ == "__main__":
    engine = ClassicalCensusEngine()
    
    # æµ‹è¯• A-03 æµ·é€‰ï¼ˆé™åˆ¶æ‰«æ 10000 æ ·æœ¬ï¼‰
    results = engine.census("A-03", limit=10000, include_tensor=True)
    
    print(f"\n=== A-03 æµ·é€‰ç»“æœ ===")
    print(f"æ‰«ææ ·æœ¬: {results['total_scanned']}")
    print(f"å‘½ä¸­æ•°é‡: {results['matched_count']}")
    print(f"ä¸°åº¦: {results['abundance']:.6f}")
    
    if results['samples']:
        print(f"\nå‰ 5 ä¸ªæ ·æœ¬ UID: {[s['uid'] for s in results['samples'][:5]]}")
