"""
FDS-Knowledge-Vault (FKV) æ ¸å¿ƒç®¡ç†ç±»
====================================
åŒè½¨çŸ¥è¯†åº“ç³»ç»Ÿ:
- è¯­ä¹‰åº“ (Semantic Vault): å­˜å‚¨è§„èŒƒæ–‡æ¡£ï¼Œä½¿ç”¨ Embedding API
- å¥‡ç‚¹åº“ (Singularity Vault): å­˜å‚¨ 5D å¼ é‡ï¼Œç›´æ¥ä½œä¸ºåæ ‡

Version: 1.0
Compliance: FDS-V3.0
"""

import logging
import os
from typing import List, Dict, Any, Optional

import chromadb
from chromadb.config import Settings

from core.config_manager import ConfigManager

logger = logging.getLogger(__name__)

# çŸ¥è¯†åº“å­˜å‚¨è·¯å¾„
VAULT_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "knowledge_vault")


class VaultManager:
    """
    FDS-Knowledge-Vault æ ¸å¿ƒç®¡ç†ç±»
    
    è´Ÿè´£ç®¡ç†åŒè½¨çŸ¥è¯†åº“:
    1. è¯­ä¹‰åº“ (fds_semantics): å­˜å‚¨è§„èŒƒæ–‡æ¡£ã€å¤ç±ã€å¿ƒå¾—
    2. å¥‡ç‚¹åº“ (fds_singularities): å­˜å‚¨ 5D ç‰¹å¾å¼ é‡ï¼Œç”¨äºç‰©ç†ç´¢å¼•
    """
    
    def __init__(self, embedding_model: str = None):
        """
        åˆå§‹åŒ– VaultManager
        
        Args:
            embedding_model: Embedding æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–æˆ–ä½¿ç”¨ nomic-embed-textï¼‰
        """
        # ä»é…ç½®è¯»å– embedding æ¨¡å‹
        config = ConfigManager()
        if embedding_model is None:
            embedding_model = config.get("knowledge_vault", {}).get(
                "embedding_model", "nomic-embed-text"
            ) if isinstance(config.get("knowledge_vault"), dict) else "nomic-embed-text"
        
        self.embedding_model = embedding_model
        self._ollama_host = config.get("ollama_host", "http://localhost:11434")
        self._ollama_client = None
        
        # åˆå§‹åŒ– ChromaDB æŒä¹…åŒ–å®¢æˆ·ç«¯
        os.makedirs(VAULT_PATH, exist_ok=True)
        self.client = chromadb.PersistentClient(
            path=VAULT_PATH,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # åˆ›å»º/è·å–åŒè½¨ Collection
        # 1. è¯­ä¹‰åº“ï¼šä½¿ç”¨ embedding function
        self.semantic_vault = self.client.get_or_create_collection(
            name="fds_semantics",
            metadata={"description": "FDS è§„èŒƒæ–‡æ¡£ä¸è¯­ä¹‰çŸ¥è¯†åº“"}
        )
        
        # 2. å¥‡ç‚¹åº“ï¼š5D å¼ é‡ç›´å­˜ï¼Œä¸éœ€è¦ embedding
        self.singularity_vault = self.client.get_or_create_collection(
            name="fds_singularities",
            metadata={"description": "FDS 5D ç‰¹å¾å¼ é‡ï¼ˆå¥‡ç‚¹å­˜è¯ï¼‰"}
        )
        
        logger.info(f"âœ… VaultManager åˆå§‹åŒ–æˆåŠŸ (Embedding: {self.embedding_model})")
        logger.info(f"   - è¯­ä¹‰åº“æ–‡æ¡£æ•°: {self.semantic_vault.count()}")
        logger.info(f"   - å¥‡ç‚¹åº“æ ·æœ¬æ•°: {self.singularity_vault.count()}")
    
    def _get_ollama_client(self):
        """è·å–æˆ–åˆ›å»º Ollama å®¢æˆ·ç«¯"""
        if self._ollama_client is None:
            try:
                import ollama
                if self._ollama_host and self._ollama_host != "http://localhost:11434":
                    self._ollama_client = ollama.Client(host=self._ollama_host)
                else:
                    self._ollama_client = ollama.Client()
                logger.debug(f"Ollama å®¢æˆ·ç«¯å·²åˆ›å»º (host: {self._ollama_host})")
            except ImportError:
                logger.error("âŒ ollama æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ embedding åŠŸèƒ½")
                raise ImportError("è¯·å®‰è£… ollama: pip install ollama")
        return self._ollama_client
    
    def get_embedding(self, text: str) -> List[float]:
        """
        è°ƒç”¨ Ollama Embedding API è·å–æ–‡æœ¬å‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            embedding å‘é‡ (List[float])
        """
        client = self._get_ollama_client()
        
        try:
            response = client.embeddings(
                model=self.embedding_model,
                prompt=text
            )
            
            # Ollama embeddings API è¿”å›æ ¼å¼: {"embedding": [...]}
            if isinstance(response, dict) and "embedding" in response:
                return response["embedding"]
            elif hasattr(response, "embedding"):
                return response.embedding
            else:
                raise ValueError(f"Unexpected embedding response format: {type(response)}")
                
        except Exception as e:
            logger.error(f"âŒ Embedding è·å–å¤±è´¥: {e}")
            raise
    
    def add_specification(self, step_name: str, content: str, metadata: dict = None):
        """
        æ³¨å…¥æ–‡æœ¬è§„èŒƒåˆ°è¯­ä¹‰åº“
        
        Args:
            step_name: è§„èŒƒæ­¥éª¤åç§°ï¼ˆå¦‚ 'Step_2_Census'ï¼‰ï¼Œä½œä¸ºå”¯ä¸€ ID
            content: è§„èŒƒå†…å®¹æ–‡æœ¬
            metadata: é¢å¤–å…ƒæ•°æ®
        """
        # è·å– embedding
        vector = self.get_embedding(content)
        
        # æ„å»ºå…ƒæ•°æ®
        meta = {"step": step_name}
        if metadata:
            meta.update(metadata)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
        existing = self.semantic_vault.get(ids=[step_name])
        if existing and existing.get("ids"):
            # æ›´æ–°å·²å­˜åœ¨çš„æ–‡æ¡£
            self.semantic_vault.update(
                embeddings=[vector],
                documents=[content],
                metadatas=[meta],
                ids=[step_name]
            )
            logger.info(f"ğŸ“ è¯­ä¹‰åº“æ›´æ–°: {step_name}")
        else:
            # æ·»åŠ æ–°æ–‡æ¡£
            self.semantic_vault.add(
                embeddings=[vector],
                documents=[content],
                metadatas=[meta],
                ids=[step_name]
            )
            logger.info(f"ğŸ“š è¯­ä¹‰åº“æ³¨å…¥: {step_name}")
    
    def add_singularity(self, case_id: str, tensor_5d: List[float], metadata: dict = None):
        """
        æ³¨å…¥å¥‡ç‚¹æ ·æœ¬åˆ°å¥‡ç‚¹åº“
        
        Args:
            case_id: æ ·æœ¬å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆå¦‚ 'CASE-9527'ï¼‰
            tensor_5d: 5D ç‰¹å¾å¼ é‡ [E, O, M, S, R]
            metadata: é¢å¤–å…ƒæ•°æ®ï¼ˆå¦‚ pattern_id, abundance, distance_to_manifoldï¼‰
        """
        # éªŒè¯å¼ é‡ç»´åº¦
        if len(tensor_5d) != 5:
            raise ValueError(f"tensor_5d å¿…é¡»æ˜¯ 5 ç»´å‘é‡ï¼Œå½“å‰ç»´åº¦: {len(tensor_5d)}")
        
        # æ„å»ºå…ƒæ•°æ®
        meta = {"case_id": case_id}
        if metadata:
            meta.update(metadata)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = self.singularity_vault.get(ids=[case_id])
        if existing and existing.get("ids"):
            # æ›´æ–°
            self.singularity_vault.update(
                embeddings=[tensor_5d],
                metadatas=[meta],
                ids=[case_id]
            )
            logger.info(f"ğŸ”„ å¥‡ç‚¹åº“æ›´æ–°: {case_id}")
        else:
            # æ·»åŠ 
            self.singularity_vault.add(
                embeddings=[tensor_5d],
                metadatas=[meta],
                ids=[case_id]
            )
            logger.info(f"âš›ï¸ å¥‡ç‚¹åº“æ³¨å…¥: {case_id}")
    
    def query_singularities(
        self, 
        tensor: List[float], 
        n_results: int = 3,
        where: dict = None
    ) -> Dict[str, Any]:
        """
        ç‰©ç†æ£€ç´¢ï¼šåœ¨å¥‡ç‚¹åº“ä¸­å¯»æ‰¾æœ€è¿‘é‚»
        
        Args:
            tensor: æŸ¥è¯¢å‘é‡ [E, O, M, S, R]
            n_results: è¿”å›ç»“æœæ•°é‡
            where: è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚ {"pattern_id": "A-03"}ï¼‰
            
        Returns:
            æ£€ç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å« ids, distances, metadatas
        """
        if len(tensor) != 5:
            raise ValueError(f"æŸ¥è¯¢å¼ é‡å¿…é¡»æ˜¯ 5 ç»´ï¼Œå½“å‰ç»´åº¦: {len(tensor)}")
        
        results = self.singularity_vault.query(
            query_embeddings=[tensor],
            n_results=n_results,
            where=where,
            include=["metadatas", "distances", "embeddings"]
        )
        
        return {
            "ids": results.get("ids", [[]])[0],
            "distances": results.get("distances", [[]])[0],
            "metadatas": results.get("metadatas", [[]])[0],
            "embeddings": results.get("embeddings", [[]])[0]
        }
    
    def query_semantics(
        self, 
        query: str, 
        n_results: int = 3,
        where: dict = None
    ) -> Dict[str, Any]:
        """
        è¯­ä¹‰æ£€ç´¢ï¼šåœ¨è¯­ä¹‰åº“ä¸­å¯»æ‰¾ç›¸å…³è§„èŒƒ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            where: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            æ£€ç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å« ids, documents, distances, metadatas
        """
        # è·å–æŸ¥è¯¢æ–‡æœ¬çš„ embedding
        query_vector = self.get_embedding(query)
        
        results = self.semantic_vault.query(
            query_embeddings=[query_vector],
            n_results=n_results,
            where=where,
            include=["metadatas", "distances", "documents"]
        )
        
        return {
            "ids": results.get("ids", [[]])[0],
            "distances": results.get("distances", [[]])[0],
            "metadatas": results.get("metadatas", [[]])[0],
            "documents": results.get("documents", [[]])[0]
        }
    
    def get_vault_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "semantic_count": self.semantic_vault.count(),
            "singularity_count": self.singularity_vault.count(),
            "vault_path": VAULT_PATH,
            "embedding_model": self.embedding_model,
            "ollama_host": self._ollama_host
        }
    
    def auto_ingest_protocol(
        self, 
        file_path: str, 
        version: str = "3.0",
        doc_type: str = "protocol"
    ) -> Dict[str, Any]:
        """
        è‡ªåŠ¨åˆ†ç‰‡æ³¨å…¥è§„èŒƒæ–‡æ¡£ (FDS-LKV å»ºè®¾å®ªæ³•)
        
        æŒ‰ ## äºŒçº§æ ‡é¢˜åˆ‡å‰²æ–‡æ¡£ï¼Œä¸ºæ¯ä¸ªåˆ†ç‰‡ç”Ÿæˆ embedding å¹¶æ³¨å…¥è¯­ä¹‰åº“ã€‚
        æ”¯æŒå¹‚ç­‰æ€§ï¼šé€šè¿‡æ ‡é¢˜ç”Ÿæˆ HashIDï¼Œé‡å¤æ³¨å…¥æ—¶æ‰§è¡Œè¦†ç›–æ›´æ–°ã€‚
        
        Args:
            file_path: è§„èŒƒæ–‡æ¡£è·¯å¾„ (å¦‚ docs/FDS_MODELING_SPEC_v3.0.md)
            version: è§„èŒƒç‰ˆæœ¬å·
            doc_type: æ–‡æ¡£ç±»å‹ (protocol/axiom/pattern)
            
        Returns:
            æ³¨å…¥ç»Ÿè®¡ä¿¡æ¯
        """
        import re
        import hashlib
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"è§„èŒƒæ–‡æ¡£ä¸å­˜åœ¨: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŒ‰ ## äºŒçº§æ ‡é¢˜åˆ†ç‰‡
        chunks = re.split(r'\n(?=##\s)', content)
        
        stats = {"total": 0, "injected": 0, "updated": 0, "errors": 0}
        
        for chunk in chunks:
            if len(chunk.strip()) < 50:  # è·³è¿‡è¿‡çŸ­çš„åˆ†ç‰‡
                continue
            
            stats["total"] += 1
            
            # æå–æ ‡é¢˜ä½œä¸ºåˆ†ç‰‡åç§°
            title_match = re.search(r'##\s*(.+?)(?:\n|$)', chunk)
            title = title_match.group(1).strip() if title_match else "General_Spec"
            
            # ç”Ÿæˆå¹‚ç­‰ HashID (åŸºäºæ ‡é¢˜ + ç‰ˆæœ¬)
            hash_input = f"{title}_{version}_{doc_type}"
            hash_id = f"PROT_{hashlib.md5(hash_input.encode()).hexdigest()[:12]}"
            
            try:
                # è·å– embedding
                vector = self.get_embedding(chunk)
                
                # æ„å»ºå…ƒæ•°æ®
                metadata = {
                    "title": title,
                    "version": version,
                    "type": doc_type,
                    "source": os.path.basename(file_path)
                }
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = self.semantic_vault.get(ids=[hash_id])
                if existing and existing.get("ids"):
                    # è¦†ç›–æ›´æ–°
                    self.semantic_vault.update(
                        embeddings=[vector],
                        documents=[chunk],
                        metadatas=[metadata],
                        ids=[hash_id]
                    )
                    stats["updated"] += 1
                    logger.debug(f"ğŸ“ æ›´æ–°åˆ†ç‰‡: {title}")
                else:
                    # æ–°å¢
                    self.semantic_vault.add(
                        embeddings=[vector],
                        documents=[chunk],
                        metadatas=[metadata],
                        ids=[hash_id]
                    )
                    stats["injected"] += 1
                    logger.debug(f"ğŸ“š æ³¨å…¥åˆ†ç‰‡: {title}")
                    
            except Exception as e:
                stats["errors"] += 1
                logger.error(f"âŒ åˆ†ç‰‡æ³¨å…¥å¤±è´¥ ({title}): {e}")
        
        logger.info(f"âœ… è‡ªåŠ¨åˆ†ç‰‡æ³¨å…¥å®Œæˆ: {file_path}")
        logger.info(f"   - æ€»åˆ†ç‰‡æ•°: {stats['total']}")
        logger.info(f"   - æ–°å¢: {stats['injected']}, æ›´æ–°: {stats['updated']}, é”™è¯¯: {stats['errors']}")
        
        return stats
    
    def check_physics_compliance(self, pattern_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆè§„æ€§å…ˆéªŒæ£€æŸ¥ (Pre-check Protocol)
        
        æ£€ç´¢è¯­ä¹‰åº“ä¸­çš„"ä¸‰å¤§ç‰©ç†å…¬ç†"ï¼ŒéªŒè¯æ ¼å±€é…ç½®æ˜¯å¦ç¬¦åˆè§„èŒƒã€‚
        
        Args:
            pattern_config: æ ¼å±€é…ç½®å­—å…¸ï¼ŒåŒ…å« weight_matrix, gating ç­‰
            
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸:
            - compliant: bool (æ˜¯å¦åˆè§„)
            - violations: List[str] (è¿è§„é¡¹åˆ—è¡¨)
            - matched_axioms: List[str] (åŒ¹é…åˆ°çš„å…¬ç†)
        """
        result = {
            "compliant": True,
            "violations": [],
            "matched_axioms": [],
            "recommendations": []
        }
        
        # 1. æ£€ç´¢"ä¸‰å¤§ç‰©ç†å…¬ç†"ç›¸å…³è§„èŒƒ
        try:
            axiom_docs = self.query_semantics(
                query="ç¬¦å·å®ˆæ’ æ‹“æ‰‘ç‰¹å¼‚æ€§ æ­£äº¤è§£è€¦ ç‰©ç†å…¬ç†",
                n_results=3
            )
            result["matched_axioms"] = axiom_docs.get("ids", [])
        except Exception as e:
            logger.warning(f"âš ï¸ å…¬ç†æ£€ç´¢å¤±è´¥: {e}")
            result["recommendations"].append("å»ºè®®è¿è¡Œè§„èŒƒæ³¨å…¥è„šæœ¬: auto_ingest_protocol()")
            return result
        
        # 2. éªŒè¯ç¬¦å·å®ˆæ’ (Conservation of Sign)
        if "weight_matrix" in pattern_config or "matrix_override" in pattern_config:
            matrix = pattern_config.get("weight_matrix") or pattern_config.get("matrix_override", {})
            
            # æ£€æŸ¥ç‰¹å®šç¬¦å·çº¦æŸ
            # ä¾‹å¦‚ï¼šå†² (clash) åº”è¯¥å¢åŠ  S (stress)
            if "clash" in str(matrix).lower():
                s_weight = matrix.get("S", 0) if isinstance(matrix, dict) else 0
                if isinstance(s_weight, (int, float)) and s_weight < 0:
                    result["compliant"] = False
                    result["violations"].append("ç¬¦å·å®ˆæ’è¿è§„: å†² (clash) ä¸åº”é™ä½ S è½´")
        
        # 3. éªŒè¯å®‰å…¨é—¨æ§ (Gating Parameters)
        if "gating" in pattern_config:
            gating = pattern_config["gating"]
            
            # æ£€æŸ¥èº«æ—ºé—¨æ§
            if "weak_self_limit" in gating:
                if gating["weak_self_limit"] < 0.3 or gating["weak_self_limit"] > 0.7:
                    result["recommendations"].append(
                        f"èº«æ—ºé—¨æ§å€¼ {gating['weak_self_limit']} è¶…å‡ºæ¨èèŒƒå›´ [0.3, 0.7]"
                    )
        
        # 4. è®°å½•æ£€æŸ¥ç»“æœ
        if result["compliant"]:
            logger.info("âœ… åˆè§„æ€§æ£€æŸ¥é€šè¿‡")
        else:
            logger.warning(f"âš ï¸ åˆè§„æ€§æ£€æŸ¥å‘ç°è¿è§„: {result['violations']}")
        
        return result


# å…¨å±€å•ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_vault_manager: Optional[VaultManager] = None


def get_vault_manager() -> VaultManager:
    """è·å– VaultManager å•ä¾‹"""
    global _vault_manager
    if _vault_manager is None:
        _vault_manager = VaultManager()
    return _vault_manager

