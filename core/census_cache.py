"""
æµ·é€‰ç»“æœå…¨æ¯ç¼“å­˜ (Census Cache Layer)
=====================================
ç¼“å­˜æµ·é€‰ç»“æœå’Œç‰¹å¾é‡å¿ƒï¼Œå®ç°æ¯«ç§’çº§é¢„æµ‹å“åº”

æ¶æ„å®šä½ï¼š
- å°†æµ·é€‰ ID é›†åˆå’Œç‰¹å¾é‡å¿ƒ (Î¼, Î£) ç¼“å­˜åˆ°å†…å­˜
- æ–°ç”¨æˆ·è¾“å…¥æ—¶è¿›è¡Œ"æŒ‡çº¹å¿«é€Ÿæ¯”å¯¹"
- å‘½ä¸­ç¼“å­˜ç›´æ¥ç§’å‡ºæŠ¥å‘Š

Version: 1.0
Compliance: FDS-LKV V1.0
"""

import os
import json
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)


class CensusCache:
    """
    æµ·é€‰ç»“æœå…¨æ¯ç¼“å­˜
    
    ç¼“å­˜ç»“æ„ï¼š
    - pattern_id -> {
        "mean_vector": [E, O, M, S, R],
        "covariance": [[...]],
        "sample_count": N,
        "abundance": float,
        "sample_ids": [uid1, uid2, ...],
        "cached_at": timestamp
      }
    """
    
    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–ç¼“å­˜
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼ˆNone åˆ™ä½¿ç”¨å†…å­˜ç¼“å­˜ï¼‰
        """
        self.cache_dir = Path(cache_dir) if cache_dir else None
        self._memory_cache: Dict[str, Dict] = {}
        self._load_persisted_cache()
    
    def _load_persisted_cache(self):
        """åŠ è½½æŒä¹…åŒ–ç¼“å­˜"""
        if self.cache_dir and self.cache_dir.exists():
            for cache_file in self.cache_dir.glob("*.cache"):
                try:
                    with open(cache_file, 'r') as f:
                        data = json.load(f)
                        pattern_id = data.get("pattern_id")
                        if pattern_id:
                            self._memory_cache[pattern_id] = data
                            logger.debug(f"åŠ è½½ç¼“å­˜: {pattern_id}")
                except Exception as e:
                    logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {cache_file}: {e}")
    
    def cache_census_result(
        self, 
        pattern_id: str, 
        samples: List[Dict],
        metadata: Dict = None
    ) -> Dict[str, Any]:
        """
        ç¼“å­˜æµ·é€‰ç»“æœ
        
        Args:
            pattern_id: æ ¼å±€ ID
            samples: æµ·é€‰æ ·æœ¬åˆ—è¡¨ [{"uid": ..., "tensor": {...}}, ...]
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            ç¼“å­˜æ‘˜è¦
        """
        if not samples:
            return {"cached": False, "reason": "ç©ºæ ·æœ¬é›†"}
        
        # æå–å¼ é‡
        tensors = []
        sample_ids = []
        
        for sample in samples:
            tensor = sample.get("tensor", {})
            if tensor:
                tensors.append([
                    tensor.get("E", 0),
                    tensor.get("O", 0),
                    tensor.get("M", 0),
                    tensor.get("S", 0),
                    tensor.get("R", 0)
                ])
                sample_ids.append(sample.get("uid"))
        
        if not tensors:
            return {"cached": False, "reason": "æ— æœ‰æ•ˆå¼ é‡"}
        
        # è®¡ç®—ç‰¹å¾é‡å¿ƒ
        tensor_array = np.array(tensors)
        mean_vector = np.mean(tensor_array, axis=0).tolist()
        
        # è®¡ç®—åæ–¹å·®ï¼ˆå¦‚æœæ ·æœ¬è¶³å¤Ÿï¼‰
        if len(tensors) >= 5:
            covariance = np.cov(tensor_array.T).tolist()
        else:
            covariance = np.eye(5).tolist()
        
        # æ„å»ºç¼“å­˜å¯¹è±¡
        cache_obj = {
            "pattern_id": pattern_id,
            "mean_vector": mean_vector,
            "covariance": covariance,
            "sample_count": len(sample_ids),
            "abundance": len(sample_ids) / 518400,
            "sample_ids": sample_ids[:1000],  # åªå­˜å‰ 1000 ä¸ª ID
            "cached_at": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        # å†™å…¥å†…å­˜
        self._memory_cache[pattern_id] = cache_obj
        
        # æŒä¹…åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.cache_dir:
            self._persist_cache(pattern_id, cache_obj)
        
        logger.info(f"âœ… ç¼“å­˜å®Œæˆ: {pattern_id} ({len(sample_ids)} æ ·æœ¬, Î¼={mean_vector[:3]}...)")
        
        return {
            "cached": True,
            "pattern_id": pattern_id,
            "sample_count": len(sample_ids),
            "mean_vector": mean_vector
        }
    
    def _persist_cache(self, pattern_id: str, cache_obj: Dict):
        """æŒä¹…åŒ–ç¼“å­˜åˆ°ç£ç›˜"""
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file = self.cache_dir / f"{pattern_id}.cache"
        with open(cache_file, 'w') as f:
            json.dump(cache_obj, f, ensure_ascii=False, indent=2)
    
    def get_cached_manifold(self, pattern_id: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„æµå½¢ç‰¹å¾"""
        return self._memory_cache.get(pattern_id)
    
    def fingerprint_match(
        self, 
        tensor_5d: List[float],
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æŒ‡çº¹å¿«é€Ÿæ¯”å¯¹
        
        è®¡ç®—è¾“å…¥å¼ é‡ä¸æ‰€æœ‰ç¼“å­˜æµå½¢çš„è·ç¦»ï¼Œè¿”å›æœ€ç›¸ä¼¼çš„ Top K
        
        Args:
            tensor_5d: è¾“å…¥å…«å­—çš„ 5D å¼ é‡
            top_k: è¿”å›æ•°é‡
            
        Returns:
            åŒ¹é…ç»“æœåˆ—è¡¨ [{pattern_id, distance, similarity, ...}]
        """
        if not self._memory_cache:
            return []
        
        input_tensor = np.array(tensor_5d)
        matches = []
        
        for pattern_id, cache_obj in self._memory_cache.items():
            mean = np.array(cache_obj["mean_vector"])
            cov = np.array(cache_obj["covariance"])
            
            # è®¡ç®—é©¬æ°è·ç¦»
            try:
                diff = input_tensor - mean
                inv_cov = np.linalg.inv(cov)
                m_dist = float(np.sqrt(np.dot(np.dot(diff, inv_cov), diff)))
            except:
                # åæ–¹å·®çŸ©é˜µä¸å¯é€†æ—¶ä½¿ç”¨æ¬§æ°è·ç¦»
                m_dist = float(np.linalg.norm(input_tensor - mean))
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            cos_sim = float(np.dot(input_tensor, mean) / (
                np.linalg.norm(input_tensor) * np.linalg.norm(mean) + 1e-10
            ))
            
            matches.append({
                "pattern_id": pattern_id,
                "pattern_name": cache_obj.get("metadata", {}).get("name", pattern_id),
                "mahalanobis_distance": m_dist,
                "cosine_similarity": cos_sim,
                "sample_count": cache_obj["sample_count"],
                "abundance": cache_obj["abundance"]
            })
        
        # æŒ‰é©¬æ°è·ç¦»æ’åº
        matches.sort(key=lambda x: x["mahalanobis_distance"])
        
        return matches[:top_k]
    
    def instant_predict(
        self, 
        tensor_5d: List[float],
        threshold: float = 2.5
    ) -> Dict[str, Any]:
        """
        æ¯«ç§’çº§ç¬æ—¶é¢„æµ‹
        
        é€šè¿‡ç¼“å­˜ç›´æ¥åˆ¤å®šï¼Œæ— éœ€é‡æ–°æµ·é€‰
        
        Args:
            tensor_5d: è¾“å…¥å…«å­—çš„ 5D å¼ é‡
            threshold: å…¥æ ¼é˜ˆå€¼
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        import time
        start = time.perf_counter()
        
        # æŒ‡çº¹æ¯”å¯¹
        matches = self.fingerprint_match(tensor_5d, top_k=3)
        
        if not matches:
            return {
                "success": False,
                "reason": "ç¼“å­˜ä¸ºç©º",
                "latency_ms": (time.perf_counter() - start) * 1000
            }
        
        # å–æœ€ä½³åŒ¹é…
        best = matches[0]
        
        # åˆ¤å®š
        if best["mahalanobis_distance"] < threshold:
            verdict = "STANDARD_MATCH"
            confidence = min(0.95, best["cosine_similarity"])
        elif best["mahalanobis_distance"] < threshold * 1.5:
            verdict = "MARGINAL_MATCH"
            confidence = 0.6
        else:
            verdict = "NO_MATCH"
            confidence = 0.3
        
        latency = (time.perf_counter() - start) * 1000
        
        return {
            "success": True,
            "best_match": best,
            "all_matches": matches,
            "verdict": verdict,
            "confidence": confidence,
            "latency_ms": latency,
            "cache_hit": True
        }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_samples = sum(c["sample_count"] for c in self._memory_cache.values())
        return {
            "cached_patterns": len(self._memory_cache),
            "total_samples": total_samples,
            "patterns": list(self._memory_cache.keys())
        }


# å…¨å±€å•ä¾‹
_census_cache: Optional[CensusCache] = None


def get_census_cache() -> CensusCache:
    """è·å– CensusCache å•ä¾‹"""
    global _census_cache
    if _census_cache is None:
        _census_cache = CensusCache()
    return _census_cache


# ============================================================
# é¢„æµ‹æµç¨‹æ•´åˆ
# ============================================================

class FastPredictor:
    """
    å¿«é€Ÿé¢„æµ‹å™¨
    
    æ•´åˆ LKV + FDS + Cache çš„å®Œæ•´é¢„æµ‹æµç¨‹
    
    è·¯å¾„ç­–ç•¥ï¼š
    - Green Path (D_M < 2.0): ä¿¡ä»»ç¼“å­˜ï¼Œç§’å‡ºæŠ¥å‘Š
    - Yellow Path (2.0 <= D_M < 3.5): ç¼“å­˜ + LKV æ·±åº¦å®¡è®¡
    - Red Path (D_M >= 3.5): å¯åŠ¨å¥‡ç‚¹æº¯æº
    """
    
    # äºŒæ¬¡ç©¿é€é˜ˆå€¼
    GREEN_THRESHOLD = 2.0   # ä¿¡ä»»ç¼“å­˜
    YELLOW_THRESHOLD = 3.5  # æ·±åº¦å®¡è®¡
    
    def __init__(self):
        self.cache = get_census_cache()
        self._protocol_checker = None
        self._report_generator = None
        self._vault_manager = None
    
    def predict(
        self, 
        bazi: Dict,
        tensor_5d: List[float],
        use_cache: bool = True,
        generate_report: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œæ•´é¢„æµ‹æµç¨‹ï¼ˆå¸¦è¯­ä¹‰å‹‹ç« ï¼‰
        """
        import time
        start = time.perf_counter()
        
        if use_cache:
            cache_result = self.cache.instant_predict(tensor_5d)
            
            if cache_result["success"]:
                best = cache_result["best_match"]
                m_dist = best["mahalanobis_distance"]
                
                # è·¯å¾„åˆ¤å®š
                if m_dist < self.GREEN_THRESHOLD:
                    path = "GREEN"
                elif m_dist < self.YELLOW_THRESHOLD:
                    path = "YELLOW"
                else:
                    path = "RED"
                
                # é€»è¾‘å®¡è®¡
                logic_result = self._check_logic(bazi, best["pattern_id"])
                dual_match = logic_result["passed"] and cache_result["verdict"] == "STANDARD_MATCH"
                
                result = {
                    "method": "CACHE_HIT",
                    "path": path,
                    "latency_ms": (time.perf_counter() - start) * 1000,
                    "pattern_id": best["pattern_id"],
                    "pattern_name": best["pattern_name"],
                    "mahalanobis_distance": m_dist,
                    "physics_verdict": cache_result["verdict"],
                    "logic_verdict": "PASSED" if logic_result["passed"] else "FAILED",
                    "confidence": cache_result["confidence"],
                    "dual_match": dual_match
                }
                
                if generate_report:
                    result["report"] = self._generate_report(tensor_5d, best, logic_result, path)
                
                return result
        
        return self._full_predict(bazi, tensor_5d, generate_report)
    
    def _check_logic(self, bazi: Dict, pattern_id: str) -> Dict:
        if self._protocol_checker is None:
            from core.protocol_checker import get_protocol_checker
            self._protocol_checker = get_protocol_checker()
        return self._protocol_checker.check_bazi(bazi, pattern_id)
    
    def _generate_report(self, tensor: List[float], match: Dict, logic: Dict, path: str) -> str:
        E, O, M, S, R = tensor
        path_desc = {"GREEN": "âœ… æ ‡å‡†åŒ¹é…", "YELLOW": "âš ï¸ è¾¹ç¼˜æ€", "RED": "ğŸ”´ å¼‚å¸¸æ€"}.get(path, "?")
        
        return f"""ã€QGA-VV å®¡è®¡æŠ¥å‘Šã€‘
æ ¼å±€: {match['pattern_id']} ({match['pattern_name']})
åˆ¤å®š: {path_desc} ({path} Path)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ç‰©ç†æ€ã€‘
E={E:.3f} | O={O:.3f} | M={M:.3f} | S={S:.3f} | R={R:.3f}
é©¬æ°è·ç¦»: {match['mahalanobis_distance']:.4f}

ã€é€»è¾‘å®¡è®¡ã€‘
çŠ¶æ€: {'âœ… é€šè¿‡' if logic['passed'] else 'âŒ æœªé€šè¿‡'}
{''.join(f'  {d}' + chr(10) for d in logic.get('details', [])[:2])}
ã€åŒè½¨éªŒè¯ã€‘
{'âœ… é€šè¿‡' if logic['passed'] else 'âš ï¸ ä»…ç‰©ç†å‘½ä¸­'}
ç½®ä¿¡åº¦: {match.get('cosine_similarity', 0):.2f}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
    
    def _full_predict(self, bazi: Dict, tensor_5d: List[float], generate_report: bool) -> Dict:
        return {"method": "FULL_COMPUTE", "message": "éœ€è¦æ‰§è¡Œå®Œæ•´é¢„æµ‹æµç¨‹"}


def get_fast_predictor() -> FastPredictor:
    return FastPredictor()

