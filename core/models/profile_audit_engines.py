"""
å…«å­—æ¡£æ¡ˆå®¡è®¡æ ¸å¿ƒå¼•æ“
å®ç°PFAã€SOAã€MCAä¸‰ä¸ªæ ¸å¿ƒç®—æ³•
"""

import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

from core.bazi_profile import BaziProfile
from core.engine_graph import GraphNetworkEngine
from core.trinity.core.engines.pattern_scout import PatternScout
from core.logic_registry import LogicRegistry
from core.config_schema import DEFAULT_FULL_ALGO_PARAMS

logger = logging.getLogger(__name__)


@dataclass
class PatternFrictionResult:
    """æ ¼å±€å†²çªåˆ†æç»“æœ"""
    friction_index: float  # 0-100ï¼Œè¶Šé«˜è¡¨ç¤ºå†²çªè¶Šå¤§
    conflicting_patterns: List[str]  # å†²çªçš„æ ¼å±€åˆ—è¡¨
    coherence_level: str  # "é«˜" / "ä¸­" / "ä½"
    semantic_interpretation: str  # è¯­ä¹‰è§£é‡Š
    detected_patterns: List[Dict] = None  # æ£€æµ‹åˆ°çš„æ‰€æœ‰æ ¼å±€ï¼ˆç”¨äºè¯¦ç»†åˆ†æï¼‰


@dataclass
class OptimizationResult:
    """å˜åˆ†å¯»ä¼˜ç»“æœ"""
    optimal_elements: Dict[str, float]  # æœ€ä¼˜äº”è¡Œæ³¨å…¥é‡
    stability_score: float  # ç¨³å®šæ€§åˆ†æ•°
    entropy_reduction: float  # ç†µå€¼é™ä½
    semantic_interpretation: str  # è¯­ä¹‰è§£é‡Š


@dataclass
class MediumCompensationResult:
    """ä»‹è´¨ä¿®æ­£ç»“æœ"""
    geo_correction: Dict[str, float]  # åœ°ç†ä¿®æ­£ç³»æ•°
    micro_env_correction: Dict[str, float]  # å¾®ç¯å¢ƒä¿®æ­£ç³»æ•°
    total_correction: Dict[str, float]  # æ€»ä¿®æ­£ç³»æ•°
    semantic_interpretation: str  # è¯­ä¹‰è§£é‡Š


class PatternFrictionAnalysisEngine:
    """
    [P.F.A] æ ¼å±€å†²çªæ˜ å°„å¼•æ“
    æ£€æµ‹å‘½å±€ä¸­ä¸åŒæ ¼å±€å¸å¼•å­çš„"ç›¸ä½å¹²æ‰°"
    éå†ç‰©ç†æ¨¡å‹ä»¿çœŸä¸»é¢˜ä¸‹æ‰€æœ‰æ³¨å†Œçš„æ ¼å±€ä¸“é¢˜
    """
    
    def __init__(self):
        self.registry = LogicRegistry()
        self.scout = PatternScout()
        
        # æ ¼å±€å†²çªè§„åˆ™è¡¨
        self.conflict_rules = {
            # åŒ–æ°”æ ¼è§ä¼¤å®˜ -> ç›¸å¹²æ€§é™ä½
            ("åŒ–æ°”æ ¼", "ä¼¤å®˜"): 0.6,
            # ä»æ ¼è§æ¯”åŠ« -> çº¯åº¦ä¸‹é™
            ("ä»æ ¼", "æ¯”åŠ«"): 0.5,
            # ä¸“æ—ºè§è´¢æ˜Ÿ -> æ ¼å±€ç ´å
            ("ä¸“æ—º", "è´¢æ˜Ÿ"): 0.4,
            # æ­£å®˜æ ¼è§ä¼¤å®˜ -> å†²çª
            ("æ­£å®˜æ ¼", "ä¼¤å®˜"): 0.7,
        }
        
        # é¢„åŠ è½½æ‰€æœ‰PATTERN_PHYSICSä¸»é¢˜ä¸‹çš„æ ¼å±€
        self._load_pattern_physics_topics()
    
    def _load_pattern_physics_topics(self):
        """åŠ è½½ç‰©ç†æ¨¡å‹ä»¿çœŸä¸»é¢˜ä¸‹çš„æ‰€æœ‰æ ¼å±€ä¸“é¢˜"""
        self.pattern_physics_topics = self.registry.get_active_modules(theme_id="PATTERN_PHYSICS")
        logger.info(f"åŠ è½½äº† {len(self.pattern_physics_topics)} ä¸ªç‰©ç†æ¨¡å‹ä»¿çœŸæ ¼å±€ä¸“é¢˜")
    
    def analyze(self, bazi_profile: BaziProfile, year: int = None, 
                geo_element: str = None, geo_factor: float = 1.0) -> PatternFrictionResult:
        """
        åˆ†ææ ¼å±€å†²çª
        
        Args:
            bazi_profile: å…«å­—æ¡£æ¡ˆå¯¹è±¡
            year: æµå¹´ï¼ˆå¯é€‰ï¼‰
            geo_element: åœ°ç†äº”è¡Œå±æ€§ï¼ˆå¯é€‰ï¼‰
            geo_factor: åœ°ç†å› å­ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ ¼å±€å†²çªåˆ†æç»“æœ
        """
        # 1. è·å–æ‰€æœ‰æ ¼å±€
        pillars = bazi_profile.pillars
        chart = [
            (pillars['year'][0], pillars['year'][1]),
            (pillars['month'][0], pillars['month'][1]),
            (pillars['day'][0], pillars['day'][1]),
            (pillars['hour'][0], pillars['hour'][1])
        ]
        
        # 2. è·å–å¤§è¿å’Œæµå¹´ï¼ˆç”¨äºæ ¼å±€æ¢æµ‹ï¼‰
        luck_pillar = bazi_profile.get_luck_pillar_at(year) if year else None
        year_pillar = bazi_profile.get_year_pillar(year) if year else None
        
        # æ„å»ºgeo_contextç”¨äºæ ¼å±€æ¢æµ‹ï¼ˆæ€»çº¿æ³¨å…¥æ–¹å¼ï¼‰
        geo_context = {}
        if luck_pillar:
            geo_context['luck_pillar'] = luck_pillar
        if year_pillar:
            geo_context['annual_pillar'] = year_pillar
        if geo_element:
            geo_context['element'] = geo_element
        if geo_factor != 1.0:
            geo_context['factor'] = geo_factor
        
        # 3. éå†æ‰€æœ‰PATTERN_PHYSICSä¸»é¢˜ä¸‹çš„æ ¼å±€ä¸“é¢˜
        detected_patterns = []
        
        logger.info(f"å¼€å§‹éå† {len(self.pattern_physics_topics)} ä¸ªæ ¼å±€ä¸“é¢˜è¿›è¡Œå†²çªåˆ†æ...")
        
        for topic in self.pattern_physics_topics:
            topic_id = topic.get('id', '')
            topic_name = topic.get('name_cn') or topic.get('name', topic_id)
            
            # åªå¤„ç†activeçš„æ ¼å±€
            if not topic.get('active', True):
                continue
            
            try:
                # è§£æé€»è¾‘ID
                registry_id, logic_ids = self.registry.resolve_logic_id(topic_id)
                
                # å¯¹æ¯ä¸ªé€»è¾‘IDè¿›è¡Œæ¢æµ‹
                for logic_id in logic_ids:
                    match = self.scout._deep_audit(chart, logic_id, geo_context=geo_context)
                    if match:
                        detected_patterns.append({
                            'id': topic_id,
                            'logic_id': logic_id,
                            'name': topic_name,
                            'category': match.get('category', ''),
                            'stress': match.get('stress', 0.0),
                            'sai': match.get('sai', 0.0),
                            'match_data': match
                        })
                        logger.debug(f"æ£€æµ‹åˆ°æ ¼å±€: {topic_name} ({topic_id})")
                        # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±è·³å‡ºï¼ˆé¿å…é‡å¤ï¼‰
                        break
            except Exception as e:
                logger.debug(f"æ¢æµ‹æ ¼å±€ {topic_id} å¤±è´¥: {e}")
                continue
        
        # [QGA V24.0] æ¨¡å¼ä¼˜å…ˆé©±åŠ¨ï¼šå…ˆæ•è·å…¸å‹å±€ï¼ˆç‰¹æ®Šæ ¼å±€ï¼‰
        special_pattern = self._capture_special_patterns(detected_patterns, chart, bazi_profile)
        
        # åˆå§‹åŒ–prioritized_patternsï¼ˆç¡®ä¿åœ¨æ‰€æœ‰åˆ†æ”¯ä¸­éƒ½æœ‰å®šä¹‰ï¼‰
        prioritized_patterns = {}
        
        # å¦‚æœæ•è·åˆ°ç‰¹æ®Šæ ¼å±€ï¼Œé€»è¾‘é”æ­»
        if special_pattern:
            logger.info(f"ğŸ”’ æ£€æµ‹åˆ°ç‰¹æ®Šæ ¼å±€: {special_pattern['name']}ï¼Œé€»è¾‘å·²é”æ­»")
            self._special_pattern_locked = special_pattern
            # ç‰¹æ®Šæ ¼å±€ä¸‹ï¼Œå†²çªæŒ‡æ•°è®¾ä¸º0ï¼ˆå› ä¸ºæ ¼å±€æœ¬èº«æ˜¯è¶…ç¨³æ€ç»“æ„ï¼‰
            friction_index = 0.0
            conflicting_pairs = []
            coherence_level = "è¶…ç¨³æ€"
            # å³ä½¿æœ‰ç‰¹æ®Šæ ¼å±€ï¼Œä¹Ÿå»ºç«‹ä¼˜å…ˆçº§ç»“æ„ï¼ˆç”¨äºåç»­åˆ†æï¼‰
            prioritized_patterns = {
                'primary': {
                    'name': special_pattern['name'],
                    'type': special_pattern['type'],
                    'pattern': special_pattern
                },
                'conflicts': [],
                'singularities': []
            }
        else:
            # [QGA V23.5] æ ¼å±€ä¼˜å…ˆçº§æ¶æ„ï¼šå»ºç«‹å±‚æ¬¡ç»“æ„
            prioritized_patterns = self._prioritize_patterns(detected_patterns, chart, bazi_profile)
            self._special_pattern_locked = None
            logger.info(f"å…±æ£€æµ‹åˆ° {len(detected_patterns)} ä¸ªæ ¼å±€ï¼Œä¸»æ ¼å±€: {prioritized_patterns.get('primary', {}).get('name', 'æ— ')}")
        
        # ä¿å­˜æ ¼å±€ä¿¡æ¯ï¼ˆç”¨äºåç»­åˆ†æï¼‰
        self._prioritized_patterns = prioritized_patterns
        self._last_detected_patterns = detected_patterns
        
        # 3. è®¡ç®—å†²çªæŒ‡æ•°ï¼ˆåŸºäºä¼˜å…ˆçº§æƒé‡ï¼‰
        friction_index = 0.0
        conflicting_pairs = []
        
        primary_pattern = prioritized_patterns.get('primary')
        conflict_patterns = prioritized_patterns.get('conflicts', [])
        
        # [QGA V23.5] å¦‚æœå­˜åœ¨ç›¸ä½å†²çªæ ¼å±€ï¼Œè¿™æ˜¯æœ€å¤§åº”åŠ›ç‚¹
        if conflict_patterns:
            for cp in conflict_patterns:
                if primary_pattern:
                    # ä¸»æ ¼å±€ä¸å†²çªæ ¼å±€çš„å†²çªï¼ˆæƒé‡60%ï¼‰
                    conflict_score = self._check_pattern_conflict(primary_pattern, cp, chart, bazi_profile.day_master)
                    if conflict_score > 0.3:
                        friction_index += conflict_score * 0.6
                        conflicting_pairs.append(f"{primary_pattern['name']} vs {cp['name']}")
                
                # å†²çªæ ¼å±€ä¹‹é—´çš„å†²çªï¼ˆæƒé‡20%ï¼‰
                for cp2 in conflict_patterns:
                    if cp['id'] != cp2['id']:
                        conflict_score = self._check_pattern_conflict(cp, cp2, chart, bazi_profile.day_master)
                        if conflict_score > 0.3:
                            friction_index += conflict_score * 0.2
                            if f"{cp['name']} vs {cp2['name']}" not in conflicting_pairs:
                                conflicting_pairs.append(f"{cp['name']} vs {cp2['name']}")
        else:
            # ä¼ ç»Ÿå†²çªæ£€æµ‹ï¼ˆé™æƒåˆ°10%ï¼‰
            for i, p1 in enumerate(detected_patterns):
                for j, p2 in enumerate(detected_patterns[i+1:], i+1):
                    conflict_score = self._check_pattern_conflict(p1, p2, chart, bazi_profile.day_master)
                    if conflict_score > 0.3:
                        friction_index += conflict_score * 0.1
                        conflicting_pairs.append(f"{p1['name']} vs {p2['name']}")
        
        # å½’ä¸€åŒ–åˆ°0-100ï¼ˆç‰¹æ®Šæ ¼å±€å·²é”æ­»ï¼Œè·³è¿‡æ­¤æ­¥éª¤ï¼‰
        if not special_pattern:
            friction_index = min(100.0, friction_index * 20.0)
            
            # 4. ç¡®å®šç›¸å¹²æ€§ç­‰çº§
            if friction_index < 30:
                coherence_level = "é«˜"
            elif friction_index < 60:
                coherence_level = "ä¸­"
            else:
                coherence_level = "ä½"
        else:
            # ç‰¹æ®Šæ ¼å±€ï¼šè¶…ç¨³æ€ç»“æ„
            coherence_level = "è¶…ç¨³æ€"
        
        # 5. ç”Ÿæˆè¯­ä¹‰è§£é‡Š
        if special_pattern:
            semantic = self._generate_special_pattern_semantic(special_pattern)
        else:
            semantic = self._generate_friction_semantic(friction_index, conflicting_pairs, coherence_level)
        
        return PatternFrictionResult(
            friction_index=friction_index,
            conflicting_patterns=conflicting_pairs,
            coherence_level=coherence_level,
            semantic_interpretation=semantic,
            detected_patterns=detected_patterns
        )
    
    def get_detected_patterns(self) -> List[Dict]:
        """è·å–æœ€è¿‘ä¸€æ¬¡åˆ†æä¸­æ£€æµ‹åˆ°çš„æ‰€æœ‰æ ¼å±€ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        return getattr(self, '_last_detected_patterns', [])
    
    def _prioritize_patterns(self, detected_patterns: List[Dict], chart: List, 
                            bazi_profile: BaziProfile) -> Dict[str, Any]:
        """
        [QGA V23.5] æ ¼å±€ä¼˜å…ˆçº§æ¶æ„
        ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šæœˆä»¤æ ¼ç¥ï¼ˆ60%æƒé‡ï¼‰
        ç¬¬äºŒä¼˜å…ˆçº§ï¼šç›¸ä½å†²çªï¼ˆ20%æƒé‡ï¼‰
        ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šç©ºé—´å¥‡ç‚¹ï¼ˆ10%æƒé‡ï¼‰
        """
        from core.trinity.core.nexus.definitions import BaziParticleNexus
        
        result = {
            'primary': None,      # ä¸»æ ¼å±€ï¼ˆæœˆä»¤æ ¼ç¥ï¼‰
            'conflicts': [],      # ç›¸ä½å†²çªæ ¼å±€
            'singularities': []   # ç©ºé—´å¥‡ç‚¹æ ¼å±€
        }
        
        month_pillar = chart[1]  # æœˆæŸ±
        month_branch = month_pillar[1]  # æœˆæ”¯
        day_master = bazi_profile.day_master
        
        # è¯†åˆ«æœˆä»¤æ ¼ç¥ï¼ˆåŸºäºæœˆæ”¯è—å¹²å’Œæ—¥ä¸»çš„å…³ç³»ï¼‰
        month_hidden = BaziParticleNexus.get_branch_weights(month_branch)
        month_ten_gods = []
        for stem, weight in month_hidden:
            ten_god = BaziParticleNexus.get_shi_shen(stem, day_master)
            month_ten_gods.append((ten_god, weight))
        
        # æŸ¥æ‰¾ä¸æœˆä»¤ç›¸å…³çš„æ ¼å±€ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        for pattern in detected_patterns:
            pattern_name = pattern.get('name', '').lower()
            pattern_id = pattern.get('id', '').lower()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœˆä»¤ç›¸å…³æ ¼å±€
            is_yue_ling_pattern = False
            if 'ä¼¤å®˜' in pattern_name or 'shang_guan' in pattern_id:
                if any('ä¼¤å®˜' in tg[0] for tg in month_ten_gods):
                    is_yue_ling_pattern = True
            elif 'æ­£å®˜' in pattern_name or 'zheng_guan' in pattern_id:
                if any('æ­£å®˜' in tg[0] for tg in month_ten_gods):
                    is_yue_ling_pattern = True
            elif 'æ­£è´¢' in pattern_name or 'zheng_cai' in pattern_id:
                if any('æ­£è´¢' in tg[0] for tg in month_ten_gods):
                    is_yue_ling_pattern = True
            elif 'åè´¢' in pattern_name or 'pian_cai' in pattern_id:
                if any('åè´¢' in tg[0] for tg in month_ten_gods):
                    is_yue_ling_pattern = True
            elif 'æ­£å°' in pattern_name or 'zheng_yin' in pattern_id:
                if any('æ­£å°' in tg[0] for tg in month_ten_gods):
                    is_yue_ling_pattern = True
            
            if is_yue_ling_pattern and not result['primary']:
                result['primary'] = pattern
                pattern['priority'] = 'primary'
                pattern['weight'] = 0.6
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸ä½å†²çªæ ¼å±€ï¼ˆå¦‚ä¼¤å®˜è§å®˜ï¼‰
            if 'è§' in pattern_name or 'jian' in pattern_id or 'conflict' in pattern_id:
                result['conflicts'].append(pattern)
                pattern['priority'] = 'conflict'
                pattern['weight'] = 0.2
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç©ºé—´å¥‡ç‚¹ï¼ˆæ‹±å¤¹ã€å¢“åº“ï¼‰
            if 'æ‹±' in pattern_name or 'å¢“' in pattern_name or 'åº“' in pattern_name or \
               'gong' in pattern_id or 'mu' in pattern_id or 'ku' in pattern_id:
                result['singularities'].append(pattern)
                pattern['priority'] = 'singularity'
                pattern['weight'] = 0.1
                continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœˆä»¤æ ¼ç¥ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªæ ¼å±€ä½œä¸ºä¸»æ ¼å±€ï¼ˆé™æƒï¼‰
        if not result['primary'] and detected_patterns:
            result['primary'] = detected_patterns[0]
            result['primary']['priority'] = 'primary'
            result['primary']['weight'] = 0.4  # é™æƒ
        
        return result
    
    def _check_pattern_conflict(self, p1: Dict, p2: Dict, chart: List, day_master: str) -> float:
        """æ£€æŸ¥ä¸¤ä¸ªæ ¼å±€ä¹‹é—´çš„å†²çªç¨‹åº¦"""
        # ç®€åŒ–ç‰ˆå†²çªæ£€æµ‹
        conflict_score = 0.0
        
        # æ£€æŸ¥åç¥å†²çª
        from core.trinity.core.nexus.definitions import BaziParticleNexus
        stems = [p[0] for p in chart]
        ten_gods = [BaziParticleNexus.get_shi_shen(s, day_master) for s in stems]
        
        # å¦‚æœæ ¼å±€1éœ€è¦æŸç§åç¥ï¼Œè€Œæ ¼å±€2ç ´åå®ƒ
        if "ä¼¤å®˜" in ten_gods and "æ­£å®˜" in ten_gods:
            conflict_score += 0.5
        
        return conflict_score
    
    def _capture_special_patterns(self, detected_patterns: List[Dict], chart: List,
                                 bazi_profile: BaziProfile) -> Optional[Dict]:
        """
        [QGA V24.0] å…¸å‹å±€æ•è·å™¨
        è¯†åˆ«ç‰¹æ®Šæ ¼å±€ï¼šä¼¤å®˜ä¼¤å°½ã€ä»è´¢æ ¼ã€åŒ–æ°”æ ¼ã€ç¾Šåˆƒé©¾æ€ç­‰
        ä¸€æ—¦è¯†åˆ«ï¼Œé€»è¾‘é”æ­»ï¼Œåç»­è®¡ç®—å¿…é¡»æœä»è¯¥æ ¼å±€çš„"æ ‡å‡†ç­”æ¡ˆ"
        """
        from core.trinity.core.nexus.definitions import BaziParticleNexus
        from typing import Optional
        
        day_master = bazi_profile.day_master
        stems = [p[0] for p in chart]
        branches = [p[1] for p in chart]
        ten_gods = [BaziParticleNexus.get_shi_shen(s, day_master) for s in stems]
        
        # 1. ä¼¤å®˜ä¼¤å°½ï¼ˆShang Guan Shang Jinï¼‰
        # åˆ¤å®šï¼šä¼¤å®˜å¤šä¸”æ— å®˜æ˜Ÿï¼Œæˆ–å®˜æ˜Ÿè¢«å®Œå…¨åˆ¶åŒ–
        shang_guan_count = ten_gods.count('ä¼¤å®˜')
        zheng_guan_count = ten_gods.count('æ­£å®˜')
        qi_sha_count = ten_gods.count('ä¸ƒæ€')
        
        if shang_guan_count >= 2 and (zheng_guan_count == 0 and qi_sha_count == 0):
            # æ£€æŸ¥æ˜¯å¦æœ‰å®˜æ˜Ÿè¢«åˆåŒ–æˆ–å†²æ‰
            for pattern in detected_patterns:
                pattern_name = pattern.get('name', '').lower()
                if 'ä¼¤å®˜' in pattern_name and ('å°½' in pattern_name or 'shang_jin' in pattern.get('id', '').lower()):
                    return {
                        'type': 'shang_guan_shang_jin',
                        'name': 'ä¼¤å®˜ä¼¤å°½',
                        'pattern': pattern,
                        'yong_shen_rule': 'shang_guan_or_wealth',  # ç”¨ç¥ï¼šä¼¤å®˜æˆ–è¡Œè´¢è¿
                        'life_theme': 'æ‰åæ¨ªæº¢ï¼Œä¸å—çº¦æŸï¼Œé€‚åˆè‡ªç”±èŒä¸šæˆ–è‰ºæœ¯åˆ›ä½œ'
                    }
        
        # 2. ä»è´¢æ ¼ï¼ˆFrom-Wealth Patternï¼‰
        # åˆ¤å®šï¼šè´¢æ˜Ÿææ—ºï¼Œæ—¥ä¸»æå¼±ï¼Œä»è´¢
        cai_count = ten_gods.count('æ­£è´¢') + ten_gods.count('åè´¢')
        if cai_count >= 3:
            # æ£€æŸ¥æ—¥ä¸»æ˜¯å¦æå¼±ï¼ˆæ— å°æ¯”æ”¯æ’‘ï¼‰
            yin_count = ten_gods.count('æ­£å°') + ten_gods.count('åå°')
            bi_jie_count = ten_gods.count('æ¯”è‚©') + ten_gods.count('åŠ«è´¢')
            if yin_count == 0 and bi_jie_count <= 1:
                return {
                    'type': 'from_wealth',
                    'name': 'ä»è´¢æ ¼',
                    'pattern': None,
                    'yong_shen_rule': 'wealth',  # ç”¨ç¥ï¼šè´¢æ˜Ÿ
                    'life_theme': 'ä»¥è´¢ä¸ºç”¨ï¼Œå–„äºç»è¥ï¼Œè´¢å¯Œæ˜¯äººç”Ÿæ ¸å¿ƒè¿½æ±‚'
                }
        
        # 3. åŒ–æ°”æ ¼ï¼ˆTransformation Patternï¼‰
        # åˆ¤å®šï¼šå¤©å¹²äº”åˆä¸”åˆåŒ–æˆåŠŸ
        for pattern in detected_patterns:
            pattern_name = pattern.get('name', '').lower()
            pattern_id = pattern.get('id', '').lower()
            if 'åŒ–æ°”' in pattern_name or 'hua_qi' in pattern_id or 'transform' in pattern_id:
                match_data = pattern.get('match_data', {})
                if match_data.get('transform', False):  # åˆåŒ–æˆåŠŸ
                    return {
                        'type': 'transformation',
                        'name': pattern.get('name', 'åŒ–æ°”æ ¼'),
                        'pattern': pattern,
                        'yong_shen_rule': 'transformed_element',  # ç”¨ç¥ï¼šåˆåŒ–åçš„å…ƒç´ 
                        'life_theme': 'æ€§æ ¼è½¬åŒ–ï¼Œå…·æœ‰åŒé‡ç‰¹è´¨ï¼Œäººç”Ÿå¤šå˜'
                    }
        
        # 4. ç¾Šåˆƒé©¾æ€ï¼ˆYang Ren Jia Shaï¼‰
        # åˆ¤å®šï¼šç¾Šåˆƒä¸ä¸ƒæ€åŒæ—¶å‡ºç°ä¸”åŠ›é‡ç›¸å½“
        day_master_yang_ren = {
            'ç”²': 'å¯', 'ä¹™': 'å¯…',
            'ä¸™': 'åˆ', 'ä¸': 'å·³',
            'æˆŠ': 'åˆ', 'å·±': 'å·³',
            'åºš': 'é…‰', 'è¾›': 'ç”³',
            'å£¬': 'å­', 'ç™¸': 'äº¥'
        }
        yang_ren = day_master_yang_ren.get(day_master)
        
        if yang_ren and yang_ren in branches and qi_sha_count >= 1:
            return {
                'type': 'yang_ren_jia_sha',
                'name': 'ç¾Šåˆƒé©¾æ€',
                'pattern': None,
                'yong_shen_rule': 'sha_or_yin',  # ç”¨ç¥ï¼šä¸ƒæ€æˆ–å°æ˜Ÿ
                'life_theme': 'åˆšå¼ºæœæ–­ï¼Œæœ‰é¢†å¯¼åŠ›ï¼Œä½†æ˜“å†²åŠ¨ï¼Œéœ€è¦åˆ¶è¡¡'
            }
        
        # 5. è¶…å¯¼ä½“/å¥‡ç‚¹æ ¼å±€ï¼ˆSuperconductor/Singularityï¼‰
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜SAIã€é«˜çº¯åº¦çš„æ ¼å±€
        for pattern in detected_patterns:
            pattern_id = pattern.get('id', '').lower()
            sai = pattern.get('sai', 0.0)
            # ç¡®ä¿saiæ˜¯æ•°å€¼ç±»å‹
            try:
                sai_float = float(sai) if sai is not None else 0.0
            except (ValueError, TypeError):
                sai_float = 0.0
            
            if 'superconductor' in pattern_id or 'singularity' in pattern_id or sai_float > 0.9:
                return {
                    'type': 'superconductor',
                    'name': pattern.get('name', 'è¶…å¯¼ä½“æ ¼å±€'),
                    'pattern': pattern,
                    'yong_shen_rule': 'maintain_purity',  # ç”¨ç¥ï¼šç»´æŒçº¯åº¦
                    'life_theme': 'çº¯ç²¹ä¸ç§©åºï¼Œè¿½æ±‚å®Œç¾ï¼Œå…·æœ‰è¶…å¸¸çš„ä¸“æ³¨åŠ›'
                }
        
        return None
    
    def _generate_special_pattern_semantic(self, special_pattern: Dict) -> str:
        """ç”Ÿæˆç‰¹æ®Šæ ¼å±€çš„è¯­ä¹‰è§£é‡Š"""
        pattern_type = special_pattern.get('type')
        pattern_name = special_pattern.get('name')
        life_theme = special_pattern.get('life_theme', '')
        
        if pattern_type == 'shang_guan_shang_jin':
            return f"å‘½å±€å‘ˆç°**{pattern_name}**æ ¼å±€ï¼Œè¿™æ˜¯è¶…ç¨³æ€ç»“æ„ã€‚{life_theme}ã€‚ç”¨ç¥ç›´æ¥é”å®šä¼¤å®˜æˆ–è¡Œè´¢è¿ï¼Œä¸å†è€ƒè™‘èº«å¼ºèº«å¼±çš„å¹³è¡¡ã€‚"
        elif pattern_type == 'from_wealth':
            return f"å‘½å±€å‘ˆç°**{pattern_name}**æ ¼å±€ï¼Œè¿™æ˜¯è¶…ç¨³æ€ç»“æ„ã€‚{life_theme}ã€‚ç”¨ç¥ç›´æ¥é”å®šè´¢æ˜Ÿï¼Œä»¥è´¢ä¸ºç”¨ã€‚"
        elif pattern_type == 'transformation':
            return f"å‘½å±€å‘ˆç°**{pattern_name}**æ ¼å±€ï¼Œè¿™æ˜¯è¶…ç¨³æ€ç»“æ„ã€‚{life_theme}ã€‚ç”¨ç¥ä¸ºåˆåŒ–åçš„å…ƒç´ ã€‚"
        elif pattern_type == 'yang_ren_jia_sha':
            return f"å‘½å±€å‘ˆç°**{pattern_name}**æ ¼å±€ï¼Œè¿™æ˜¯è¶…ç¨³æ€ç»“æ„ã€‚{life_theme}ã€‚ç”¨ç¥ä¸ºä¸ƒæ€æˆ–å°æ˜Ÿã€‚"
        elif pattern_type == 'superconductor':
            return f"å‘½å±€å‘ˆç°**{pattern_name}**æ ¼å±€ï¼Œè¿™æ˜¯è¶…ç¨³æ€ç»“æ„ã€‚{life_theme}ã€‚ç”¨ç¥ä¸ºç»´æŒçº¯åº¦ï¼Œé¿å…æ‚è´¨å¹²æ‰°ã€‚"
        else:
            return f"å‘½å±€å‘ˆç°**{pattern_name}**æ ¼å±€ï¼Œè¿™æ˜¯è¶…ç¨³æ€ç»“æ„ã€‚{life_theme}ã€‚"
    
    def _generate_friction_semantic(self, friction: float, conflicts: List[str], coherence: str) -> str:
        """ç”Ÿæˆè¯­ä¹‰è§£é‡Š"""
        if friction < 30:
            return "æ ¼å±€ä½“ç³»é«˜åº¦åè°ƒï¼Œå„æ ¼å±€åŠ›é‡ç›¸äº’æ”¯æ’‘ï¼Œå½¢æˆç¨³å®šçš„èƒ½é‡åœºã€‚"
        elif friction < 60:
            if conflicts:
                return f"å‘½å±€ä¸­å­˜åœ¨ä¸€å®šçš„æ ¼å±€å†²çªï¼ˆ{', '.join(conflicts[:2])}ï¼‰ï¼Œå¯¼è‡´ç†æƒ³ä¸ç°å®ä¹‹é—´å­˜åœ¨å¼ åŠ›ï¼Œéœ€è¦è°ƒå’Œã€‚"
            else:
                return "æ ¼å±€ä½“ç³»åŸºæœ¬åè°ƒï¼Œä½†å­˜åœ¨å¾®å¦™çš„ç›¸ä½å¹²æ‰°ï¼Œéœ€è¦å…³æ³¨å†…åœ¨å¹³è¡¡ã€‚"
        else:
            if conflicts:
                return f"å‘½å±€ä¸­å­˜åœ¨ä¸¥é‡çš„æ ¼å±€å†²çªï¼ˆ{', '.join(conflicts[:2])}ï¼‰ï¼Œå¯¼è‡´æ€§æ ¼ä¸­çš„è‡ªæˆ‘æ‹†å°ï¼Œç†æƒ³ä¸ç°å®çš„æ’•è£‚æ„Ÿå¼ºçƒˆï¼Œéœ€è¦å¯»æ‰¾å¹³è¡¡ç‚¹ã€‚"
            else:
                return "æ ¼å±€ä½“ç³»å­˜åœ¨æ˜¾è‘—å†²çªï¼Œèƒ½é‡åœºä¸ç¨³å®šï¼Œéœ€è¦å¤–éƒ¨å¹²é¢„æ¥è°ƒå’ŒçŸ›ç›¾ã€‚"


class SystemOptimizationEngine:
    """
    [S.O.A] å˜åˆ†å¯»ä¼˜ç®—æ³•å¼•æ“
    åœ¨åå°æ¨¡æ‹Ÿæ³¨å…¥é‡‘æœ¨æ°´ç«åœŸ5ç§å› å­ï¼Œå¯»æ‰¾èƒ½è®©ç³»ç»Ÿ"ç†µå€¼"æœ€å°ã€ç¨³å®šæ€§æœ€é«˜çš„ç»„åˆ
    """
    
    def __init__(self):
        self.step_size = 0.05
        self.elements = ['metal', 'wood', 'water', 'fire', 'earth']
        self.element_cn = {
            'metal': 'é‡‘', 'wood': 'æœ¨', 'water': 'æ°´', 'fire': 'ç«', 'earth': 'åœŸ'
        }
    
    def optimize(self, bazi_profile: BaziProfile, year: int = None,
                 geo_element: str = None, geo_factor: float = 1.0,
                 primary_pattern: Dict = None, conflict_patterns: List[Dict] = None,
                 special_pattern: Dict = None) -> OptimizationResult:
        """
        å˜åˆ†å¯»ä¼˜ï¼ˆ3å¹´æ»šåŠ¨çª—å£ç‰ˆæœ¬ + å®šæµ·ç¥é’ˆé€»è¾‘ï¼‰
        
        Args:
            bazi_profile: å…«å­—æ¡£æ¡ˆå¯¹è±¡
            year: æµå¹´ï¼ˆå¯é€‰ï¼‰
            geo_element: åœ°ç†äº”è¡Œå±æ€§ï¼ˆå¯é€‰ï¼‰
            geo_factor: åœ°ç†å› å­ï¼ˆå¯é€‰ï¼‰
            primary_pattern: ä¸»æ ¼å±€ï¼ˆç”¨äºå®šæµ·ç¥é’ˆé€»è¾‘ï¼‰
            conflict_patterns: å†²çªæ ¼å±€åˆ—è¡¨ï¼ˆç”¨äºå®šæµ·ç¥é’ˆé€»è¾‘ï¼‰
            
        Returns:
            ä¼˜åŒ–ç»“æœï¼ˆç¡®ä¿ç”¨ç¥åœ¨æœªæ¥36ä¸ªæœˆå†…ç¨³å®šï¼‰
        """
        if not year:
            year = 2024  # é»˜è®¤å¹´ä»½
        
        # [ä¼˜åŒ–2] 3å¹´æ»šåŠ¨çª—å£ï¼šæ‰«ææœªæ¥3å¹´
        window_years = [year, year + 1, year + 2]
        
        # 1. åˆå§‹åŒ–å¼•æ“
        from core.engine_graph import GraphNetworkEngine
        
        # 2. è·å–åŸºç¡€å…«å­—
        pillars = bazi_profile.pillars
        bazi = [
            pillars['year'],
            pillars['month'],
            pillars['day'],
            pillars['hour']
        ]
        
        # 3. åŸºå‡†åœ°ç†ä¿®æ­£
        baseline_geo_modifiers = {}
        if geo_element:
            element_map = {
                'metal': 'metal', 'wood': 'wood', 'water': 'water',
                'fire': 'fire', 'earth': 'earth'
            }
            if geo_element in element_map:
                baseline_geo_modifiers[element_map[geo_element]] = geo_factor - 1.0
        
        # 4. è®¡ç®—åŸºå‡†çŠ¶æ€ï¼ˆå½“å‰å¹´ï¼‰
        baseline_engine = GraphNetworkEngine(config=DEFAULT_FULL_ALGO_PARAMS)
        luck_pillar = bazi_profile.get_luck_pillar_at(year)
        year_pillar = bazi_profile.get_year_pillar(year)
        
        baseline_engine.initialize_nodes(
            bazi, bazi_profile.day_master,
            luck_pillar, year_pillar,
            geo_modifiers=baseline_geo_modifiers if baseline_geo_modifiers else None
        )
        baseline_engine.build_adjacency_matrix()
        baseline_engine.propagate()
        
        baseline_entropy = self._calculate_entropy(baseline_engine)
        baseline_stability = self._calculate_stability(baseline_engine)
        
        # [QGA V24.0] ç”¨ç¥åˆ¤å®šä¼˜å…ˆçº§ï¼šæ ¼ç¥ä¼˜å…ˆ > ç—…è¯ä¼˜å…ˆ > å¹³è¡¡æœ€å
        target_elements = []
        
        # [QGA V24.0] ä¼˜å…ˆçº§1ï¼šæ ¼ç¥ä¼˜å…ˆï¼ˆç‰¹æ®Šæ ¼å±€é”æ­»ï¼‰
        if special_pattern:
            yong_shen_rule = special_pattern.get('yong_shen_rule')
            target_elements = self._resolve_special_pattern_yong_shen(
                yong_shen_rule, bazi_profile.day_master
            )
            logger.info(f"ğŸ”’ ç‰¹æ®Šæ ¼å±€é”æ­»ï¼Œç”¨ç¥è§„åˆ™: {yong_shen_rule}, ç›®æ ‡å…ƒç´ : {target_elements}")
        # ä¼˜å…ˆçº§2ï¼šç—…è¯ä¼˜å…ˆï¼ˆæ ¼å±€å†²çªï¼‰
        elif primary_pattern is not None or (conflict_patterns is not None and len(conflict_patterns) > 0):
            target_elements = self._determine_yong_shen_direction(
                primary_pattern, conflict_patterns or [], bazi_profile.day_master
            )
            logger.info(f"ğŸ’Š ç—…è¯ä¼˜å…ˆï¼Œç›®æ ‡å…ƒç´ : {target_elements}")
        # ä¼˜å…ˆçº§3ï¼šå¹³è¡¡æœ€åï¼ˆæ™®é€šå‘½å±€æ‰è®¡ç®—äº”è¡Œå¹³è¡¡ï¼‰
        else:
            logger.info("âš–ï¸ å¹³è¡¡æœ€åï¼Œä½¿ç”¨äº”è¡Œå¹³è¡¡ç®—æ³•")
        
        # 5. å˜åˆ†æœç´¢ï¼ˆ3å¹´æ»šåŠ¨çª—å£ï¼‰
        best_result = None
        best_score = float('inf')
        
        # å¦‚æœå®šæµ·ç¥é’ˆé€»è¾‘ç¡®å®šäº†æ–¹å‘ï¼Œä¼˜å…ˆæœç´¢è¿™äº›å…ƒç´ 
        search_elements = target_elements if target_elements else self.elements
        
        for element in search_elements:
            for injection_amount in np.arange(0.0, 1.0, self.step_size):
                # æµ‹è¯•è¯¥å…ƒç´ åœ¨æœªæ¥3å¹´çš„ç¨³å®šæ€§
                year_scores = []
                year_stabilities = []
                year_entropies = []
                
                for test_year in window_years:
                    test_luck = bazi_profile.get_luck_pillar_at(test_year)
                    test_year_pillar = bazi_profile.get_year_pillar(test_year)
                    
                    test_engine = GraphNetworkEngine(config=DEFAULT_FULL_ALGO_PARAMS)
                    test_geo_modifiers = baseline_geo_modifiers.copy()
                    test_geo_modifiers[element] = test_geo_modifiers.get(element, 0.0) + injection_amount
                    
                    test_engine.initialize_nodes(
                        bazi, bazi_profile.day_master,
                        test_luck, test_year_pillar,
                        geo_modifiers=test_geo_modifiers if test_geo_modifiers else None
                    )
                    test_engine.build_adjacency_matrix()
                    test_engine.propagate()
                    
                    entropy = self._calculate_entropy(test_engine)
                    stability = self._calculate_stability(test_engine)
                    
                    year_entropies.append(entropy)
                    year_stabilities.append(stability)
                    # ç»¼åˆè¯„åˆ†
                    year_scores.append(entropy - stability * 10.0)
                
                # 3å¹´ç»¼åˆè¯„åˆ†ï¼šè¦æ±‚ç¨³å®šæ€§ä¸èƒ½å¤§å¹…ä¸‹é™
                avg_score = np.mean(year_scores)
                avg_stability = np.mean(year_stabilities)
                stability_trend = year_stabilities[-1] - year_stabilities[0]  # ç¨³å®šæ€§è¶‹åŠ¿
                
                # å¦‚æœç¨³å®šæ€§ä¸‹é™è¶…è¿‡20%ï¼Œæƒ©ç½šè¯¥æ–¹æ¡ˆ
                if stability_trend < -0.2:
                    avg_score += 5.0  # æƒ©ç½šåˆ†
                
                # å¦‚æœæœªæ¥å¹´ä»½ç†µå€¼å¢åŠ ï¼Œè¯´æ˜ä¼šæ¿€åŒ–å†²çªï¼Œæƒ©ç½š
                entropy_trend = year_entropies[-1] - year_entropies[0]
                if entropy_trend > 0.05:
                    avg_score += 3.0  # æƒ©ç½šåˆ†
                
                if avg_score < best_score:
                    best_score = avg_score
                    best_result = {
                        'element': element,
                        'amount': injection_amount,
                        'entropy': np.mean(year_entropies),
                        'stability': avg_stability,
                        'entropy_reduction': baseline_entropy - np.mean(year_entropies),
                        'stability_trend': stability_trend,
                        '3year_stable': stability_trend >= -0.1  # 3å¹´ç¨³å®šæ€§æ ‡å¿—
                    }
        
        # 6. ç”Ÿæˆæœ€ä¼˜ç»„åˆ
        optimal_elements = {}
        if best_result:
            optimal_elements[best_result['element']] = best_result['amount']
        
        # 7. ç”Ÿæˆè¯­ä¹‰è§£é‡Šï¼ˆåŒ…å«3å¹´ç¨³å®šæ€§ä¿¡æ¯ï¼‰
        semantic = self._generate_optimization_semantic(best_result, baseline_entropy, baseline_stability)
        
        return OptimizationResult(
            optimal_elements=optimal_elements,
            stability_score=best_result['stability'] if best_result else baseline_stability,
            entropy_reduction=best_result['entropy_reduction'] if best_result else 0.0,
            semantic_interpretation=semantic
        )
    
    def _determine_yong_shen_direction(self, primary_pattern: Dict, conflict_patterns: List[Dict],
                                      day_master: str) -> List[str]:
        """
        [QGA V23.5] å®šæµ·ç¥é’ˆé€»è¾‘ï¼šåŸºäºæ ¼å±€å†²çªç›´æ¥é”å®šç”¨ç¥æ–¹å‘
        
        å¦‚æœä¸»æ ¼å±€æ˜¯[ä¼¤å®˜è§å®˜]ï¼ŒçŸ›ç›¾ç‚¹åœ¨"å®˜æ˜Ÿå—æŸ"ï¼š
        - é€šå…³æ–¹å‘ï¼šè´¢æ˜Ÿï¼ˆä¼¤å®˜ç”Ÿè´¢ï¼Œè´¢ç”Ÿå®˜ï¼‰
        - åˆ¶è¡¡æ–¹å‘ï¼šå°æ˜Ÿï¼ˆå°å…‹ä¼¤å®˜ï¼Œä¿æŠ¤å®˜æ˜Ÿï¼‰
        
        Returns:
            ç›®æ ‡å…ƒç´ åˆ—è¡¨ï¼ˆä¼˜å…ˆæœç´¢æ–¹å‘ï¼‰
        """
        if not conflict_patterns:
            return []  # æ²¡æœ‰å†²çªï¼Œä¸é”å®šæ–¹å‘
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¼¤å®˜è§å®˜æ ¼å±€
        for cp in conflict_patterns:
            pattern_name = cp.get('name', '').lower()
            pattern_id = cp.get('id', '').lower()
            
            if 'ä¼¤å®˜' in pattern_name and 'å®˜' in pattern_name or \
               'shang_guan' in pattern_id and 'guan' in pattern_id:
                # ä¼¤å®˜è§å®˜ï¼šé€šå…³ç”¨è´¢ï¼Œåˆ¶è¡¡ç”¨å°
                # æ ¹æ®æ—¥ä¸»ç¡®å®šè´¢å’Œå°çš„å…ƒç´ 
                day_master_elements = {
                    'ç”²': 'wood', 'ä¹™': 'wood',
                    'ä¸™': 'fire', 'ä¸': 'fire',
                    'æˆŠ': 'earth', 'å·±': 'earth',
                    'åºš': 'metal', 'è¾›': 'metal',
                    'å£¬': 'water', 'ç™¸': 'water'
                }
                dm_element = day_master_elements.get(day_master, 'earth')
                
                # è´¢æ˜Ÿï¼šæˆ‘å…‹è€…ä¸ºè´¢
                # å°æ˜Ÿï¼šç”Ÿæˆ‘è€…ä¸ºå°
                generation_map = {
                    'wood': 'water',  # æ°´ç”Ÿæœ¨ï¼ˆå°ï¼‰
                    'fire': 'wood',   # æœ¨ç”Ÿç«ï¼ˆå°ï¼‰
                    'earth': 'fire',  # ç«ç”ŸåœŸï¼ˆå°ï¼‰
                    'metal': 'earth', # åœŸç”Ÿé‡‘ï¼ˆå°ï¼‰
                    'water': 'metal' # é‡‘ç”Ÿæ°´ï¼ˆå°ï¼‰
                }
                control_map = {
                    'wood': 'earth',  # æœ¨å…‹åœŸï¼ˆè´¢ï¼‰
                    'fire': 'metal',   # ç«å…‹é‡‘ï¼ˆè´¢ï¼‰
                    'earth': 'water',  # åœŸå…‹æ°´ï¼ˆè´¢ï¼‰
                    'metal': 'wood',   # é‡‘å…‹æœ¨ï¼ˆè´¢ï¼‰
                    'water': 'fire'    # æ°´å…‹ç«ï¼ˆè´¢ï¼‰
                }
                
                yin_element = generation_map.get(dm_element, 'earth')
                cai_element = control_map.get(dm_element, 'earth')
                
                # ä¼˜å…ˆé€šå…³ï¼ˆè´¢ï¼‰ï¼Œå…¶æ¬¡åˆ¶è¡¡ï¼ˆå°ï¼‰
                return [cai_element, yin_element]
        
        return []  # å…¶ä»–å†²çªæ ¼å±€ï¼Œä¸é”å®šæ–¹å‘
    
    def _resolve_special_pattern_yong_shen(self, yong_shen_rule: str, day_master: str) -> List[str]:
        """
        [QGA V24.0] è§£æç‰¹æ®Šæ ¼å±€çš„ç”¨ç¥è§„åˆ™
        æ ¼ç¥ä¼˜å…ˆï¼šç›´æ¥é”å®šç”¨ç¥ï¼Œä¸å†è€ƒè™‘å¹³è¡¡
        """
        day_master_elements = {
            'ç”²': 'wood', 'ä¹™': 'wood',
            'ä¸™': 'fire', 'ä¸': 'fire',
            'æˆŠ': 'earth', 'å·±': 'earth',
            'åºš': 'metal', 'è¾›': 'metal',
            'å£¬': 'water', 'ç™¸': 'water'
        }
        dm_element = day_master_elements.get(day_master, 'earth')
        
        # åç¥åˆ°äº”è¡Œçš„æ˜ å°„
        generation_map = {
            'wood': 'water',  # æ°´ç”Ÿæœ¨ï¼ˆå°ï¼‰
            'fire': 'wood',   # æœ¨ç”Ÿç«ï¼ˆå°ï¼‰
            'earth': 'fire',  # ç«ç”ŸåœŸï¼ˆå°ï¼‰
            'metal': 'earth', # åœŸç”Ÿé‡‘ï¼ˆå°ï¼‰
            'water': 'metal'  # é‡‘ç”Ÿæ°´ï¼ˆå°ï¼‰
        }
        control_map = {
            'wood': 'earth',  # æœ¨å…‹åœŸï¼ˆè´¢ï¼‰
            'fire': 'metal',  # ç«å…‹é‡‘ï¼ˆè´¢ï¼‰
            'earth': 'water', # åœŸå…‹æ°´ï¼ˆè´¢ï¼‰
            'metal': 'wood',  # é‡‘å…‹æœ¨ï¼ˆè´¢ï¼‰
            'water': 'fire'   # æ°´å…‹ç«ï¼ˆè´¢ï¼‰
        }
        output_map = {
            'wood': 'fire',   # æœ¨ç”Ÿç«ï¼ˆä¼¤å®˜/é£Ÿç¥ï¼‰
            'fire': 'earth',  # ç«ç”ŸåœŸï¼ˆä¼¤å®˜/é£Ÿç¥ï¼‰
            'earth': 'metal', # åœŸç”Ÿé‡‘ï¼ˆä¼¤å®˜/é£Ÿç¥ï¼‰
            'metal': 'water', # é‡‘ç”Ÿæ°´ï¼ˆä¼¤å®˜/é£Ÿç¥ï¼‰
            'water': 'wood'   # æ°´ç”Ÿæœ¨ï¼ˆä¼¤å®˜/é£Ÿç¥ï¼‰
        }
        control_reverse_map = {
            'wood': 'metal',  # é‡‘å…‹æœ¨ï¼ˆå®˜æ€ï¼‰
            'fire': 'water',  # æ°´å…‹ç«ï¼ˆå®˜æ€ï¼‰
            'earth': 'wood',  # æœ¨å…‹åœŸï¼ˆå®˜æ€ï¼‰
            'metal': 'fire',  # ç«å…‹é‡‘ï¼ˆå®˜æ€ï¼‰
            'water': 'earth'  # åœŸå…‹æ°´ï¼ˆå®˜æ€ï¼‰
        }
        
        if yong_shen_rule == 'shang_guan_or_wealth':
            # ä¼¤å®˜ä¼¤å°½ï¼šç”¨ç¥ä¸ºä¼¤å®˜ï¼ˆæˆ‘ç”Ÿï¼‰æˆ–è´¢ï¼ˆæˆ‘å…‹ï¼‰
            return [output_map.get(dm_element, 'fire'), control_map.get(dm_element, 'earth')]
        elif yong_shen_rule == 'wealth':
            # ä»è´¢æ ¼ï¼šç”¨ç¥ä¸ºè´¢ï¼ˆæˆ‘å…‹ï¼‰
            return [control_map.get(dm_element, 'earth')]
        elif yong_shen_rule == 'sha_or_yin':
            # ç¾Šåˆƒé©¾æ€ï¼šç”¨ç¥ä¸ºä¸ƒæ€ï¼ˆå…‹æˆ‘ï¼‰æˆ–å°ï¼ˆç”Ÿæˆ‘ï¼‰
            return [control_reverse_map.get(dm_element, 'metal'), generation_map.get(dm_element, 'water')]
        elif yong_shen_rule == 'maintain_purity':
            # è¶…å¯¼ä½“ï¼šç»´æŒçº¯åº¦ï¼Œç”¨ç¥ä¸ºæ—¥ä¸»æœ¬èº«
            return [dm_element]
        elif yong_shen_rule == 'transformed_element':
            # åŒ–æ°”æ ¼ï¼šç”¨ç¥ä¸ºåˆåŒ–åçš„å…ƒç´ ï¼ˆéœ€è¦ä»æ ¼å±€ä¿¡æ¯ä¸­è·å–ï¼‰
            # ç®€åŒ–ï¼šè¿”å›æ—¥ä¸»å…ƒç´ ï¼ˆå®é™…åº”è¯¥ä»åˆåŒ–ä¿¡æ¯ä¸­è·å–ï¼‰
            return [dm_element]
        else:
            return []
    
    def _calculate_entropy(self, engine: GraphNetworkEngine) -> float:
        """è®¡ç®—ç³»ç»Ÿç†µå€¼"""
        energies = []
        if not engine.nodes:
            return 1.0
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„èƒ½é‡ç±»å‹
        first_node_energy = engine.nodes[0].current_energy
        is_probvalue = hasattr(first_node_energy, 'mean')
        
        for node in engine.nodes:
            if is_probvalue:
                # ProbValueç±»å‹
                energies.append(node.current_energy.mean)
            else:
                energies.append(float(node.current_energy))
        
        if not energies:
            return 1.0
        
        # å½’ä¸€åŒ–
        total = sum(energies)
        if total == 0:
            return 1.0
        
        probs = [e / total for e in energies]
        # è®¡ç®—ä¿¡æ¯ç†µ
        entropy = -sum(p * np.log2(p + 1e-10) for p in probs if p > 0)
        return entropy
    
    def _calculate_stability(self, engine: GraphNetworkEngine) -> float:
        """è®¡ç®—ç³»ç»Ÿç¨³å®šæ€§"""
        # ç®€åŒ–ç‰ˆï¼šåŸºäºèƒ½é‡åˆ†å¸ƒçš„æ–¹å·®
        energies = []
        if not engine.nodes:
            return 0.0
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„èƒ½é‡ç±»å‹
        first_node_energy = engine.nodes[0].current_energy
        is_probvalue = hasattr(first_node_energy, 'mean')
        
        for node in engine.nodes:
            if is_probvalue:
                energies.append(node.current_energy.mean)
            else:
                energies.append(float(node.current_energy))
        
        if not energies:
            return 0.0
        
        # ç¨³å®šæ€§ = 1 / (1 + æ–¹å·®)
        variance = np.var(energies)
        stability = 1.0 / (1.0 + variance)
        return stability
    
    def _generate_optimization_semantic(self, best_result: Dict, baseline_entropy: float, 
                                       baseline_stability: float) -> str:
        """ç”Ÿæˆä¼˜åŒ–è¯­ä¹‰è§£é‡Šï¼ˆåŒ…å«3å¹´ç¨³å®šæ€§éªŒè¯ï¼‰"""
        if not best_result:
            return "å½“å‰ç³»ç»Ÿå·²è¾¾åˆ°è¾ƒä¼˜çŠ¶æ€ï¼Œæ— éœ€å¤§å¹…è°ƒæ•´ã€‚"
        
        element_cn = self.element_cn.get(best_result['element'], best_result['element'])
        reduction = best_result['entropy_reduction']
        is_3year_stable = best_result.get('3year_stable', True)
        stability_trend = best_result.get('stability_trend', 0.0)
        
        base_msg = ""
        if reduction > 0.1:
            base_msg = f"ç³»ç»Ÿé€šè¿‡æ³¨å…¥{element_cn}å…ƒç´ ï¼ˆå¼ºåº¦{best_result['amount']:.2f}ï¼‰èƒ½å¤Ÿæ˜¾è‘—é™ä½å†…è€—ï¼Œæå‡ç¨³å®šæ€§ã€‚"
        elif reduction > 0.05:
            base_msg = f"ç³»ç»Ÿé€šè¿‡é€‚åº¦æ³¨å…¥{element_cn}å…ƒç´ èƒ½å¤Ÿæ”¹å–„èƒ½é‡åˆ†å¸ƒï¼Œå‡å°‘å†…éƒ¨å†²çªã€‚"
        else:
            base_msg = "å½“å‰ç³»ç»ŸçŠ¶æ€è¾ƒä¸ºå¹³è¡¡ï¼Œå°å¹…è°ƒæ•´å³å¯ç»´æŒç¨³å®šã€‚"
        
        # [ä¼˜åŒ–2] æ·»åŠ 3å¹´ç¨³å®šæ€§éªŒè¯
        if is_3year_stable:
            if stability_trend > 0.05:
                base_msg += " ç»è¿‡3å¹´æ»šåŠ¨çª—å£éªŒè¯ï¼Œè¯¥ç”¨ç¥åœ¨æœªæ¥36ä¸ªæœˆå†…å°†å¸¦æ¥æŒç»­ç¨³å®šçš„å¢ç›Šï¼Œä¸ä¼šæ¿€åŒ–æ½œåœ¨å†²çªã€‚"
            else:
                base_msg += " ç»è¿‡3å¹´æ»šåŠ¨çª—å£éªŒè¯ï¼Œè¯¥ç”¨ç¥åœ¨æœªæ¥36ä¸ªæœˆå†…ä¿æŒç¨³å®šï¼Œä¸ä¼šå‡ºç°'ä»Šå¹´å‘è´¢ï¼Œæ˜å¹´åç‰¢'çš„çŸ­è§†é£é™©ã€‚"
        else:
            base_msg += f" âš ï¸ æ³¨æ„ï¼šè¯¥ç”¨ç¥åœ¨æœªæ¥3å¹´å†…å¯èƒ½å¯¼è‡´ç¨³å®šæ€§ä¸‹é™ï¼ˆè¶‹åŠ¿{stability_trend:.2f}ï¼‰ï¼Œå»ºè®®è°¨æ…ä½¿ç”¨æˆ–å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆã€‚"
        
        if reduction > 0.1:
            base_msg += " è¿™æ˜¯æœ€èƒ½å¹³æ¯å†…è€—ã€å¼€å¯è´¢å¯Œçš„é’¥åŒ™ã€‚"
        
        return base_msg


class MediumCompensationEngine:
    """
    [M.C.A] ä»‹è´¨ä¿®æ­£æ¨¡å‹å¼•æ“
    å°†åœ°ç†ï¼ˆå®è§‚ï¼‰å’Œå±…å®¶ç¯å¢ƒï¼ˆå¾®è§‚ï¼‰å®šä¹‰ä¸º"åœºå¼ºä¿®æ­£ç³»æ•°"
    """
    
    def __init__(self):
        # åŸå¸‚äº”è¡Œå±æ€§æ˜ å°„ï¼ˆå‚è€ƒé‡å­çœŸè¨€é¡µé¢çš„GEO_CITY_MAPï¼‰
        # æ ¼å¼: "åŸå¸‚å": (geo_factor, "element_affinity")
        # è¿™é‡Œæå–ä¸»è¦å…ƒç´ ï¼ˆå–ç¬¬ä¸€ä¸ªï¼‰
        self.city_elements = {
            # ä¸­å›½ç›´è¾–å¸‚/ä¸€çº¿åŸå¸‚
            'åŒ—äº¬': 'fire', 'ä¸Šæµ·': 'water', 'æ·±åœ³': 'fire', 'å¹¿å·': 'fire',
            'å¤©æ´¥': 'water', 'é‡åº†': 'water',
            # çœä¼šåŸå¸‚
            'çŸ³å®¶åº„': 'earth', 'å¤ªåŸ': 'metal', 'å‘¼å’Œæµ©ç‰¹': 'metal',
            'æ²ˆé˜³': 'water', 'é•¿æ˜¥': 'water', 'å“ˆå°”æ»¨': 'water',
            'å—äº¬': 'fire', 'æ­å·': 'water', 'åˆè‚¥': 'earth', 'ç¦å·': 'water',
            'å—æ˜Œ': 'fire', 'æµå—': 'water', 'éƒ‘å·': 'earth', 'æ­¦æ±‰': 'water',
            'é•¿æ²™': 'fire', 'å—å®': 'wood', 'æµ·å£': 'water', 'æˆéƒ½': 'earth',
            'è´µé˜³': 'wood', 'æ˜†æ˜': 'wood', 'æ‹‰è¨': 'metal', 'è¥¿å®‰': 'metal',
            'å…°å·': 'metal', 'è¥¿å®': 'water', 'é“¶å·': 'metal', 'ä¹Œé²æœ¨é½': 'metal',
            # å…¶ä»–é‡è¦åŸå¸‚
            'è‹å·': 'water', 'æ— é”¡': 'water', 'å®æ³¢': 'water', 'é’å²›': 'water',
            'å¤§è¿': 'water', 'å¦é—¨': 'water', 'ç æµ·': 'water', 'ä¸œè': 'fire',
            'ä½›å±±': 'fire',
            # æ¸¯æ¾³å°
            'é¦™æ¸¯': 'water', 'æ¾³é—¨': 'water', 'å°åŒ—': 'water', 'é«˜é›„': 'fire',
            # äºšæ´²åŸå¸‚
            'ä¸œäº¬': 'water', 'å¤§é˜ª': 'water', 'é¦–å°”': 'metal', 'æ–°åŠ å¡': 'fire',
            'å‰éš†å¡': 'fire', 'æ›¼è°·': 'fire', 'é©¬å°¼æ‹‰': 'fire', 'é›…åŠ è¾¾': 'fire',
            'æ²³å†…': 'water', 'èƒ¡å¿—æ˜å¸‚': 'fire', 'å­Ÿä¹°': 'fire', 'æ–°å¾·é‡Œ': 'fire',
            'è¿ªæ‹œ': 'fire',
            # æ¬§æ´²åŸå¸‚
            'ä¼¦æ•¦': 'water', 'å·´é»': 'metal', 'æŸæ—': 'metal', 'æ³•å…°å…‹ç¦': 'metal',
            'é˜¿å§†æ–¯ç‰¹ä¸¹': 'water', 'è‹é»ä¸–': 'metal', 'ç±³å…°': 'fire', 'è«æ–¯ç§‘': 'water',
            # åŒ—ç¾åŸå¸‚
            'çº½çº¦': 'metal', 'æ´›æ‰çŸ¶': 'fire', 'æ—§é‡‘å±±': 'water', 'è¥¿é›…å›¾': 'water',
            'èŠåŠ å“¥': 'metal', 'å¤šä¼¦å¤š': 'water', 'æ¸©å“¥å': 'water',
            # å¤§æ´‹æ´²åŸå¸‚
            'æ‚‰å°¼': 'fire', 'å¢¨å°”æœ¬': 'water', 'å¥¥å…‹å…°': 'water',
        }
        
        # [ä¼˜åŒ–4] å¾®ç¯å¢ƒä¿®æ­£ç³»æ•°ï¼ˆæ·»åŠ ç‰¹å®šçŸ¢é‡åç§»ï¼‰
        # ä¾‹å¦‚ï¼šè¿‘æ°´å¢åŠ æ°´å…ƒç´ 15%ï¼ŒåŒæ—¶é™ä½ç«å…ƒç´ ç¨³å®šæ€§
        self.micro_env_factors = {
            'è¿‘æ°´': {'water': 1.15, 'fire': 0.85, 'earth': 0.95, 'wood': 1.05, 'metal': 1.0},
            'è¿‘å±±': {'earth': 1.15, 'wood': 1.10, 'fire': 0.90, 'water': 0.95, 'metal': 1.05},
            'é«˜å±‚': {'fire': 1.10, 'metal': 1.05, 'earth': 0.95, 'water': 0.90, 'wood': 1.0},
            'ä½å±‚': {'earth': 1.10, 'water': 1.05, 'wood': 1.0, 'fire': 0.95, 'metal': 0.95},
        }
        
        # [ä¼˜åŒ–4] å¾®ç¯å¢ƒçŸ¢é‡åç§»ï¼ˆç›´æ¥ä½œç”¨äºäº”è¡Œèƒ½é‡åˆ†å¸ƒï¼‰
        self.micro_env_vector_offsets = {
            'è¿‘æ°´': {'water': +15.0, 'fire': -10.0},  # è¿‘æ°´ï¼šæ°´+15%ï¼Œç«-10%
            'è¿‘å±±': {'earth': +15.0, 'wood': +10.0},  # è¿‘å±±ï¼šåœŸ+15%ï¼Œæœ¨+10%
            'é«˜å±‚': {'fire': +10.0, 'metal': +5.0, 'water': -10.0},  # é«˜å±‚ï¼šç«+10%ï¼Œé‡‘+5%ï¼Œæ°´-10%
            'ä½å±‚': {'earth': +10.0, 'water': +5.0},  # ä½å±‚ï¼šåœŸ+10%ï¼Œæ°´+5%
        }
    
    def compensate(self, bazi_profile: BaziProfile, city: str = None,
                   micro_env: List[str] = None) -> MediumCompensationResult:
        """
        ä»‹è´¨ä¿®æ­£
        
        Args:
            bazi_profile: å…«å­—æ¡£æ¡ˆå¯¹è±¡
            city: åŸå¸‚åç§°
            micro_env: å¾®ç¯å¢ƒåˆ—è¡¨ï¼ˆå¦‚['è¿‘æ°´', 'é«˜å±‚']ï¼‰
            
        Returns:
            ä¿®æ­£ç»“æœ
        """
        # 1. åœ°ç†ä¿®æ­£
        geo_correction = {'metal': 1.0, 'wood': 1.0, 'water': 1.0, 'fire': 1.0, 'earth': 1.0}
        
        if city:
            city_element = self.city_elements.get(city, 'neutral')
            if city_element != 'neutral':
                # åŒå±æ€§å¢å¼ºï¼Œç›¸ç”Ÿå¢å¼ºï¼Œç›¸å…‹å‡å¼±
                geo_correction[city_element] = 1.15
                # ç›¸ç”Ÿå…³ç³»
                generation_map = {
                    'wood': 'fire', 'fire': 'earth', 'earth': 'metal',
                    'metal': 'water', 'water': 'wood'
                }
                if city_element in generation_map:
                    geo_correction[generation_map[city_element]] = 1.10
                # ç›¸å…‹å…³ç³»
                control_map = {
                    'wood': 'earth', 'earth': 'water', 'water': 'fire',
                    'fire': 'metal', 'metal': 'wood'
                }
                if city_element in control_map:
                    geo_correction[control_map[city_element]] = 0.90
        
        # 2. å¾®ç¯å¢ƒä¿®æ­£ï¼ˆ[ä¼˜åŒ–4] åº”ç”¨çŸ¢é‡åç§»ï¼‰
        micro_correction = {'metal': 1.0, 'wood': 1.0, 'water': 1.0, 'fire': 1.0, 'earth': 1.0}
        micro_vector_offsets = {'metal': 0.0, 'wood': 0.0, 'water': 0.0, 'fire': 0.0, 'earth': 0.0}
        
        if micro_env:
            for env in micro_env:
                if env in self.micro_env_factors:
                    factors = self.micro_env_factors[env]
                    for element, factor in factors.items():
                        micro_correction[element] *= factor
                
                # [ä¼˜åŒ–4] åº”ç”¨çŸ¢é‡åç§»
                if env in self.micro_env_vector_offsets:
                    offsets = self.micro_env_vector_offsets[env]
                    for element, offset in offsets.items():
                        micro_vector_offsets[element] += offset
        
        # 3. æ€»ä¿®æ­£ï¼ˆå–å¹³å‡å€¼ï¼‰
        total_correction = {}
        for element in ['metal', 'wood', 'water', 'fire', 'earth']:
            total_correction[element] = (geo_correction[element] + micro_correction[element]) / 2.0
        
        # 4. ç”Ÿæˆè¯­ä¹‰è§£é‡Š
        semantic = self._generate_compensation_semantic(city, micro_env, geo_correction, micro_correction)
        
        return MediumCompensationResult(
            geo_correction=geo_correction,
            micro_env_correction=micro_correction,
            total_correction=total_correction,
            semantic_interpretation=semantic
        )
    
    def get_micro_env_vector_offsets(self, micro_env: List[str] = None) -> Dict[str, float]:
        """
        [ä¼˜åŒ–4] è·å–å¾®ç¯å¢ƒçš„çŸ¢é‡åç§»
        
        Args:
            micro_env: å¾®ç¯å¢ƒåˆ—è¡¨
            
        Returns:
            çŸ¢é‡åç§»å­—å…¸ï¼ˆç™¾åˆ†æ¯”ï¼‰
        """
        offsets = {'metal': 0.0, 'wood': 0.0, 'water': 0.0, 'fire': 0.0, 'earth': 0.0}
        
        if micro_env:
            for env in micro_env:
                if env in self.micro_env_vector_offsets:
                    env_offsets = self.micro_env_vector_offsets[env]
                    for element, offset in env_offsets.items():
                        offsets[element] += offset
        
        return offsets
    
    def _generate_compensation_semantic(self, city: str, micro_env: List[str],
                                      geo_correction: Dict, micro_correction: Dict) -> str:
        """ç”Ÿæˆä¿®æ­£è¯­ä¹‰è§£é‡Š"""
        parts = []
        
        if city:
            city_element = self.city_elements.get(city, 'neutral')
            if city_element != 'neutral':
                element_cn = {'metal': 'é‡‘', 'wood': 'æœ¨', 'water': 'æ°´', 'fire': 'ç«', 'earth': 'åœŸ'}.get(city_element, '')
                if geo_correction.get(city_element, 1.0) > 1.1:
                    parts.append(f"å½“å‰åŸå¸‚ï¼ˆ{city}ï¼‰çš„{element_cn}å±æ€§è¡¥å¼ºäº†å‘½å±€ï¼Œå½¢æˆæœ‰åˆ©çš„èƒ½é‡åœºã€‚")
                elif geo_correction.get(city_element, 1.0) < 0.95:
                    parts.append(f"å½“å‰åŸå¸‚ï¼ˆ{city}ï¼‰çš„ç¯å¢ƒå±æ€§ä¸å‘½å±€å­˜åœ¨ä¸€å®šå†²çªï¼Œå¯èƒ½æ¿€åŒ–å†…åœ¨çŸ›ç›¾ã€‚")
        
        if micro_env:
            env_desc = []
            for env in micro_env:
                if env == 'è¿‘æ°´':
                    if micro_correction.get('water', 1.0) > 1.1:
                        env_desc.append("è¿‘æ°´ç¯å¢ƒå¢å¼ºäº†æ°´å…ƒç´ ")
                    elif micro_correction.get('fire', 1.0) < 0.9:
                        env_desc.append("è¿‘æ°´ç¯å¢ƒæŠ‘åˆ¶äº†ç«å…ƒç´ ")
                elif env == 'è¿‘å±±':
                    if micro_correction.get('earth', 1.0) > 1.1:
                        env_desc.append("è¿‘å±±ç¯å¢ƒå¢å¼ºäº†åœŸå…ƒç´ ")
                elif env == 'é«˜å±‚':
                    if micro_correction.get('fire', 1.0) > 1.05:
                        env_desc.append("é«˜å±‚ç¯å¢ƒå¢å¼ºäº†ç«å…ƒç´ ")
                elif env == 'ä½å±‚':
                    if micro_correction.get('earth', 1.0) > 1.05:
                        env_desc.append("ä½å±‚ç¯å¢ƒå¢å¼ºäº†åœŸå…ƒç´ ")
            
            if env_desc:
                parts.append(f"å¾®ç¯å¢ƒï¼ˆ{', '.join(micro_env)}ï¼‰çš„å½±å“ï¼š{', '.join(env_desc)}ã€‚")
        
        if not parts:
            return "å½“å‰ç¯å¢ƒå¯¹å‘½å±€å½±å“ä¸­æ€§ï¼Œæ— æ˜æ˜¾è¡¥å¼ºæˆ–å‰Šå¼±ã€‚"
        
        return " ".join(parts)

