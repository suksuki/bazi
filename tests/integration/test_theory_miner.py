import pytest
from unittest.mock import MagicMock, patch
from learning.theory_miner import TheoryMiner

@pytest.fixture
def mock_ollama_client():
    with patch('learning.theory_miner.ollama.Client') as MockClient:
        client_instance = MockClient.return_value
        yield client_instance

def test_extract_rules_summary_mode(mock_ollama_client):
    """
    Verify that extract_rules with mode='summary' correctly parses the LLM response.
    """
    miner = TheoryMiner()
    
    # Mock LLM Response
    mock_response = {
        'message': {
            'content': '```json\n{"title": "Test Book", "summary": "Great book", "key_concepts": ["A", "B"]}\n```'
        }
    }
    miner.client.chat.return_value = mock_response
    
    result = miner.extract_rules("Some long text...", mode="summary")
    
    assert result['title'] == "Test Book"
    assert "Great book" in result['summary']
    assert len(result['key_concepts']) == 2
    # Verify we called chat with the right model and prompt containing "summary" instruction
    call_args = miner.client.chat.call_args
    assert "summary" in str(call_args).lower() or "核心算法思想" in str(call_args)

def test_process_book_chunking(mock_ollama_client):
    """
    Verify process_book yields correct number of chunks.
    If text length is 1500 and chunk_size is 1000, should yield 2 chunks.
    """
    miner = TheoryMiner()
    
    # Mock LLM to return valid JSON so it doesn't crash on json.loads
    miner.client.chat.return_value = {
        'message': {'content': '{"rule_name": "Test"}'}
    }
    
    # Disable autogenerated saving to disk during test to keep it clean
    with patch.object(miner, 'save_rule') as mock_save:
        long_text = "a" * 1500
        generator = miner.process_book(long_text, chunk_size=1000)
        
        results = list(generator)
        
        assert len(results) == 2
        assert results[0]['chunk_index'] == 1
        assert results[0]['total_chunks'] == 2
        assert results[1]['chunk_index'] == 2

def test_process_book_empty_input(mock_ollama_client):
    """
    Verify process_book handles empty input gracefully (yields nothing or handles it).
    """
    miner = TheoryMiner()
    generator = miner.process_book("")
    results = list(generator)
    assert len(results) == 0

