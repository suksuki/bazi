"""
[QGA V25.0 Phase 5.2] 1000æ ·æœ¬å‹åŠ›æµ‹è¯•
å¤§è§„æ¨¡é€»è¾‘ç¨³å¥æ€§å®¡è®¡ï¼šéªŒè¯ç¥ç»çŸ©é˜µè·¯ç”±ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚å¤åˆå†²çªæ€æ—¶çš„ç¨³å®šæ€§
"""

import sys
from pathlib import Path
import json
import time
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tests.pattern_lab import generate_synthetic_bazi, PATTERN_TEMPLATES
from core.subjects.neural_router.execution_kernel import NeuralRouterKernel
from core.subjects.neural_router.feature_vectorizer import FeatureVectorizer
# from core.models.pattern_engine import get_pattern_registry  # æš‚æ—¶ä¸éœ€è¦

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/batch_pressure_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ç¡®ä¿logsç›®å½•å­˜åœ¨
Path('logs').mkdir(exist_ok=True)


class BatchPressureTest:
    """æ‰¹é‡å‹åŠ›æµ‹è¯•ç±»"""
    
    def __init__(self, sample_count: int = 1000, max_workers: int = 4):
        """
        åˆå§‹åŒ–æ‰¹é‡æµ‹è¯•
        
        Args:
            sample_count: æµ‹è¯•æ ·æœ¬æ•°é‡
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
        """
        self.sample_count = sample_count
        self.max_workers = max_workers
        self.kernel = NeuralRouterKernel()
        self.vectorizer = FeatureVectorizer()
        # self.pattern_registry = get_pattern_registry()  # æš‚æ—¶ä¸éœ€è¦
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'outliers': 0,
            'weight_normalization_errors': 0,
            'semantic_healing_errors': 0,
            'api_errors': 0,
            'processing_times': [],
            'token_estimates': []
        }
        
        # ç¦»ç¾¤æ ·æœ¬
        self.outliers = []
        
        logger.info(f"âœ… æ‰¹é‡å‹åŠ›æµ‹è¯•åˆå§‹åŒ–å®Œæˆ: {sample_count}ä¸ªæ ·æœ¬, {max_workers}ä¸ªå¹¶å‘çº¿ç¨‹")
    
    def generate_complex_samples(self) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆ1000ä¸ªå¤æ‚æ ·æœ¬ï¼ˆåŒ…å«å¤åˆå†²çªæ€ï¼‰
        
        Returns:
            æ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«å…«å­—ã€æ ¼å±€ã€ç¯å¢ƒç­‰ä¿¡æ¯
        """
        logger.info("ğŸš€ å¼€å§‹ç”Ÿæˆå¤æ‚æ ·æœ¬...")
        
        samples = []
        pattern_ids = list(PATTERN_TEMPLATES.keys())
        
        # ç¯å¢ƒé…ç½®ï¼ˆç”¨äºåˆ›å»ºå¤åˆå†²çªæ€ï¼‰
        geo_environments = [
            ("åŒ—æ–¹/åŒ—äº¬", ["è¿‘æ°´"]),  # æ°´æ—ºç¯å¢ƒ
            ("å—æ–¹/æ·±åœ³", []),  # ç«æ—ºç¯å¢ƒ
            ("ä¸œæ–¹/ä¸Šæµ·", ["è¿‘æ°´"]),  # æœ¨æ—ºç¯å¢ƒ
            ("è¥¿æ–¹/è¥¿å®‰", []),  # é‡‘æ—ºç¯å¢ƒ
            ("ä¸­å¤®/éƒ‘å·", []),  # åœŸæ—ºç¯å¢ƒ
        ]
        
        # ç”Ÿæˆå•æ ¼å±€æ ·æœ¬ï¼ˆ70%ï¼‰
        single_pattern_count = int(self.sample_count * 0.7)
        for i in range(single_pattern_count):
            pattern_id = random.choice(pattern_ids)
            geo_info, micro_env = random.choice(geo_environments)
            
            try:
                virtual_profile = generate_synthetic_bazi(
                    pattern_id=pattern_id,
                    use_hardcoded=True
                )
                # æ·»åŠ profile_nameå­—æ®µ
                if virtual_profile:
                    virtual_profile['name'] = f"å‹åŠ›æµ‹è¯•æ ·æœ¬_{i+1}"
                
                if virtual_profile:
                    samples.append({
                        'sample_id': i + 1,
                        'pattern_id': pattern_id,
                        'pattern_name': PATTERN_TEMPLATES[pattern_id].get('name', pattern_id),
                        'virtual_profile': virtual_profile,
                        'geo_info': geo_info,
                        'micro_env': micro_env,
                        'complexity': 'single',
                        'year': random.randint(2020, 2025)
                    })
            except Exception as e:
                logger.warning(f"âš ï¸ ç”Ÿæˆæ ·æœ¬ {i+1} å¤±è´¥: {e}")
        
        # ç”Ÿæˆå¤åˆå†²çªæ€æ ·æœ¬ï¼ˆ30%ï¼‰
        dual_pattern_count = self.sample_count - len(samples)
        conflict_patterns = [
            ('SHANG_GUAN_JIAN_GUAN', 'YANG_REN_JIA_SHA'),  # ä¼¤å®˜è§å®˜ + ç¾Šåˆƒæ¶æ€
            ('XIAO_SHEN_DUO_SHI', 'JIAN_LU_YUE_JIE'),  # æ­ç¥å¤ºé£Ÿ + å»ºç¦„æœˆåŠ«
            ('HUA_HUO_GE', 'GUAN_YIN_XIANG_SHENG'),  # åŒ–ç«æ ¼ + å®˜å°ç›¸ç”Ÿ
            ('CONG_ER_GE', 'XIAO_SHEN_DUO_SHI'),  # ä»å„¿æ ¼ + æ­ç¥å¤ºé£Ÿ
        ]
        
        for i in range(dual_pattern_count):
            pattern1_id, pattern2_id = random.choice(conflict_patterns)
            geo_info, micro_env = random.choice(geo_environments)
            
            try:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ¼å±€ç”ŸæˆåŸºç¡€å…«å­—
                virtual_profile = generate_synthetic_bazi(
                    pattern_id=pattern1_id,
                    use_hardcoded=True
                )
                # æ·»åŠ profile_nameå­—æ®µ
                if virtual_profile:
                    virtual_profile['name'] = f"å¤åˆå†²çªæ ·æœ¬_{len(samples)+1}"
                
                if virtual_profile:
                    samples.append({
                        'sample_id': len(samples) + 1,
                        'pattern_id': f"{pattern1_id}+{pattern2_id}",
                        'pattern_name': f"{PATTERN_TEMPLATES[pattern1_id].get('name', pattern1_id)} + {PATTERN_TEMPLATES[pattern2_id].get('name', pattern2_id)}",
                        'virtual_profile': virtual_profile,
                        'geo_info': geo_info,
                        'micro_env': micro_env,
                        'complexity': 'dual',
                        'year': random.randint(2020, 2025),
                        'secondary_pattern': pattern2_id
                    })
            except Exception as e:
                logger.warning(f"âš ï¸ ç”Ÿæˆå¤åˆæ ·æœ¬ {len(samples)+1} å¤±è´¥: {e}")
        
        logger.info(f"âœ… æ ·æœ¬ç”Ÿæˆå®Œæˆ: {len(samples)}ä¸ªæ ·æœ¬ï¼ˆå•æ ¼å±€: {single_pattern_count}, å¤åˆ: {dual_pattern_count}ï¼‰")
        return samples
    
    def process_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ ·æœ¬
        
        Args:
            sample: æ ·æœ¬æ•°æ®
            
        Returns:
            å¤„ç†ç»“æœ
        """
        sample_id = sample['sample_id']
        start_time = time.time()
        
        try:
            virtual_profile = sample['virtual_profile']
            
            # æå–å…«å­—ä¿¡æ¯
            chart_pillars = None
            
            # æ–¹æ³•1: ä»hardcoded_pillarsæå–ï¼ˆä¼˜å…ˆï¼‰
            hardcoded = virtual_profile.get('_hardcoded_pillars', {})
            if hardcoded:
                chart_pillars = [
                    (hardcoded['year'][0], hardcoded['year'][1]),
                    (hardcoded['month'][0], hardcoded['month'][1]),
                    (hardcoded['day'][0], hardcoded['day'][1]),
                    (hardcoded['hour'][0], hardcoded['hour'][1])
                ]
            
            # æ–¹æ³•2: ä»bazi_dataæå–ï¼ˆå¤‡ç”¨ï¼‰
            if not chart_pillars:
                bazi_data = virtual_profile.get('bazi_data', {})
                if bazi_data and 'year' in bazi_data:
                    # bazi_dataæ ¼å¼: {"year": "æˆŠæˆŒ", "month": "å·±æœª", "day": "ä¸™åˆ", "hour": "æˆŠæˆŒ"}
                    chart_pillars = [
                        (bazi_data['year'][0], bazi_data['year'][1]),
                        (bazi_data['month'][0], bazi_data['month'][1]),
                        (bazi_data['day'][0], bazi_data['day'][1]),
                        (bazi_data['hour'][0], bazi_data['hour'][1])
                    ]
            
            if not chart_pillars:
                raise ValueError("æ— æ³•æå–å…«å­—ä¿¡æ¯")
            
            day_master = virtual_profile.get('_day_master', '')
            if not day_master:
                raise ValueError("æ— æ³•æå–æ—¥ä¸»")
            
            # æ„å»ºæ¿€æ´»æ ¼å±€åˆ—è¡¨
            active_patterns = [{
                "id": sample['pattern_id'].split('+')[0],  # ä¸»æ ¼å±€
                "name": sample['pattern_name'].split(' + ')[0],
                "weight": 0.8,
                "base_strength": 0.75
            }]
            
            # å¦‚æœæ˜¯å¤åˆæ ¼å±€ï¼Œæ·»åŠ ç¬¬äºŒä¸ªæ ¼å±€
            if sample.get('complexity') == 'dual' and 'secondary_pattern' in sample:
                active_patterns.append({
                    "id": sample['secondary_pattern'],
                    "name": PATTERN_TEMPLATES[sample['secondary_pattern']].get('name', sample['secondary_pattern']),
                    "weight": 0.6,
                    "base_strength": 0.7
                })
            
            # å…ˆä½¿ç”¨FeatureVectorizeræå–ç‰¹å¾å‘é‡
            feature_vector = self.vectorizer.vectorize_bazi(
                chart=chart_pillars,
                day_master=day_master,
                luck_pillar=None,
                year_pillar=None,
                geo_info=sample['geo_info'],
                micro_env=sample['micro_env'],
                synthesized_field={
                    "friction_index": random.randint(20, 80),  # éšæœºæ‘©æ“¦æŒ‡æ•°
                    "micro_env": sample['micro_env']
                }
            )
            
            # ä»ç‰¹å¾å‘é‡ä¸­æå–äº”è¡Œåœºå¼ºä½œä¸ºforce_vectors
            force_vectors = feature_vector.get("elemental_fields_dict", {})
            
            # æ‰§è¡Œç¥ç»çŸ©é˜µè·¯ç”±
            result = self.kernel.process_bazi_profile(
                active_patterns=active_patterns,
                synthesized_field={
                    "friction_index": random.randint(20, 80),  # éšæœºæ‘©æ“¦æŒ‡æ•°
                    "micro_env": sample['micro_env']
                },
                profile_name=f"æ ·æœ¬_{sample_id}",
                day_master=day_master,
                force_vectors=force_vectors,  # ä½¿ç”¨æå–çš„ç‰¹å¾å‘é‡
                year=sample['year'],
                luck_pillar=None,
                year_pillar=None,
                geo_info=sample['geo_info']
            )
            
            processing_time = time.time() - start_time
            
            # éªŒè¯ç»“æœ
            validation_result = self._validate_result(result, sample)
            
            return {
                'sample_id': sample_id,
                'success': True,
                'result': result,
                'validation': validation_result,
                'processing_time': processing_time,
                'error': None
            }
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ æ ·æœ¬ {sample_id} å¤„ç†å¤±è´¥: {e}", exc_info=True)
            
            return {
                'sample_id': sample_id,
                'success': False,
                'result': None,
                'validation': None,
                'processing_time': processing_time,
                'error': str(e)
            }
    
    def _validate_result(self, result: Dict[str, Any], sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        éªŒè¯å¤„ç†ç»“æœ
        
        Args:
            result: å¤„ç†ç»“æœ
            sample: åŸå§‹æ ·æœ¬
            
        Returns:
            éªŒè¯ç»“æœ
        """
        validation = {
            'weight_normalization_ok': False,
            'semantic_healing_ok': False,
            'api_ok': False,
            'is_outlier': False,
            'issues': []
        }
        
        # 1. éªŒè¯æƒé‡å½’ä¸€åŒ–
        logic_collapse = result.get('logic_collapse', {})
        if logic_collapse:
            total_weight = sum(logic_collapse.values())
            if 0.95 <= total_weight <= 1.05:
                validation['weight_normalization_ok'] = True
            else:
                validation['issues'].append(f"æƒé‡å½’ä¸€åŒ–å¼‚å¸¸: {total_weight:.4f}")
                self.stats['weight_normalization_errors'] += 1
        
        # 2. éªŒè¯è¯­ä¹‰è‡ªæ„ˆï¼ˆç¨³å®šæ€§ < 0.2 æ—¶åº”è¯†åˆ«ä¸ºå´©æ€ï¼‰
        energy_state = result.get('energy_state_report', {})
        if energy_state:
            system_stability = energy_state.get('system_stability', 1.0)
            critical_state = energy_state.get('critical_state', '')
            
            if system_stability < 0.2:
                if 'å´©æ€' in critical_state or 'ä¸ç¨³å®š' in critical_state or 'å†²çª' in critical_state:
                    validation['semantic_healing_ok'] = True
                else:
                    validation['issues'].append(f"è¯­ä¹‰è‡ªæ„ˆå¤±è´¥: ç¨³å®šæ€§{system_stability:.3f}ä½†æœªè¯†åˆ«ä¸ºå´©æ€")
                    self.stats['semantic_healing_errors'] += 1
            else:
                validation['semantic_healing_ok'] = True  # éå´©æ€ï¼Œæ— éœ€éªŒè¯
        
        # 3. éªŒè¯APIå¥å£®æ€§ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ï¼‰
        if 'error' in result:
            validation['issues'].append(f"APIé”™è¯¯: {result['error']}")
            self.stats['api_errors'] += 1
        else:
            validation['api_ok'] = True
        
        # 4. åˆ¤æ–­æ˜¯å¦ä¸ºç¦»ç¾¤æ ·æœ¬
        if validation['issues'] or not result.get('persona') or not result.get('logic_collapse'):
            validation['is_outlier'] = True
            self.stats['outliers'] += 1
        
        return validation
    
    def run_batch_test(self) -> Dict[str, Any]:
        """
        è¿è¡Œæ‰¹é‡æµ‹è¯•
        
        Returns:
            æµ‹è¯•ç»“æœæ‘˜è¦
        """
        logger.info("=" * 80)
        logger.info("ğŸš€ å¯åŠ¨1000æ ·æœ¬å‹åŠ›æµ‹è¯•")
        logger.info("=" * 80)
        
        # 1. ç”Ÿæˆæ ·æœ¬
        samples = self.generate_complex_samples()
        self.stats['total'] = len(samples)
        
        # 2. å¹¶å‘å¤„ç†
        logger.info(f"ğŸ“Š å¼€å§‹å¹¶å‘å¤„ç† {len(samples)} ä¸ªæ ·æœ¬...")
        start_time = time.time()
        
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_sample = {
                executor.submit(self.process_single_sample, sample): sample
                for sample in samples
            }
            
            completed = 0
            for future in as_completed(future_to_sample):
                completed += 1
                if completed % 100 == 0:
                    logger.info(f"ğŸ“ˆ è¿›åº¦: {completed}/{len(samples)} ({completed*100/len(samples):.1f}%)")
                
                result = future.result()
                results.append(result)
                
                # æ›´æ–°ç»Ÿè®¡
                if result['success']:
                    self.stats['success'] += 1
                    self.stats['processing_times'].append(result['processing_time'])
                    
                    # ä¼°ç®—Tokenæ¶ˆè€—ï¼ˆç®€åŒ–ï¼šåŸºäºPrompté•¿åº¦ï¼‰
                    if result['result'] and 'neural_router_metadata' in result['result']:
                        prompt_length = result['result']['neural_router_metadata'].get('inline_prompt_length', 0)
                        # ç²—ç•¥ä¼°ç®—ï¼š1å­—ç¬¦ â‰ˆ 0.25 token
                        token_estimate = prompt_length * 0.25
                        self.stats['token_estimates'].append(token_estimate)
                else:
                    self.stats['failed'] += 1
                    self.stats['api_errors'] += 1
                
                # æ”¶é›†ç¦»ç¾¤æ ·æœ¬
                if result['validation'] and result['validation'].get('is_outlier'):
                    self.outliers.append({
                        'sample_id': result['sample_id'],
                        'sample': future_to_sample[future],
                        'result': result['result'],
                        'validation': result['validation']
                    })
        
        total_time = time.time() - start_time
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        report = self._generate_report(results, total_time)
        
        # 4. ä¿å­˜ç¦»ç¾¤æ ·æœ¬
        self._save_outliers()
        
        logger.info("=" * 80)
        logger.info("âœ… æ‰¹é‡å‹åŠ›æµ‹è¯•å®Œæˆ")
        logger.info("=" * 80)
        
        return report
    
    def _generate_report(self, results: List[Dict[str, Any]], total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        
        avg_processing_time = sum(self.stats['processing_times']) / len(self.stats['processing_times']) if self.stats['processing_times'] else 0
        avg_token_estimate = sum(self.stats['token_estimates']) / len(self.stats['token_estimates']) if self.stats['token_estimates'] else 0
        
        report = {
            'test_timestamp': datetime.now().isoformat(),
            'summary': {
                'total_samples': self.stats['total'],
                'success_count': self.stats['success'],
                'failed_count': self.stats['failed'],
                'success_rate': self.stats['success'] / self.stats['total'] * 100 if self.stats['total'] > 0 else 0,
                'outlier_count': self.stats['outliers'],
                'outlier_rate': self.stats['outliers'] / self.stats['total'] * 100 if self.stats['total'] > 0 else 0
            },
            'validation_metrics': {
                'weight_normalization_errors': self.stats['weight_normalization_errors'],
                'semantic_healing_errors': self.stats['semantic_healing_errors'],
                'api_errors': self.stats['api_errors']
            },
            'performance_metrics': {
                'total_time_seconds': total_time,
                'avg_processing_time_seconds': avg_processing_time,
                'throughput_samples_per_second': self.stats['total'] / total_time if total_time > 0 else 0,
                'avg_token_estimate': avg_token_estimate,
                'total_token_estimate': sum(self.stats['token_estimates'])
            },
            'outliers': len(self.outliers)
        }
        
        # æ‰“å°æŠ¥å‘Š
        print("\n" + "=" * 80)
        print("ğŸ“Š æ‰¹é‡å‹åŠ›æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        print(f"\nã€æµ‹è¯•æ‘˜è¦ã€‘")
        print(f"  æ€»æ ·æœ¬æ•°: {report['summary']['total_samples']}")
        print(f"  æˆåŠŸ: {report['summary']['success_count']} ({report['summary']['success_rate']:.2f}%)")
        print(f"  å¤±è´¥: {report['summary']['failed_count']}")
        print(f"  ç¦»ç¾¤æ ·æœ¬: {report['summary']['outlier_count']} ({report['summary']['outlier_rate']:.2f}%)")
        
        print(f"\nã€éªŒè¯æŒ‡æ ‡ã€‘")
        print(f"  æƒé‡å½’ä¸€åŒ–é”™è¯¯: {report['validation_metrics']['weight_normalization_errors']}")
        print(f"  è¯­ä¹‰è‡ªæ„ˆé”™è¯¯: {report['validation_metrics']['semantic_healing_errors']}")
        print(f"  APIé”™è¯¯: {report['validation_metrics']['api_errors']}")
        
        print(f"\nã€æ€§èƒ½æŒ‡æ ‡ã€‘")
        print(f"  æ€»è€—æ—¶: {report['performance_metrics']['total_time_seconds']:.2f}ç§’")
        print(f"  å¹³å‡å¤„ç†æ—¶é—´: {report['performance_metrics']['avg_processing_time_seconds']:.3f}ç§’/æ ·æœ¬")
        print(f"  ååé‡: {report['performance_metrics']['throughput_samples_per_second']:.2f}æ ·æœ¬/ç§’")
        print(f"  å¹³å‡Tokenä¼°ç®—: {report['performance_metrics']['avg_token_estimate']:.1f}")
        print(f"  æ€»Tokenä¼°ç®—: {report['performance_metrics']['total_token_estimate']:.0f}")
        
        print("\n" + "=" * 80)
        
        return report
    
    def _save_outliers(self):
        """ä¿å­˜ç¦»ç¾¤æ ·æœ¬"""
        if not self.outliers:
            logger.info("âœ… æ— ç¦»ç¾¤æ ·æœ¬")
            return
        
        outliers_file = Path('logs/outliers_v25.json')
        outliers_data = {
            'timestamp': datetime.now().isoformat(),
            'count': len(self.outliers),
            'outliers': self.outliers
        }
        
        with open(outliers_file, 'w', encoding='utf-8') as f:
            json.dump(outliers_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ç¦»ç¾¤æ ·æœ¬å·²ä¿å­˜: {outliers_file} ({len(self.outliers)}ä¸ª)")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¬ QGA V25.0 Phase 5.2: 1000æ ·æœ¬å‹åŠ›æµ‹è¯•")
    print("   æ­¤æµ‹è¯•å°†éªŒè¯ç¥ç»çŸ©é˜µè·¯ç”±ç³»ç»Ÿåœ¨å¤§è§„æ¨¡å¤æ‚æ ·æœ¬ä¸‹çš„ç¨³å®šæ€§")
    print("")
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    tester = BatchPressureTest(sample_count=1000, max_workers=4)
    
    # è¿è¡Œæµ‹è¯•
    report = tester.run_batch_test()
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = Path('logs/batch_pressure_test_report.json')
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print("\nğŸ‰ Phase 5.2 å‹åŠ›æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()

