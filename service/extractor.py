# service/extractor.py
"""
Case Extraction Module
Project Crimson Vein - Information Extraction Layer

This module defines the LLM prompt and interface for extracting structured 
Bazi cases from raw unstructured text.
"""

import json
import re
import time
from typing import Dict, Optional, Any

# Optional Import for Ollama
try:
    import ollama
except ImportError:
    ollama = None

# The core system prompt defined by the user
SYSTEM_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªâ€œå…«å­—æ¡ˆä¾‹ä¿¡æ¯æŠ½å– agentâ€ã€‚
ä½ çš„ç›®æ ‡æ˜¯ï¼šè‡ªåŠ¨æ”¶é›†â€œçœŸå®å…«å­—æ¡ˆä¾‹â€ï¼Œç”¨äºå­¦æœ¯ç ”ç©¶ä¸­çš„ç®—æ³•éªŒè¯ã€‚

çº¦æŸæ¡ä»¶ï¼š
- ä¸ä½¿ç”¨ ModelScope æˆ–ä»»ä½•åˆæˆå‘½ç†æ•°æ®
- ä¸ä¾èµ–ç”¨æˆ·æ³¨å†Œæˆ–äººå·¥æäº¤
- ä¸ä½¿ç”¨çŸ­è§†é¢‘å¹³å°ä½œä¸ºä¸»è¦æ•°æ®æº
- æ•°æ®å¿…é¡»æ¥è‡ªå…¬å¼€ç½‘é¡µ
- æ¡ˆä¾‹å¿…é¡»åŒ…å«ï¼šå‡ºç”Ÿä¿¡æ¯ + å·²å‘ç”Ÿçš„äººç”Ÿäº‹ä»¶

ä»»åŠ¡ï¼š
- ä»æ–‡æœ¬ä¸­æŠ½å–å‡ºç”Ÿä¿¡æ¯ï¼ˆå¹´/æœˆ/æ—¥/æ—¶/å‡ºç”Ÿåœ°ï¼‰
- æŠ½å–æ‰€æœ‰å·²ç»å‘ç”Ÿçš„äººç”Ÿäº‹ä»¶ï¼ˆå¹´ä»½/å¹´é¾„/äº‹ä»¶ç±»å‹/æè¿°ï¼‰
- è¯„ä¼°æ¡ˆä¾‹è´¨é‡ï¼ˆQuality Scoreï¼‰
- åˆ¤æ–­æ˜¯å¦å¯ç”¨äºç®—æ³•éªŒè¯ï¼ˆå¿…é¡»æœ‰å®Œæ•´çš„ç”Ÿæ—¶å’Œè‡³å°‘1ä¸ªéªŒè¯äº‹ä»¶ï¼‰

è¾“å‡ºç›®æ ‡ï¼šä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON Schema è¾“å‡ºï¼Œä¸åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚

```json
{
  "profile": {
    "name": "å§“åæˆ–åŒ¿å",
    "gender": "M (ç”·) æˆ– F (å¥³)",
    "birth_year": 1990,
    "birth_month": 1,
    "birth_day": 1,
    "birth_hour": 12,  // 24å°æ—¶åˆ¶ï¼Œå¿…é¡»å°½é‡ç²¾ç¡®
    "birth_minute": 0, // å¦‚æœæœ‰
    "birth_city": "åŸå¸‚å"
  },
  "life_events": [
    {
      "year": 2015,
      "age": 25,
      "event_type": "Marriage", // ç±»åˆ«: Marriage, Career, Health, Wealth, Study, Crisis, Other
      "description": "äº‹ä»¶æè¿°",
      "verified": true
    }
  ],
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
  "quality_score": 85, // 0-100, åŸºäºä¿¡æ¯å®Œæ•´åº¦å’Œå¯ä¿¡åº¦
  "valid_for_validation": true // true if birth_hour is present AND events > 0
}
```
"""

def construct_prompt(raw_text: str) -> str:
    """
    Constructs the final prompt to be sent to the LLM.
    """
    return f"{SYSTEM_PROMPT_TEMPLATE}\n\nå¾…å¤„ç†æ–‡æœ¬ï¼š\n'''\n{raw_text}\n'''"

class CaseExtractor:
    def __init__(self, llm_client=None):
        """
        Initialize the Extractor.
        :param llm_client: Optional LLM client wrapper (if not using direct Ollama)
        """
        self.llm_client = llm_client
        
        # Load Config
        try:
            from core.config_manager import ConfigManager
            self.config = ConfigManager()
            # Default to qwen2.5 if not set, user can override to 'qwen2:7b' etc.
            # Align with UI key: 'selected_model_name'
            pass
        except ImportError:
            pass

    def _smart_compress(self, text: str) -> str:
        """
        Heuristic algorithm to keep only relevant lines for Bazi extraction.
        Reduces token usage and noise.
        """
        lines = text.splitlines()
        if len(lines) < 20: 
            return text # Too short, just keep it all
            
        kept_indices = set()
        
        # High-Value Keywords for Bazi
        keywords_high = ["ç”Ÿäº", "å‡ºç”Ÿ", "ä¹¾é€ ", "å¤é€ ", "ç”·å‘½", "å¥³å‘½", "å…«å­—", "å¹´", "æœˆ", "æ—¥", "æ—¶", "å¤§è¿", "æµå¹´", "å²è¿", "æ’ç›˜"]
        stems_branches = "ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥"
        
        for i, line in enumerate(lines):
            score = 0
            line_str = line.strip()
            if not line_str: continue
            
            # Check Keywords
            for kw in keywords_high:
                if kw in line_str: score += 5
                
            # Check Stems/Branches
            count_sb = sum(1 for char in line_str if char in stems_branches)
            if count_sb > 0: score += (count_sb * 2)
            
            # Check Digits (Birth years, dates)
            count_digit = sum(1 for char in line_str if char.isdigit())
            if count_digit >= 4: score += 2
            
            # Keep line if relevant
            if score >= 5: # Threshold
                kept_indices.add(i)
                # Add context (previous and next line)
                if i > 0: kept_indices.add(i-1)
                if i < len(lines) - 1: kept_indices.add(i+1)
        
        # Always keep first 5 lines (often contain Title/Name)
        for i in range(min(5, len(lines))):
            kept_indices.add(i)
            
        # Reconstruct text
        sorted_indices = sorted(list(kept_indices))
        compressed_lines = [lines[i] for i in sorted_indices]
        
        result = "\n".join(compressed_lines)
        return result

    def extract(self, raw_text: str, model: Optional[str] = None) -> Optional[Dict]:
        """
        Main extraction method.
        Connects to Local LLM (Ollama) to perform structural extraction.
        """
        # 1. Runtime Config Loading (Hot-Swapping Support)
        current_host = self.config.get('ollama_host', 'http://localhost:11434')
        # Priority: Method Argument > Config > Default
        target_model = model or self.config.get('selected_model_name', 'qwen2.5')
        
        # Fast Path: Local Regex Mode
        if target_model == 'regex':
            print("   â© [Extractor] Using Local Regex Mode (Configured)")
            return self._extract_with_regex(raw_text)
        
        # 1. Compress Text (Remove Noise)
        compressed_text = self._smart_compress(raw_text)
        print(f"   ğŸ“‰ [Extractor] Compressed {len(raw_text)} -> {len(compressed_text)} chars.")
        
        # 2. Final Truncation (Safety Net)
        final_text = compressed_text[:12000]
        if len(compressed_text) > 12000:
             print(f"   âœ‚ï¸ [Extractor] Text truncated {len(compressed_text)} -> 12000 chars.")

        prompt = construct_prompt(final_text)
        
        try:
            if not ollama:
                raise ImportError("Ollama library not loaded")
            
            # Use custom host if configured
            client = ollama.Client(host=current_host, timeout=300)
            
            print(f"   â³ [Extractor] Sending {len(final_text)} chars to {target_model}... (Host: {current_host})")
            import time
            start_t = time.time()
            
            # Call Ollama
            # We use generation instead of chat for stricter control, or chat with system prompt
            response = client.chat(model=target_model, messages=[
                {'role': 'system', 'content': SYSTEM_PROMPT_TEMPLATE},
                {'role': 'user', 'content': f"å¾…å¤„ç†æ–‡æœ¬ï¼š\n'''\n{final_text}\n'''"}
            ])
            
            duration = time.time() - start_t
            print(f"   â±ï¸ [Extractor] LLM responded in {duration:.1f}s.")
            
            content = response['message']['content']
            
            # Clean Markdown Code Blocks  (```json ... ```)
            if "```" in content:
                import re
                # Extract content between first ```json and ``` or just ``` and ```
                match = re.search(r"```(?:json)?\s*(.*?)```", content, re.DOTALL)
                if match:
                    content = match.group(1)
            
            # Parse JSON
            data = json.loads(content.strip())
            return data
            
        except ImportError:
            print("âŒ [Extractor] 'ollama' library not installed. Please pip install ollama.")
            return self._extract_with_regex(raw_text)
        except json.JSONDecodeError as e:
            content_preview = content[:50] if 'content' in locals() else "Unknown"
            print(f"âŒ [Extractor] LLM Output not valid JSON: {content_preview}...")
            return self._extract_with_regex(raw_text)
        except Exception as e:
            print(f"âŒ [Extractor] LLM Inference Failed: {e}")
            return self._extract_with_regex(raw_text)

    def _extract_with_regex(self, text: str) -> Optional[Dict]:
        """
        Emergency Fallback: Rules-based extraction for common formats.
        """
        print("   âš ï¸ Engaging Regex Fallback extraction...")
        
        # 0. Initialize Defaults
        profile = {
            "name": "Unknown_Regex",
            "gender": "Unknown",
            "birth_year": 1990,
            "birth_month": 1,
            "birth_day": 1,
            "birth_hour": 12,
            "birth_minute": 0,
            "birth_city": "Unknown"
        }
        
        # 1. Gender Detection
        if re.search(r'(ä¹¾é€ |ç”·å‘½|ä¹¾)', text):
            profile['gender'] = 'M'
        elif re.search(r'(å¤é€ |å¥³å‘½|å¤)', text):
            profile['gender'] = 'F'
            
        # 2. Date Extraction
        # Priority A: "1988å¹´8æœˆ8æ—¥"
        date_cn = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
        if date_cn:
            profile['birth_year'] = int(date_cn.group(1))
            profile['birth_month'] = int(date_cn.group(2))
            profile['birth_day'] = int(date_cn.group(3))
        else:
            # Priority B: ADB style "24 February 1955" or simple year
            year_match = re.search(r'\b(19\d{2}|20\d{2})\b', text)
            if not year_match:
                return None # No date, no case.
            profile['birth_year'] = int(year_match.group(1))

        # 3. Hour Extraction (Simple)
        hour_match = re.search(r'(\d{1,2})ç‚¹|(\d{1,2}):\d{2}', text)
        if hour_match:
            h = hour_match.group(1) or hour_match.group(2)
            profile['birth_hour'] = int(h)

        # 4. Name Heuristic
        # Look for "ã€å§“åã€‘XXX" or first line
        lines = text.splitlines()
        clean_lines = [l.strip() for l in lines if l.strip()]
        if clean_lines:
            potential_name = clean_lines[0]
            if len(potential_name) < 20: 
                profile['name'] = potential_name
        
        # Construct Result
        import hashlib
        id_str = f"{profile['name']}_{profile['birth_year']}_{profile['gender']}"
        case_id = hashlib.md5(id_str.encode()).hexdigest()
        
        return {
            "id": case_id,
            "profile": profile,
            "life_events": [], # Regex usually can't reliably extract events
            "quality_score": 60,
            "valid_for_validation": False,
            "source": "regex_fallback"
        }

    def mock_extract(self, raw_text: str) -> Dict:
        """
        Simulate extraction for testing logic flow without API cost.
        """
        # Minimal parser for specific mock patterns (just for demo)
        # in reality, this is where the LLM magic happens.
        return {
            "profile": {
                "name": "Mock User",
                "gender": "M",
                "birth_year": 1988,
                "birth_month": 8,
                "birth_day": 8,
                "birth_hour": 8,
                "birth_city": "Beijing"
            },
            "life_events": [
                {"year": 2018, "age": 30, "event_type": "Career", "description": "Founded verification engine", "verified": True}
            ],
            "tags": ["Mock", "Test"]
        }

if __name__ == "__main__":
    # Test the prompt construction
    sample_text = """
    ã€åé¦ˆã€‘ç”·å‘½ï¼Œ1985å¹´10æœˆ5æ—¥æ—©ä¸Š6ç‚¹ç”Ÿäºä¸Šæµ·ã€‚
    å¤§å®¶éƒ½è¯´æˆ‘å©šå§»ä¸é¡ºï¼Œç¡®å®å¦‚æ­¤ã€‚2012å¹´ç»“å©šï¼Œ2015å¹´å› ä¸ºæ€§æ ¼ä¸åˆç¦»å©šäº†ã€‚
    ä¸è¿‡è´¢è¿è¿˜è¡Œï¼Œ2018å¹´è‡ªå·±å‡ºæ¥å•å¹²ï¼Œèµšäº†ç¬¬ä¸€æ¡¶é‡‘ã€‚
    """
    
    extractor = CaseExtractor()
    print("Testing Smart Extraction...")
    extractor.extract(sample_text)
