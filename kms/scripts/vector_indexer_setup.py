"""
å‘é‡ç´¢å¼•å™¨è®¾ç½®è„šæœ¬
æ¿€æ´»å‘é‡è®°å¿†ï¼Œå°†Codexæ¡ç›®å­˜å…¥ChromaDB

åŸºäº: FDS_KMS_SPEC_v1.0-BETA.md ç¬¬6.1èŠ‚
"""

import json
import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    DEPENDENCIES_AVAILABLE = True
except ImportError as e:
    DEPENDENCIES_AVAILABLE = False
    print(f"âŒ ä¾èµ–æœªå®‰è£…: {e}")
    print("   å®‰è£…å‘½ä»¤: pip install chromadb sentence-transformers")
    sys.exit(1)


# é…ç½®
VECTOR_DB_PATH = os.path.join(os.path.dirname(__file__), '../data/vector_db')
COLLECTION_NAME = "classical_canon"
EMBED_MODEL_NAME = "BAAI/bge-m3"  # æˆ–ä½¿ç”¨ 'BAAI/bge-base-zh-v1.5' ä½œä¸ºå¤‡é€‰


class LocalBGEEmbeddingFunction:
    """è‡ªå®šä¹‰åµŒå…¥å‡½æ•°ï¼Œè°ƒç”¨æœ¬åœ° BGE-M3"""
    
    def __init__(self, model_name: str):
        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {model_name}...")
        print("   (é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…)")
        self.model = SentenceTransformer(model_name)
        print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def __call__(self, input_texts):
        """å°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºå‘é‡"""
        if isinstance(input_texts, str):
            input_texts = [input_texts]
        embeddings = self.model.encode(input_texts, normalize_embeddings=True)
        return embeddings.tolist()


def init_vector_db():
    """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
    print("=" * 60)
    print("åˆå§‹åŒ–å‘é‡æ•°æ®åº“")
    print("=" * 60)
    print()
    
    # 1. åˆå§‹åŒ–ChromaDB (æŒä¹…åŒ–å­˜å‚¨)
    print(f"ğŸ“‚ æ•°æ®åº“è·¯å¾„: {VECTOR_DB_PATH}")
    os.makedirs(VECTOR_DB_PATH, exist_ok=True)
    
    client = chromadb.PersistentClient(path=VECTOR_DB_PATH)
    print("   âœ… ChromaDBå®¢æˆ·ç«¯å·²åˆ›å»º")
    print()
    
    # 2. ç»‘å®šåµŒå…¥æ¨¡å‹
    print("ğŸ¤– åŠ è½½Embeddingæ¨¡å‹...")
    emb_fn = LocalBGEEmbeddingFunction(EMBED_MODEL_NAME)
    print()
    
    # 3. è·å–æˆ–åˆ›å»ºé›†åˆï¼ˆä½¿ç”¨é»˜è®¤embeddingï¼Œæ‰‹åŠ¨è®¡ç®—å‘é‡ï¼‰
    print(f"ğŸ“š åˆ›å»º/è·å–é›†åˆ: {COLLECTION_NAME}")
    
    # ChromaDBæ–°ç‰ˆæœ¬å¯èƒ½éœ€è¦ä½¿ç”¨default_embedding_function
    # æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰æ–¹å¼ï¼šæ‰‹åŠ¨è®¡ç®—embedding
    try:
        collection = client.get_or_create_collection(
            name=COLLECTION_NAME,
            metadata={"hnsw:space": "cosine", "description": "FDS-KMS Classical Canon Vector Index"}
        )
    except Exception:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨embedding_function
        collection = client.get_or_create_collection(
            name=COLLECTION_NAME,
            metadata={"hnsw:space": "cosine", "description": "FDS-KMS Classical Canon Vector Index"}
        )
    
    print("   âœ… é›†åˆå·²åˆ›å»º")
    print()
    
    return collection, client, emb_fn


def index_entry(collection, codex_entry: dict, emb_fn=None):
    """å°†ä¸€æ¡Codexæ¡ç›®å…¥åº“"""
    canon_id = codex_entry.get("canon_id", "unknown")
    
    # æ„é€ ç”¨äºæœç´¢çš„æ–‡æœ¬ï¼š[æ ‡ç­¾] + åŸæ–‡
    tags = codex_entry.get("tags", [])
    original_text = codex_entry.get("original_text", "")
    searchable_text = f"[{', '.join(tags)}] {original_text}"
    
    # å‡†å¤‡å…ƒæ•°æ®
    logic_extraction = codex_entry.get("logic_extraction", {})
    metadata = {
        "canon_id": canon_id,
        "source_book": codex_entry.get("source_book", ""),
        "chapter": codex_entry.get("chapter", ""),
        "pattern": logic_extraction.get("target_pattern", ""),
        "logic_type": logic_extraction.get("logic_type", ""),
        "relevance_score": str(codex_entry.get("relevance_score", 1.0)),
        "json_payload": json.dumps(codex_entry, ensure_ascii=False)  # å­˜å…¥å®Œæ•´JSON
    }
    
    # è®¡ç®—embeddingï¼ˆå¦‚æœæä¾›äº†embeddingå‡½æ•°ï¼‰
    embeddings = None
    if emb_fn:
        embeddings = [emb_fn([searchable_text])[0]]
    
    # å…¥åº“
    if embeddings:
        collection.add(
            documents=[searchable_text],
            embeddings=embeddings,
            metadatas=[metadata],
            ids=[canon_id]
        )
    else:
        collection.add(
            documents=[searchable_text],
            metadatas=[metadata],
            ids=[canon_id]
        )
    
    return True


def search_similar(collection, query_text: str, n_results: int = 5, emb_fn=None):
    """æœç´¢ç›¸ä¼¼æ¡ç›®"""
    # å¦‚æœæä¾›äº†embeddingå‡½æ•°ï¼Œæ‰‹åŠ¨è®¡ç®—query embedding
    query_embeddings = None
    if emb_fn:
        query_embeddings = [emb_fn([query_text])[0]]
    
    if query_embeddings:
        results = collection.query(
            query_embeddings=query_embeddings,
            n_results=n_results
        )
    else:
        results = collection.query(
            query_texts=[query_text],
            n_results=n_results
        )
    
    similar_entries = []
    if results["ids"] and len(results["ids"][0]) > 0:
        for i, canon_id in enumerate(results["ids"][0]):
            metadata = results["metadatas"][0][i]
            json_payload = metadata.get("json_payload", "")
            
            if json_payload:
                entry = json.loads(json_payload)
                # æ·»åŠ è·ç¦»ä¿¡æ¯ï¼ˆChromaDBè¿”å›çš„è·ç¦»ï¼‰
                if "distances" in results and results["distances"][0]:
                    entry["_distance"] = results["distances"][0][i]
                similar_entries.append(entry)
    
    return similar_entries


def main():
    """ä¸»å‡½æ•°ï¼šæµ‹è¯•å‘é‡ç´¢å¼•"""
    
    print("ğŸš€ FDS-KMS å‘é‡ç´¢å¼•å™¨è®¾ç½®")
    print()
    
    # åˆå§‹åŒ–æ•°æ®åº“
    collection, client, emb_fn = init_vector_db()
    
    # åŠ è½½é»„é‡‘æµ‹è¯•æ•°æ®
    print("ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®...")
    data_path = os.path.join(os.path.dirname(__file__), '../data/golden_test_data.json')
    
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            entries = json.load(f)
        print(f"   âœ… åŠ è½½äº† {len(entries)} æ¡codexæ¡ç›®")
        print()
    except FileNotFoundError:
        print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return
    
    # ç´¢å¼•æ¡ç›®
    print("ğŸ’¾ ç´¢å¼•codexæ¡ç›®...")
    success_count = 0
    for entry in entries:
        try:
            index_entry(collection, entry, emb_fn)
            canon_id = entry.get("canon_id", "unknown")
            print(f"   âœ… å·²ç´¢å¼•: {canon_id}")
            success_count += 1
        except Exception as e:
            print(f"   âŒ ç´¢å¼•å¤±è´¥ {entry.get('canon_id', 'unknown')}: {e}")
    
    print()
    print(f"âœ… æˆåŠŸç´¢å¼• {success_count}/{len(entries)} æ¡æ¡ç›®")
    print()
    
    # æµ‹è¯•æœç´¢
    print("ğŸ” æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢...")
    print()
    
    test_queries = [
        "æ­ç¥å¤ºé£Ÿ",
        "é£Ÿç¥æ ¼æˆæ ¼æ¡ä»¶",
        "è´¢æ˜Ÿè§£æ•‘"
    ]
    
    for query in test_queries:
        print(f"æŸ¥è¯¢: {query}")
        similar = search_similar(collection, query, n_results=2, emb_fn=emb_fn)
        print(f"   æ‰¾åˆ° {len(similar)} ä¸ªç›¸ä¼¼æ¡ç›®:")
        for entry in similar:
            canon_id = entry.get("canon_id", "unknown")
            logic_type = entry.get("logic_extraction", {}).get("logic_type", "unknown")
            distance = entry.get("_distance", "N/A")
            print(f"      - {canon_id}: {logic_type} (è·ç¦»: {distance})")
        print()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = collection.count()
    print("=" * 60)
    print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡")
    print("=" * 60)
    print(f"   æ€»æ¡ç›®æ•°: {stats}")
    print(f"   æ•°æ®åº“è·¯å¾„: {VECTOR_DB_PATH}")
    print(f"   é›†åˆåç§°: {COLLECTION_NAME}")
    print()
    print("âœ… å‘é‡ç´¢å¼•å™¨è®¾ç½®å®Œæˆï¼")
    print()
    print("ğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. ä½¿ç”¨å‘é‡ç´¢å¼•å™¨è¿›è¡Œå¥‡ç‚¹æ£€ç´¢")
    print("   2. é›†æˆåˆ°SOPå·¥ä½œæµçš„Step 5.4")
    print("   3. æ‰¹é‡ç´¢å¼•æ›´å¤šcodexæ¡ç›®")


if __name__ == "__main__":
    main()

