"""
æ‰¹é‡å¤„ç†è„šæœ¬ (Batch Processor)
è¯»å–åŸå§‹æ–‡æœ¬ï¼Œæ‰¹é‡è¿›è¡Œè¯­ä¹‰è’¸é¦å’Œç´¢å¼•

æµæ°´çº¿: Read â†’ Split â†’ Distill â†’ Index â†’ Aggregate
"""

import json
import sys
import os
import re
from typing import List, Dict, Any, Optional

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: ollamaæœªå®‰è£…ï¼Œå°†è·³è¿‡LLMè°ƒç”¨")
    print("   å®‰è£…å‘½ä»¤: pip install ollama")

# å°è¯•ä½¿ç”¨ä¿®å¤ç‰ˆï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨V2
try:
    from kms.core.semantic_distiller_v2_fixed import SemanticDistillerV2Fixed as SemanticDistillerV2
except ImportError:
    from kms.core.semantic_distiller_v2 import SemanticDistillerV2


# é…ç½®
MODEL_NAME = "qwen2.5:3b"
RAW_TEXTS_DIR = os.path.join(os.path.dirname(__file__), '../data/raw_texts')
OUTPUT_CODEX_PATH = os.path.join(os.path.dirname(__file__), '../data/classical_codex.jsonl')
VECTOR_DB_PATH = os.path.join(os.path.dirname(__file__), '../data/vector_db')


def read_raw_text(file_path: str) -> str:
    """è¯»å–åŸå§‹æ–‡æœ¬æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return ""


def split_text_into_segments(text: str, max_length: int = 50) -> List[str]:
    """
    å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºæ®µè½
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        max_length: æœ€å¤§æ®µè½é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œé»˜è®¤50ä»¥ç¡®ä¿æ¯ä¸ªå¥å­å•ç‹¬å¤„ç†
        
    Returns:
        æ®µè½åˆ—è¡¨
    """
    # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€æ¢è¡Œç¬¦åˆ‡åˆ†
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
    
    segments = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence or len(sentence) < 3:  # è·³è¿‡ç©ºå¥å­æˆ–è¿‡çŸ­ç‰‡æ®µ
            continue
        
        # å¦‚æœå¥å­æœ¬èº«è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œå¼ºåˆ¶åˆ‡åˆ†
        if len(sentence) > max_length:
            # æŒ‰é€—å·è¿›ä¸€æ­¥åˆ‡åˆ†
            sub_sentences = re.split(r'[ï¼Œ,]', sentence)
            for sub in sub_sentences:
                sub = sub.strip()
                if sub and len(sub) >= 3:
                    segments.append(sub)
        else:
            # å•ç‹¬æˆæ®µï¼Œç¡®ä¿æ¯ä¸ªå¥å­ç‹¬ç«‹å¤„ç†
            segments.append(sentence)
    
    return segments if segments else [text.strip()]  # å¦‚æœåˆ‡åˆ†å¤±è´¥ï¼Œè¿”å›åŸæ–‡æœ¬


def call_llm_distill(text: str, source_book: str, topic: str) -> Optional[Dict[str, Any]]:
    """è°ƒç”¨LLMè¿›è¡Œè¯­ä¹‰è’¸é¦"""
    if not OLLAMA_AVAILABLE:
        return None
    
    distiller = SemanticDistillerV2()
    system_prompt = distiller.get_system_prompt(source_book, topic)
    
    try:
        response = ollama.chat(
            model=MODEL_NAME,
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': f"åˆ†æä»¥ä¸‹æ–‡æœ¬å¹¶è¾“å‡ºJSON:\n\n{text}"}
            ],
            format='json',
            options={
                'temperature': 0.1,      # ä¿æŒä½åˆ›é€ æ€§ï¼Œä¿è¯é€»è¾‘ç¨³å®š
                'num_predict': 1024,      # å¢åŠ è¾“å‡ºä¸Šé™è‡³1024 tokensï¼Œé¿å…JSONæˆªæ–­
                'num_ctx': 2048           # ç¡®ä¿ä¸Šä¸‹æ–‡çª—å£è¶³å¤Ÿå¤§
            }
        )
        
        llm_response = response['message']['content']
        output = distiller.parse_llm_response(llm_response)
        
        # ç¡®ä¿original_textå­—æ®µå­˜åœ¨
        if "original_text" not in output:
            output["original_text"] = text
        
        # éªŒè¯è¾“å‡º
        is_valid, error = distiller.validate_output(output)
        if not is_valid:
            print(f"   âš ï¸  éªŒè¯å¤±è´¥: {error}")
            print(f"   å“åº”å†…å®¹é¢„è§ˆ: {llm_response[:300]}...")
            return None
        
        # è¡¥å…¨å­—æ®µ
        codex_entry = {
            "canon_id": f"AUTO-{abs(hash(text)) % 10000:04d}",
            "source_book": source_book,
            "chapter": topic,
            "tags": ["æ‰¹é‡ç”Ÿæˆ", topic],
            "relevance_score": 0.9,
            **output
        }
        
        # ç¡®ä¿original_textä½¿ç”¨åŸå§‹æ–‡æœ¬
        codex_entry["original_text"] = text
        
        return codex_entry
        
    except Exception as e:
        print(f"   âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
        return None


def save_codex_entry(entry: Dict[str, Any], file_path: str):
    """ä¿å­˜codexæ¡ç›®åˆ°JSONLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    
    with open(file_path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(entry, ensure_ascii=False) + '\n')


def process_batch(input_file: str, 
                  source_book: str = "å­å¹³çœŸè¯ ",
                  topic: str = "é£Ÿç¥æ ¼",
                  enable_indexing: bool = False):
    """
    æ‰¹é‡å¤„ç†æ–‡æœ¬æ–‡ä»¶
    
    Args:
        input_file: è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        source_book: å…¸ç±åç§°
        topic: ä¸»é¢˜/æ ¼å±€åç§°
        enable_indexing: æ˜¯å¦å¯ç”¨å‘é‡ç´¢å¼•
    """
    print("=" * 60)
    print("FDS-KMS æ‰¹é‡å¤„ç†æµæ°´çº¿")
    print("=" * 60)
    print()
    
    # Step 1: Read
    print("ğŸ“– æ­¥éª¤1: è¯»å–åŸå§‹æ–‡æœ¬...")
    text = read_raw_text(input_file)
    if not text:
        return
    
    print(f"   âœ… å·²è¯»å– {len(text)} å­—ç¬¦")
    print()
    
    # Step 2: Split
    print("âœ‚ï¸  æ­¥éª¤2: åˆ‡åˆ†æ–‡æœ¬æ®µè½...")
    segments = split_text_into_segments(text, max_length=200)
    print(f"   âœ… åˆ‡åˆ†ä¸º {len(segments)} ä¸ªæ®µè½")
    print()
    
    # Step 3: Distill & Index
    print("ğŸ§  æ­¥éª¤3: æ‰¹é‡è¯­ä¹‰è’¸é¦...")
    print()
    
    # å°è¯•å¯¼å…¥tqdmç”¨äºè¿›åº¦æ¡
    try:
        from tqdm import tqdm
        USE_TQDM = True
    except ImportError:
        USE_TQDM = False
        print("ğŸ’¡ æç¤º: å®‰è£…tqdmå¯æ˜¾ç¤ºè¿›åº¦æ¡: pip install tqdm")
        print()
    
    success_count = 0
    failed_count = 0
    
    # åˆå§‹åŒ–å‘é‡ç´¢å¼•å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    collection = None
    if enable_indexing:
        try:
            from kms.scripts.vector_indexer_setup import init_vector_db, index_entry
            collection, _, emb_fn = init_vector_db()
            print("   âœ… å‘é‡ç´¢å¼•å™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            print(f"   âš ï¸  å‘é‡ç´¢å¼•å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            enable_indexing = False
            emb_fn = None
    
    # ä½¿ç”¨tqdmè¿›åº¦æ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    iterator = tqdm(enumerate(segments, 1), total=len(segments), desc="å¤„ç†ä¸­") if USE_TQDM else enumerate(segments, 1)
    
    for i, segment in iterator:
        if not USE_TQDM:
            print(f"   [{i}/{len(segments)}] å¤„ç†æ®µè½: {segment[:30]}...")
        
        # Distill
        entry = call_llm_distill(segment, source_book, topic)
        
        if entry:
            # ä¿å­˜åˆ°JSONL
            save_codex_entry(entry, OUTPUT_CODEX_PATH)
            success_count += 1
            
            # Index (å¦‚æœéœ€è¦)
            if enable_indexing and collection:
                try:
                    index_entry(collection, entry, emb_fn)
                except Exception as e:
                    if not USE_TQDM:
                        print(f"      âš ï¸  ç´¢å¼•å¤±è´¥: {e}")
            
            if not USE_TQDM:
                print(f"      âœ… æˆåŠŸ")
            elif USE_TQDM:
                iterator.set_postfix({"æˆåŠŸ": success_count, "å¤±è´¥": failed_count})
        else:
            failed_count += 1
            if not USE_TQDM:
                print(f"      âŒ å¤±è´¥")
            elif USE_TQDM:
                iterator.set_postfix({"æˆåŠŸ": success_count, "å¤±è´¥": failed_count})
    
    print()
    print("=" * 60)
    print("ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡")
    print("=" * 60)
    print(f"   æ€»æ®µè½æ•°: {len(segments)}")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {failed_count}")
    print(f"   æˆåŠŸç‡: {success_count/len(segments)*100:.1f}%")
    print()
    print(f"   Codexæ–‡ä»¶: {OUTPUT_CODEX_PATH}")
    if enable_indexing:
        print(f"   å‘é‡æ•°æ®åº“: {VECTOR_DB_PATH}")
    print()
    print("âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="FDS-KMS æ‰¹é‡å¤„ç†è„šæœ¬")
    parser.add_argument("input_file", help="è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--book", default="å­å¹³çœŸè¯ ", help="å…¸ç±åç§°")
    parser.add_argument("--topic", default="é£Ÿç¥æ ¼", help="ä¸»é¢˜/æ ¼å±€åç§°")
    parser.add_argument("--index", action="store_true", help="å¯ç”¨å‘é‡ç´¢å¼•")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        print()
        print("ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        print(f"   python {sys.argv[0]} raw_texts/å­å¹³çœŸè¯ _è®ºé£Ÿç¥.txt --book å­å¹³çœŸè¯  --topic é£Ÿç¥æ ¼ --index")
        return
    
    process_batch(
        input_file=args.input_file,
        source_book=args.book,
        topic=args.topic,
        enable_indexing=args.index
    )


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œæ˜¾ç¤ºä½¿ç”¨è¯´æ˜
    if len(sys.argv) == 1:
        print("=" * 60)
        print("FDS-KMS æ‰¹é‡å¤„ç†è„šæœ¬")
        print("=" * 60)
        print()
        print("ä½¿ç”¨æ–¹æ³•:")
        print(f"   python {sys.argv[0]} <è¾“å…¥æ–‡ä»¶> [é€‰é¡¹]")
        print()
        print("é€‰é¡¹:")
        print("   --book <åç§°>    å…¸ç±åç§° (é»˜è®¤: å­å¹³çœŸè¯ )")
        print("   --topic <åç§°>   ä¸»é¢˜/æ ¼å±€åç§° (é»˜è®¤: é£Ÿç¥æ ¼)")
        print("   --index          å¯ç”¨å‘é‡ç´¢å¼•")
        print()
        print("ç¤ºä¾‹:")
        print(f"   python {sys.argv[0]} raw_texts/å­å¹³çœŸè¯ _è®ºé£Ÿç¥.txt --book å­å¹³çœŸè¯  --topic é£Ÿç¥æ ¼ --index")
        print()
        print("ğŸ’¡ æç¤º:")
        print("   1. å‡†å¤‡åŸå§‹æ–‡æœ¬æ–‡ä»¶ï¼ˆUTF-8ç¼–ç ï¼‰")
        print("   2. æ–‡æœ¬å°†è¢«è‡ªåŠ¨åˆ‡åˆ†ä¸ºæ®µè½")
        print("   3. æ¯ä¸ªæ®µè½å°†è¢«LLMå¤„ç†ç”Ÿæˆcodexæ¡ç›®")
        print("   4. ç»“æœä¿å­˜åˆ° classical_codex.jsonl")
    else:
        main()

