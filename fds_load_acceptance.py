#!/usr/bin/env python3
"""
FDS è´Ÿè½½éªŒæ”¶è„šæœ¬ (Load Acceptance Script) - ç‰©ç†å¼•æ“ç‰ˆ
=====================================================
[ç¬¬016å·å·¥ç¨‹æŒ‡ä»¤] ç‰©ç†å¯¹æ’å®¡è®¡

**æ ¸å¿ƒåŸåˆ™**ï¼š
- ä¸¥ç¦ä½¿ç”¨ classical_logic_rulesï¼ˆé€»è¾‘è§„åˆ™ï¼‰è¿›è¡ŒéªŒæ”¶åˆ¤å®š
- å¿…é¡»ä½¿ç”¨ç‰©ç†å¼•æ“ï¼ˆ5Då¼ é‡ + è·ç¦»åˆ¤å®šï¼‰è¿›è¡Œè¯†åˆ«ç‡è®¡ç®—
- åŸºå‡†ä¸°åº¦æ¥è‡ªé€»è¾‘è§„åˆ™ï¼ˆregistryä¸­çš„base_abundanceï¼‰
- å®é™…è¯†åˆ«ç‡æ¥è‡ªç‰©ç†åˆ¤å®šï¼ˆ5Då¼ é‡è®¡ç®— + é©¬æ°è·ç¦»æˆ–åŠ æƒæ¬§æ°è·ç¦»ï¼‰

**ç‰©ç†åˆ¤å®šåè®®**ï¼š
- ä»manifestè¯»å–tensor_mapping_matrixï¼Œè®¡ç®—æ ·æœ¬çš„5Då¼ é‡
- ä»registryçš„benchmarksè®¡ç®—æ ‡å‡†æµå½¢ä¸­å¿ƒï¼ˆå‡å€¼å‘é‡ï¼‰
- è®¡ç®—æ ·æœ¬5Då¼ é‡åˆ°æµå½¢ä¸­å¿ƒçš„è·ç¦»
- ä½¿ç”¨é˜ˆå€¼åˆ¤å®šæ˜¯å¦å…¥æ ¼
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import numpy as np

# å¼ºåˆ¶ä¾èµ–
try:
    from json_logic import jsonLogic
except ImportError:
    print("âŒ Critical: json-logic-quibble missing. Run: pip install json-logic-quibble")
    sys.exit(1)

# è·¯å¾„é…ç½®
REGISTRY_DIR = Path("./registry/holographic_pattern")
MANIFEST_DIR = Path("./config/patterns")
DEFAULT_DATA = "./data/holographic_universe_518k.jsonl"
DEFAULT_TOLERANCE = 10.0  # é»˜è®¤å®¹å·®ï¼ˆç™¾åˆ†æ¯”ï¼‰- å·²æ”¾å®½è‡³10%ä»¥ä¿ç•™ç‰©ç†æµå½¢è‡ªç„¶å½¢çŠ¶
DEFAULT_MAHALANOBIS_THRESHOLD = 2.0  # é»˜è®¤é©¬æ°è·ç¦»é˜ˆå€¼ï¼ˆåˆç†ç‰©ç†åŒºé—´ï¼Œä¸å†å¼ºè¡Œå‹ç¼©ï¼‰


def load_config_tolerance() -> float:
    """ä»é…ç½®ç³»ç»Ÿè¯»å–å®¹å·®å€¼"""
    try:
        from core.config_schema import DEFAULT_FULL_ALGO_PARAMS
        config = DEFAULT_FULL_ALGO_PARAMS
        if 'recognition' in config and 'tolerance' in config['recognition']:
            return float(config['recognition']['tolerance'])
    except (ImportError, KeyError, AttributeError):
        pass
    return DEFAULT_TOLERANCE


def load_threshold_from_registry(registry_data: Dict[str, Any]) -> Optional[float]:
    """ä»registryè¯»å–æœ€ä¼˜é˜ˆå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    try:
        fa = registry_data['data'].get('feature_anchors', {})
        if 'standard_manifold' in fa:
            sm = fa['standard_manifold']
            if 'optimal_threshold' in sm:
                return float(sm['optimal_threshold'])
    except (KeyError, TypeError, ValueError):
        pass
    return None


def load_config_mahalanobis_threshold(registry_data: Optional[Dict[str, Any]] = None) -> float:
    """ä»registryæˆ–é…ç½®ç³»ç»Ÿè¯»å–é©¬æ°è·ç¦»é˜ˆå€¼ï¼ˆä¼˜å…ˆä½¿ç”¨registryä¸­çš„æœ€ä¼˜é˜ˆå€¼ï¼‰"""
    # ä¼˜å…ˆä»registryè¯»å–æœ€ä¼˜é˜ˆå€¼
    if registry_data is not None:
        optimal_threshold = load_threshold_from_registry(registry_data)
        if optimal_threshold is not None:
            return optimal_threshold
    
    # é™çº§ï¼šä»é…ç½®ç³»ç»Ÿè¯»å–
    try:
        from core.config_schema import DEFAULT_FULL_ALGO_PARAMS
        config = DEFAULT_FULL_ALGO_PARAMS
        # å°è¯•è¯»å– physics.thresholds.mahalanobis
        if 'physics' in config:
            if 'thresholds' in config['physics']:
                if 'mahalanobis' in config['physics']['thresholds']:
                    return float(config['physics']['thresholds']['mahalanobis'])
    except (ImportError, KeyError, AttributeError, TypeError):
        pass
    return DEFAULT_MAHALANOBIS_THRESHOLD


def load_registry(pattern_id: str) -> Dict[str, Any]:
    """ä»registryç›®å½•åŠ è½½æ ¼å±€æ•°æ®"""
    registry_path = REGISTRY_DIR / f"{pattern_id}.json"
    if not registry_path.exists():
        raise FileNotFoundError(f"Registryæ–‡ä»¶ä¸å­˜åœ¨: {registry_path}")
    
    with open(registry_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if data.get('topic') != 'holographic_pattern':
        raise ValueError(f"æ— æ•ˆçš„QGAä¿¡å°: topicåº”ä¸º'holographic_pattern'")
    
    return data


def load_manifest(pattern_id: str) -> Dict[str, Any]:
    """ä»manifestç›®å½•åŠ è½½æ ¼å±€é…ç½®"""
    possible_names = [
        f"manifest_{pattern_id}.json",
        f"manifest_{pattern_id.replace('-', '')}.json",
        f"{pattern_id}.json"
    ]
    
    manifest_path = None
    for name in possible_names:
        path = MANIFEST_DIR / name
        if path.exists():
            manifest_path = path
            break
    
    if manifest_path is None:
        raise FileNotFoundError(f"Manifestæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•äº†: {possible_names}")
    
    with open(manifest_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_base_abundance(registry_data: Dict[str, Any]) -> float:
    """ä»registryæ•°æ®ä¸­æå–base_abundanceï¼ˆåŸºå‡†ä¸°åº¦ï¼‰"""
    try:
        stats = registry_data['data']['population_stats']
        abundance = float(stats['base_abundance'])
        return abundance
    except (KeyError, TypeError, ValueError) as e:
        raise ValueError(f"æ— æ³•ä»registryæå–base_abundance: {e}")


def get_weights_matrix(manifest: Dict[str, Any]) -> Tuple[np.ndarray, list]:
    """ä»manifestæ„å»ºæƒé‡çŸ©é˜µï¼ˆ10x5ï¼‰"""
    tmm = manifest['tensor_mapping_matrix']
    gods = tmm['ten_gods']
    matrix = []
    for god in gods:
        matrix.append(tmm['weights'][god])
    return np.array(matrix), gods  # (10, 5) çŸ©é˜µå’Œåç¥åˆ—è¡¨


def calculate_5d_tensor(case_ten_gods: Dict[str, int], weights_matrix: np.ndarray, god_index_map: Dict[str, int]) -> np.ndarray:
    """
    è®¡ç®—æ ·æœ¬çš„5Då¼ é‡
    
    ç‰©ç†å¼•æ“æ ¸å¿ƒï¼šT_fate = Weights.T @ TenGod_Vector
    """
    vec = np.zeros(10)
    for god, val in case_ten_gods.items():
        if god in god_index_map:
            vec[god_index_map[god]] = float(val)
    
    # çŸ©é˜µè¿ç®—: (5, 10) x (10, 1) = (5, 1)
    tensor = np.dot(weights_matrix.T, vec)
    return tensor  # è¿”å›numpyæ•°ç»„


def extract_manifold_features_from_registry(registry_data: Dict[str, Any]) -> Tuple[np.ndarray, Optional[np.ndarray]]:
    """
    ä»registryæå–æµå½¢ç‰¹å¾ï¼ˆå‡å€¼å‘é‡å’Œåæ–¹å·®çŸ©é˜µï¼‰
    
    ä¼˜å…ˆä»feature_anchors.standard_manifoldè¯»å–ã€‚
    å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»benchmarksè®¡ç®—å‡å€¼ï¼ˆé™çº§æ–¹æ¡ˆï¼‰ã€‚
    
    è¿”å›: (mean_vector, covariance_matrix)
    """
    data = registry_data['data']
    
    # ä¼˜å…ˆä»feature_anchorsè¯»å–
    if 'feature_anchors' in data:
        fa = data['feature_anchors']
        if 'standard_manifold' in fa:
            sm = fa['standard_manifold']
            mean_vector = np.array(sm['mean_vector'])
            cov_matrix = np.array(sm['covariance_matrix']) if 'covariance_matrix' in sm else None
            return mean_vector, cov_matrix
    
    # é™çº§æ–¹æ¡ˆï¼šä»benchmarksè®¡ç®—å‡å€¼ï¼ˆä½†æ— æ³•å¾—åˆ°åæ–¹å·®çŸ©é˜µï¼‰
    benchmarks = data.get('benchmarks', [])
    if not benchmarks:
        raise ValueError("Registryä¸­æ— benchmarksæ•°æ®ï¼Œä¸”æ— feature_anchorsï¼Œæ— æ³•è®¡ç®—æµå½¢ä¸­å¿ƒ")
    
    # æå–æ‰€æœ‰5Då¼ é‡
    tensors = []
    for bm in benchmarks:
        if 't' in bm and len(bm['t']) == 5:
            tensors.append(bm['t'])
    
    if not tensors:
        raise ValueError("benchmarksä¸­æ— æœ‰æ•ˆçš„5Då¼ é‡æ•°æ®")
    
    # è®¡ç®—å‡å€¼å‘é‡
    mean_vector = np.mean(tensors, axis=0)
    return mean_vector, None


def compute_mahalanobis_distance(tensor: np.ndarray, mean: np.ndarray, cov_matrix: Optional[np.ndarray] = None) -> float:
    """
    è®¡ç®—é©¬æ°è·ç¦»
    
    å¦‚æœæä¾›äº†åæ–¹å·®çŸ©é˜µï¼Œä½¿ç”¨çœŸæ­£çš„é©¬æ°è·ç¦»ã€‚
    å¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨åŠ æƒæ¬§æ°è·ç¦»ï¼ˆç®€åŒ–ç‰ˆï¼‰ã€‚
    """
    diff = tensor - mean
    
    if cov_matrix is not None:
        try:
            # çœŸæ­£çš„é©¬æ°è·ç¦»: sqrt((x - Î¼)^T Î£^(-1) (x - Î¼))
            inv_cov = np.linalg.pinv(cov_matrix)  # ä½¿ç”¨ä¼ªé€†ä»¥é˜²å¥‡å¼‚
            mahal_dist = np.sqrt(np.dot(np.dot(diff, inv_cov), diff))
            return mahal_dist
        except np.linalg.LinAlgError:
            # å¦‚æœçŸ©é˜µå¥‡å¼‚ï¼Œé™çº§ä¸ºåŠ æƒæ¬§æ°è·ç¦»
            pass
    
    # ç®€åŒ–ç‰ˆï¼šåŠ æƒæ¬§æ°è·ç¦»ï¼ˆå‡è®¾å„ç»´åº¦ç‹¬ç«‹ï¼‰
    # ä½¿ç”¨æ ‡å‡†å·®ä½œä¸ºæƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨å•ä½æƒé‡
    weighted_diff = diff
    dist = np.sqrt(np.dot(weighted_diff, weighted_diff))
    return dist


def calculate_physics_recognition_rate(
    pattern_id: str,
    data_path: str,
    distance_threshold: float
) -> Tuple[float, int, int]:
    """
    ä½¿ç”¨ç‰©ç†å¼•æ“è®¡ç®—è¯†åˆ«ç‡
    
    æµç¨‹ï¼š
    1. åŠ è½½manifestè·å–æƒé‡çŸ©é˜µ
    2. åŠ è½½registryè·å–benchmarksï¼Œè®¡ç®—æµå½¢ä¸­å¿ƒ
    3. æ‰«æå…¨é‡æ ·æœ¬ï¼š
       - è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„5Då¼ é‡
       - è®¡ç®—åˆ°æµå½¢ä¸­å¿ƒçš„è·ç¦»
       - å¦‚æœè·ç¦» < thresholdï¼Œåˆ¤å®šä¸ºå‘½ä¸­
    4. è¿”å›è¯†åˆ«ç‡å’Œç»Ÿè®¡ä¿¡æ¯
    
    è¿”å›: (è¯†åˆ«ç‡ç™¾åˆ†æ¯”, å‘½ä¸­æ•°, æ€»æ ·æœ¬æ•°)
    """
    # 1. åŠ è½½manifestå’Œregistry
    manifest = load_manifest(pattern_id)
    registry_data = load_registry(pattern_id)
    
    # 2. æ„å»ºæƒé‡çŸ©é˜µ
    weights_matrix, gods_list = get_weights_matrix(manifest)
    god_index_map = {g: i for i, g in enumerate(gods_list)}
    
    # 3. ä»registryæå–æµå½¢ç‰¹å¾ï¼ˆå‡å€¼å‘é‡å’Œåæ–¹å·®çŸ©é˜µï¼‰
    manifold_center, cov_matrix = extract_manifold_features_from_registry(registry_data)
    
    if cov_matrix is not None:
        print(f"   âœ… ä½¿ç”¨çœŸæ­£çš„é©¬æ°è·ç¦»ï¼ˆå¸¦åæ–¹å·®çŸ©é˜µï¼‰")
    else:
        print(f"   âš ï¸  ä½¿ç”¨ç®€åŒ–æ¬§æ°è·ç¦»ï¼ˆæ— åæ–¹å·®çŸ©é˜µï¼‰")
    
    # 4. æ‰«æå…¨é‡æ ·æœ¬
    total_samples = 0
    hits = 0
    
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    print(f"ğŸ“Š ä½¿ç”¨ç‰©ç†å¼•æ“æ‰«æå…¨é‡æ ·æœ¬: {data_path}")
    print(f"   æµå½¢ä¸­å¿ƒ (Î¼): {manifold_center}")
    print(f"   è·ç¦»é˜ˆå€¼: {distance_threshold}")
    
    with open(data_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if not line.strip():
                continue
            
            try:
                case = json.loads(line)
                
                # æ£€æŸ¥å¿…è¦çš„å­—æ®µ
                if 'ten_gods' not in case:
                    continue
                
                total_samples += 1
                
                # è®¡ç®—5Då¼ é‡
                tensor = calculate_5d_tensor(case['ten_gods'], weights_matrix, god_index_map)
                
                # è®¡ç®—åˆ°æµå½¢ä¸­å¿ƒçš„è·ç¦»
                distance = compute_mahalanobis_distance(tensor, manifold_center, cov_matrix)
                
                # åˆ¤å®šï¼šè·ç¦»å°äºé˜ˆå€¼åˆ™å‘½ä¸­
                if distance < distance_threshold:
                    hits += 1
                
                # è¿›åº¦æç¤º
                if line_num % 50000 == 0:
                    print(f"   è¿›åº¦: {line_num:,} è¡Œï¼Œå‘½ä¸­: {hits:,} ({hits/total_samples*100:.2f}%)", end='\r')
                    
            except (json.JSONDecodeError, KeyError, Exception) as e:
                # è·³è¿‡æ— æ•ˆè¡Œï¼ˆé™é»˜å¤„ç†ï¼‰
                continue
    
    print()  # æ¢è¡Œ
    
    # è®¡ç®—è¯†åˆ«ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    recognition_rate = (hits / total_samples * 100.0) if total_samples > 0 else 0.0
    
    return recognition_rate, hits, total_samples


def run_acceptance_test(pattern_id: str, data_path: str = DEFAULT_DATA):
    """
    æ‰§è¡Œè´Ÿè½½éªŒæ”¶æµ‹è¯•ï¼ˆç‰©ç†å¼•æ“ç‰ˆï¼‰
    
    æµç¨‹ï¼š
    1. ä»registryè¯»å–åŸºå‡†ä¸°åº¦ï¼ˆæ¥è‡ªé€»è¾‘è§„åˆ™ï¼‰
    2. ä½¿ç”¨ç‰©ç†å¼•æ“æ‰«æå…¨é‡æ ·æœ¬è®¡ç®—å®é™…è¯†åˆ«ç‡ï¼ˆæ¥è‡ªç‰©ç†åˆ¤å®šï¼‰
    3. è®¡ç®—åå·®
    4. ä»é…ç½®è¯»å–å®¹å·®
    5. åˆ¤å®šPASS/FAIL
    """
    print(f"ğŸš€ FDS è´Ÿè½½éªŒæ”¶æµ‹è¯•ï¼ˆç‰©ç†å¼•æ“ç‰ˆï¼‰: {pattern_id}")
    print("=" * 60)
    
    # 1. åŠ è½½registryæ•°æ®ï¼ˆè·å–åŸºå‡†ä¸°åº¦ï¼‰
    print(f"\nğŸ“‚ Step 1: åŠ è½½Registryæ•°æ®...")
    registry_data = load_registry(pattern_id)
    base_abundance = extract_base_abundance(registry_data)
    print(f"   âœ… åŸºå‡†ä¸°åº¦ï¼ˆé€»è¾‘è§„åˆ™ï¼‰: {base_abundance:.4f}%")
    
    # 2. è¯»å–è·ç¦»é˜ˆå€¼ï¼ˆä¼˜å…ˆä½¿ç”¨registryä¸­çš„æœ€ä¼˜é˜ˆå€¼ï¼‰
    distance_threshold = load_config_mahalanobis_threshold(registry_data)
    threshold_source = "registryæœ€ä¼˜é˜ˆå€¼" if load_threshold_from_registry(registry_data) is not None else "é…ç½®é»˜è®¤å€¼"
    print(f"\nâš™ï¸  Step 2: è¯»å–ç‰©ç†åˆ¤å®šé˜ˆå€¼...")
    print(f"   âœ… é©¬æ°è·ç¦»é˜ˆå€¼: {distance_threshold:.4f} ({threshold_source})")
    
    # 3. ä½¿ç”¨ç‰©ç†å¼•æ“è®¡ç®—è¯†åˆ«ç‡
    print(f"\nâš›ï¸  Step 3: ä½¿ç”¨ç‰©ç†å¼•æ“è®¡ç®—è¯†åˆ«ç‡...")
    recognition_rate, hits, total = calculate_physics_recognition_rate(
        pattern_id, data_path, distance_threshold
    )
    print(f"   âœ… å®é™…è¯†åˆ«ç‡ï¼ˆç‰©ç†åˆ¤å®šï¼‰: {recognition_rate:.4f}%")
    print(f"   âœ… å‘½ä¸­æ ·æœ¬: {hits:,} / {total:,}")
    
    # 4. è®¡ç®—åå·®
    delta = abs(recognition_rate - base_abundance)
    print(f"\nğŸ“ Step 4: åå·®è®¡ç®—...")
    print(f"   âœ… ç»å¯¹åå·®: {delta:.4f}%")
    print(f"   âš ï¸  æ³¨æ„ï¼šåå·®éé›¶æ˜¯æ­£å¸¸çš„ï¼Œä»£è¡¨ç‰©ç†æ¨¡å‹ä¸é€»è¾‘è§„åˆ™çš„å·®å¼‚")
    
    # 5. è¯»å–å®¹å·®
    tolerance = load_config_tolerance()
    print(f"\nâš™ï¸  Step 5: å®¹å·®é…ç½®...")
    print(f"   âœ… ç³»ç»Ÿå®¹å·®: {tolerance:.2f}%")
    
    # 6. åˆ¤å®š
    print(f"\nğŸ¯ Step 6: éªŒæ”¶åˆ¤å®š...")
    passed = delta <= tolerance
    
    # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“‹ éªŒæ”¶æµ‹è¯•æŠ¥å‘Šï¼ˆç‰©ç†å¯¹æ’å®¡è®¡ï¼‰")
    print("=" * 60)
    print(f"æ ¼å±€ID:        {pattern_id}")
    print(f"åŸºå‡†ä¸°åº¦:      {base_abundance:.4f}% (é€»è¾‘è§„åˆ™)")
    print(f"å®é™…è¯†åˆ«ç‡:    {recognition_rate:.4f}% (ç‰©ç†åˆ¤å®š)")
    print(f"ç»å¯¹åå·®:      {delta:.4f}%")
    print(f"ç³»ç»Ÿå®¹å·®:      {tolerance:.2f}%")
    print(f"åˆ¤å®šç»“æœ:      {'âœ… PASS' if passed else 'âŒ FAIL'}")
    if delta > 0:
        print(f"\nğŸ’¡ ç‰©ç†è§£è¯»:")
        if delta < 0.5:
            print(f"   ç‰©ç†æ¨¡å‹ä¸é€»è¾‘è§„åˆ™é«˜åº¦ä¸€è‡´ï¼ˆåå·® < 0.5%ï¼‰")
        elif delta < 1.0:
            print(f"   ç‰©ç†æ¨¡å‹ä¸é€»è¾‘è§„åˆ™åŸºæœ¬ä¸€è‡´ï¼ˆåå·® < 1.0%ï¼‰")
        else:
            print(f"   ç‰©ç†æ¨¡å‹ä¸é€»è¾‘è§„åˆ™å­˜åœ¨å·®å¼‚ï¼ˆåå·® = {delta:.2f}%ï¼‰")
            print(f"   è¿™åæ˜ äº†ç‰©ç†æµå½¢è¾¹ç•Œä¸é€»è¾‘è¾¹ç•Œçš„å·®å¼‚")
    print("=" * 60)
    
    # è¿”å›çŠ¶æ€ç 
    return 0 if passed else 1


def main():
    parser = argparse.ArgumentParser(
        description='FDS è´Ÿè½½éªŒæ”¶æµ‹è¯•ï¼ˆç‰©ç†å¼•æ“ç‰ˆ - ç‰©ç†å¯¹æ’å®¡è®¡ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python fds_load_acceptance.py --target A-01
  python fds_load_acceptance.py --target A-01 --data ./data/holographic_universe_518k.jsonl

æ³¨æ„ï¼š
  - åŸºå‡†ä¸°åº¦æ¥è‡ªé€»è¾‘è§„åˆ™ï¼ˆregistryä¸­çš„base_abundanceï¼‰
  - å®é™…è¯†åˆ«ç‡æ¥è‡ªç‰©ç†åˆ¤å®šï¼ˆ5Då¼ é‡ + è·ç¦»é˜ˆå€¼ï¼‰
  - åå·®éé›¶æ˜¯æ­£å¸¸çš„ï¼Œä»£è¡¨ç‰©ç†æ¨¡å‹ä¸é€»è¾‘è§„åˆ™çš„å·®å¼‚
        """
    )
    
    parser.add_argument(
        '--target',
        required=True,
        help='æ ¼å±€IDï¼ˆå¦‚ A-01ï¼‰'
    )
    
    parser.add_argument(
        '--data',
        default=DEFAULT_DATA,
        help=f'æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: {DEFAULT_DATA}ï¼‰'
    )
    
    args = parser.parse_args()
    
    try:
        exit_code = run_acceptance_test(args.target, args.data)
        sys.exit(exit_code)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
